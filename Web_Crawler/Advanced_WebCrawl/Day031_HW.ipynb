{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 反爬：瀏覽器標頭與基本資訊\n",
    "\n",
    "\n",
    "* 了解「檢查 HTTP 標頭檔」的反爬蟲機制\n",
    "* 「檢查 HTTP 標頭檔」反爬蟲的因應策略\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 作業目標\n",
    "\n",
    "* 尋找一個網站是有作 Header 檢查的（Hint: 有加跟沒有加回傳結果不一樣）\n",
    "* 用上述網站說明該如何判斷 Header 中必須加上哪些資料才會正確\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 尋找一個網站是有作 Header 檢查的（Hint: 有加跟沒有加回傳結果不一樣）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<html>\\r\\n<head><title>400 Bad Request</title></head>\\r\\n<body bgcolor=\"white\">\\r\\n<center><h1>400 Bad Request</h1></center>\\r\\n<hr><center>openresty</center>\\r\\n</body>\\r\\n</html>\\r\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Your Code Here\n",
    "\"\"\"\n",
    "import requests\n",
    "url = 'https://www.zhihu.com/question/29316149'\n",
    "res = requests.get(url)\n",
    "response = res.text\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 用上述網站說明該如何判斷 Header 中必須加上哪些資料才會正確\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<!doctype html>\\n<html lang=\"zh\" data-hairline=\"true\" data-theme=\"light\"><head><meta charSet=\"utf-8\"/><title data-react-helmet=\"true\">特征工程到底是什么？ - 知乎</title><meta name=\"viewport\" content=\"width=device-width,initial-scale=1,maximum-scale=1\"/><meta name=\"renderer\" content=\"webkit\"/><meta name=\"force-rendering\" content=\"webkit\"/><meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge,chrome=1\"/><meta name=\"google-site-verification\" content=\"FTeR0c8arOPKh8c5DYh_9uu98_zJbaWw53J-Sch9MTg\"/><meta data-react-helmet=\"true\" name=\"description\" property=\"og:description\" content=\"大概知道一些，但是不是很清楚领域内的知识是怎么帮助特征工程的呢？如果有例子就更好了。\"/><meta data-react-helmet=\"true\" name=\"keywords\" content=\"数据挖掘,数据分析,推荐系统,大数据,特征选择\"/><link data-react-helmet=\"true\" rel=\"apple-touch-icon\" href=\"https://static.zhihu.com/heifetz/assets/apple-touch-icon-152.67c7b278.png\"/><link data-react-helmet=\"true\" rel=\"apple-touch-icon\" href=\"https://static.zhihu.com/heifetz/assets/apple-touch-icon-152.67c7b278.png\" sizes=\"152x152\"/><link data-react-helmet=\"true\" rel=\"apple-touch-icon\" href=\"https://static.zhihu.com/heifetz/assets/apple-touch-icon-120.b3e6278d.png\" sizes=\"120x120\"/><link data-react-helmet=\"true\" rel=\"apple-touch-icon\" href=\"https://static.zhihu.com/heifetz/assets/apple-touch-icon-76.7a750095.png\" sizes=\"76x76\"/><link data-react-helmet=\"true\" rel=\"apple-touch-icon\" href=\"https://static.zhihu.com/heifetz/assets/apple-touch-icon-60.a4a761d4.png\" sizes=\"60x60\"/><link rel=\"shortcut icon\" type=\"image/x-icon\" href=\"https://static.zhihu.com/static/favicon.ico\"/><link rel=\"search\" type=\"application/opensearchdescription+xml\" href=\"https://static.zhihu.com/static/search.xml\" title=\"知乎\"/><link rel=\"dns-prefetch\" href=\"//static.zhimg.com\"/><link rel=\"dns-prefetch\" href=\"//pic1.zhimg.com\"/><link rel=\"dns-prefetch\" href=\"//pic2.zhimg.com\"/><link rel=\"dns-prefetch\" href=\"//pic3.zhimg.com\"/><link rel=\"dns-prefetch\" href=\"//pic4.zhimg.com\"/><style>\\n.u-safeAreaInset-top {\\n  height: constant(safe-area-inset-top) !important;\\n  height: env(safe-area-inset-top) !important;\\n  \\n}\\n.u-safeAreaInset-bottom {\\n  height: constant(safe-area-inset-bottom) !important;\\n  height: env(safe-area-inset-bottom) !important;\\n  \\n}\\n</style><link href=\"https://static.zhihu.com/heifetz/main.app.216a26f4.63b2dac39544fee96b8b.css\" rel=\"stylesheet\"/><link href=\"https://static.zhihu.com/heifetz/main.question-routes.216a26f4.a3a3042cdcc9f44bb6c2.css\" rel=\"stylesheet\"/><script defer=\"\" crossorigin=\"anonymous\" src=\"https://unpkg.zhimg.com/@cfe/sentry-script@latest/dist/init.js\" data-sentry-config=\"{&quot;dsn&quot;:&quot;https://2d8d764432cc4f6fb3bc78ab9528299d@crash2.zhihu.com/1224&quot;,&quot;sampleRate&quot;:0.1,&quot;release&quot;:&quot;1187-4b9dcf20&quot;,&quot;ignoreErrorNames&quot;:[&quot;NetworkError&quot;,&quot;SecurityError&quot;],&quot;ignoreErrors&quot;:[&quot;origin message&quot;,&quot;Network request failed&quot;,&quot;Loading chunk&quot;,&quot;这个系统不支持该功能。&quot;,&quot;Can&#x27;t find variable: webkit&quot;,&quot;Can&#x27;t find variable: $&quot;,&quot;内存不足&quot;,&quot;out of memory&quot;,&quot;DOM Exception 18&quot;,&quot;The operation is insecure&quot;,&quot;[object Event]&quot;,&quot;[object FileError]&quot;,&quot;[object DOMError]&quot;,&quot;[object Object]&quot;,&quot;拒绝访问。&quot;,&quot;Maximum call stack size exceeded&quot;,&quot;UploadError&quot;,&quot;无法 fetch&quot;,&quot;draft-js&quot;,&quot;缺少 JavaScript 对象&quot;,&quot;componentWillEnter&quot;,&quot;componentWillLeave&quot;,&quot;componentWillAppear&quot;,&quot;getInlineStyleAt&quot;,&quot;getCharacterList&quot;],&quot;whitelistUrls&quot;:[&quot;static.zhihu.com&quot;]}\"></script></head><body><div id=\"root\"><div><div class=\"LoadingBar\"></div><div><header role=\"banner\" class=\"Sticky AppHeader\" data-za-module=\"TopNavBar\"><div class=\"AppHeader-inner\"><a href=\"//www.zhihu.com\" aria-label=\"知乎\"><svg viewBox=\"0 0 200 91\" fill=\"#0084FF\" width=\"64\" height=\"30\"><path d=\"M53.29 80.035l7.32.002 2.41 8.24 13.128-8.24h15.477v-67.98H53.29v67.978zm7.79-60.598h22.756v53.22h-8.73l-8.718 5.473-1.587-5.46-3.72-.012v-53.22zM46.818 43.162h-16.35c.545-8.467.687-16.12.687-22.955h15.987s.615-7.05-2.68-6.97H16.807c1.09-4.1 2.46-8.332 4.1-12.708 0 0-7.523 0-10.085 6.74-1.06 2.78-4.128 13.48-9.592 24.41 1.84-.2 7.927-.37 11.512-6.94.66-1.84.785-2.08 1.605-4.54h9.02c0 3.28-.374 20.9-.526 22.95H6.51c-3.67 0-4.863 7.38-4.863 7.38H22.14C20.765 66.11 13.385 79.24 0 89.62c6.403 1.828 12.784-.29 15.937-3.094 0 0 7.182-6.53 11.12-21.64L43.92 85.18s2.473-8.402-.388-12.496c-2.37-2.788-8.768-10.33-11.496-13.064l-4.57 3.627c1.363-4.368 2.183-8.61 2.46-12.71H49.19s-.027-7.38-2.372-7.38zm128.752-.502c6.51-8.013 14.054-18.302 14.054-18.302s-5.827-4.625-8.556-1.27c-1.874 2.548-11.51 15.063-11.51 15.063l6.012 4.51zm-46.903-18.462c-2.814-2.577-8.096.667-8.096.667s12.35 17.2 12.85 17.953l6.08-4.29s-8.02-11.752-10.83-14.33zM199.99 46.5c-6.18 0-40.908.292-40.953.292v-31.56c1.503 0 3.882-.124 7.14-.376 12.773-.753 21.914-1.25 27.427-1.504 0 0 3.817-8.496-.185-10.45-.96-.37-7.24 1.43-7.24 1.43s-51.63 5.153-72.61 5.64c.5 2.756 2.38 5.336 4.93 6.11 4.16 1.087 7.09.53 15.36.277 7.76-.5 13.65-.76 17.66-.76v31.19h-41.71s.88 6.97 7.97 7.14h33.73v22.16c0 4.364-3.498 6.87-7.65 6.6-4.4.034-8.15-.36-13.027-.566.623 1.24 1.977 4.496 6.035 6.824 3.087 1.502 5.054 2.053 8.13 2.053 9.237 0 14.27-5.4 14.027-14.16V53.93h38.235c3.026 0 2.72-7.432 2.72-7.432z\" fill-rule=\"evenodd\"></path></svg></a><ul role=\"navigation\" class=\"Tabs AppHeader-Tabs\"><li role=\"tab\" class=\"Tabs-item AppHeader-Tab Tabs-item--noMeta\"><a class=\"Tabs-link AppHeader-TabsLink\" href=\"//www.zhihu.com/\" data-za-not-track-link=\"true\">首页</a></li><li role=\"tab\" class=\"Tabs-item AppHeader-Tab Tabs-item--noMeta\"><a class=\"Tabs-link AppHeader-TabsLink\" href=\"//www.zhihu.com/explore\" data-za-not-track-link=\"true\">发现</a></li><li role=\"tab\" class=\"Tabs-item AppHeader-Tab Tabs-item--noMeta\"><a class=\"Tabs-link AppHeader-TabsLink\" href=\"//www.zhihu.com/question/waiting\" data-za-not-track-link=\"true\">等你来答</a></li></ul><div class=\"SearchBar\" role=\"search\" data-za-module=\"PresetWordItem\"><div class=\"SearchBar-toolWrapper\"><form class=\"SearchBar-tool\"><div><div class=\"Popover\"><label class=\"SearchBar-input Input-wrapper Input-wrapper--grey\"><input type=\"text\" maxLength=\"100\" value=\"\" autoComplete=\"off\" role=\"combobox\" aria-expanded=\"false\" aria-autocomplete=\"list\" aria-activedescendant=\"null--1\" id=\"null-toggle\" aria-haspopup=\"true\" aria-owns=\"null-content\" class=\"Input\" placeholder=\"\"/><button aria-label=\"搜索\" type=\"button\" class=\"Button SearchBar-searchButton Button--primary\"><span style=\"display:inline-flex;align-items:center\">\\u200b<svg class=\"Zi Zi--Search SearchBar-searchIcon\" fill=\"currentColor\" viewBox=\"0 0 24 24\" width=\"18\" height=\"18\"><path d=\"M17.068 15.58a8.377 8.377 0 0 0 1.774-5.159 8.421 8.421 0 1 0-8.42 8.421 8.38 8.38 0 0 0 5.158-1.774l3.879 3.88c.957.573 2.131-.464 1.488-1.49l-3.879-3.878zm-6.647 1.157a6.323 6.323 0 0 1-6.316-6.316 6.323 6.323 0 0 1 6.316-6.316 6.323 6.323 0 0 1 6.316 6.316 6.323 6.323 0 0 1-6.316 6.316z\" fill-rule=\"evenodd\"></path></svg></span></button></label></div></div></form></div></div><div class=\"AppHeader-userInfo\"><div class=\"AppHeader-profile\"><div><button type=\"button\" class=\"Button AppHeader-login Button--blue\">登录</button><button type=\"button\" class=\"Button Button--primary Button--blue\">加入知乎</button></div></div></div></div><div></div></header></div><main role=\"main\" class=\"App-main\"><div class=\"QuestionPage\" itemscope=\"\" itemType=\"http://schema.org/Question\"><meta itemProp=\"name\" content=\"特征工程到底是什么？\"/><meta itemProp=\"url\" content=\"https://www.zhihu.com/question/29316149\"/><meta itemProp=\"keywords\" content=\"数据挖掘,数据分析,推荐系统,大数据,特征选择\"/><meta itemProp=\"answerCount\" content=\"38\"/><meta itemProp=\"commentCount\" content=\"0\"/><meta itemProp=\"dateCreated\" content=\"2015-04-05T08:01:16.000Z\"/><meta itemProp=\"dateModified\" content=\"2015-04-05T08:01:16.000Z\"/><meta itemProp=\"zhihu:visitsCount\"/><meta itemProp=\"zhihu:followerCount\" content=\"6022\"/><div data-zop-question=\"{&quot;title&quot;:&quot;特征工程到底是什么？&quot;,&quot;topics&quot;:[{&quot;name&quot;:&quot;数据挖掘&quot;,&quot;id&quot;:&quot;19553534&quot;},{&quot;name&quot;:&quot;数据分析&quot;,&quot;id&quot;:&quot;19559424&quot;},{&quot;name&quot;:&quot;推荐系统&quot;,&quot;id&quot;:&quot;19563024&quot;},{&quot;name&quot;:&quot;大数据&quot;,&quot;id&quot;:&quot;19740929&quot;},{&quot;name&quot;:&quot;特征选择&quot;,&quot;id&quot;:&quot;19809410&quot;}],&quot;id&quot;:29316149,&quot;isEditable&quot;:false}\"><div><div class=\"QuestionHeader\"><div class=\"QuestionHeader-content\"><div class=\"QuestionHeader-main\"><div class=\"QuestionHeader-tags\"><div class=\"QuestionHeader-topics\"><div class=\"Tag QuestionTopic\"><span class=\"Tag-content\"><a class=\"TopicLink\" href=\"//www.zhihu.com/topic/19553534\" target=\"_blank\"><div class=\"Popover\"><div id=\"null-toggle\" aria-haspopup=\"true\" aria-expanded=\"false\" aria-owns=\"null-content\">数据挖掘</div></div></a></span></div><div class=\"Tag QuestionTopic\"><span class=\"Tag-content\"><a class=\"TopicLink\" href=\"//www.zhihu.com/topic/19559424\" target=\"_blank\"><div class=\"Popover\"><div id=\"null-toggle\" aria-haspopup=\"true\" aria-expanded=\"false\" aria-owns=\"null-content\">数据分析</div></div></a></span></div><div class=\"Tag QuestionTopic\"><span class=\"Tag-content\"><a class=\"TopicLink\" href=\"//www.zhihu.com/topic/19563024\" target=\"_blank\"><div class=\"Popover\"><div id=\"null-toggle\" aria-haspopup=\"true\" aria-expanded=\"false\" aria-owns=\"null-content\">推荐系统</div></div></a></span></div><div class=\"Tag QuestionTopic\"><span class=\"Tag-content\"><a class=\"TopicLink\" href=\"//www.zhihu.com/topic/19740929\" target=\"_blank\"><div class=\"Popover\"><div id=\"null-toggle\" aria-haspopup=\"true\" aria-expanded=\"false\" aria-owns=\"null-content\">大数据</div></div></a></span></div><div class=\"Tag QuestionTopic\"><span class=\"Tag-content\"><a class=\"TopicLink\" href=\"//www.zhihu.com/topic/19809410\" target=\"_blank\"><div class=\"Popover\"><div id=\"null-toggle\" aria-haspopup=\"true\" aria-expanded=\"false\" aria-owns=\"null-content\">特征选择</div></div></a></span></div></div></div><h1 class=\"QuestionHeader-title\">特征工程到底是什么？</h1><div><div class=\"QuestionHeader-detail\"><div class=\"QuestionRichText QuestionRichText--expandable QuestionRichText--collapsed\"><div><span class=\"RichText ztext\" itemProp=\"text\">大概知道一些，但是不是很清楚领域内的知识是怎么帮助特征工程的呢？如果有例子就更好了。</span><button type=\"button\" class=\"Button QuestionRichText-more Button--plain\">显示全部 <span style=\"display:inline-flex;align-items:center\">\\u200b<svg class=\"Zi Zi--ArrowDown\" fill=\"currentColor\" viewBox=\"0 0 24 24\" width=\"24\" height=\"24\"><path d=\"M12 13L8.285 9.218a.758.758 0 0 0-1.064 0 .738.738 0 0 0 0 1.052l4.249 4.512a.758.758 0 0 0 1.064 0l4.246-4.512a.738.738 0 0 0 0-1.052.757.757 0 0 0-1.063 0L12.002 13z\" fill-rule=\"evenodd\"></path></svg></span></button></div></div></div></div></div><div class=\"QuestionHeader-side\"><div class=\"QuestionHeader-follow-status\"><div class=\"QuestionFollowStatus\"><div class=\"NumberBoard QuestionFollowStatus-counts NumberBoard--divider\"><div class=\"NumberBoard-item\"><div class=\"NumberBoard-itemInner\"><div class=\"NumberBoard-itemName\">关注者</div><strong class=\"NumberBoard-itemValue\" title=\"6022\">6,022</strong></div></div><div class=\"NumberBoard-item\"><div class=\"NumberBoard-itemInner\"><div class=\"NumberBoard-itemName\">被浏览</div><strong class=\"NumberBoard-itemValue\" title=\"403390\">403,390</strong></div></div></div></div></div></div></div><div class=\"QuestionHeader-footer\"><div class=\"QuestionHeader-footer-inner\"><div class=\"QuestionHeader-main QuestionHeader-footer-main\"><div class=\"QuestionButtonGroup\"><button type=\"button\" class=\"Button FollowButton Button--primary Button--blue\">关注问题</button><button type=\"button\" class=\"Button Button--blue\"><span style=\"display:inline-flex;align-items:center\">\\u200b<svg class=\"Zi Zi--Edit QuestionButton-icon\" fill=\"currentColor\" viewBox=\"0 0 24 24\" width=\"16\" height=\"16\"><path d=\"M4.076 16.966a4.19 4.19 0 0 1 1.05-1.76l8.568-8.569a.524.524 0 0 1 .741 0l2.928 2.927a.524.524 0 0 1 0 .74l-8.568 8.57c-.49.49-1.096.852-1.761 1.051l-3.528 1.058a.394.394 0 0 1-.49-.488l1.06-3.53zM20.558 4.83c.59.59.59 1.546 0 2.136l-1.693 1.692a.503.503 0 0 1-.712 0l-2.812-2.812a.504.504 0 0 1 0-.712l1.693-1.693a1.51 1.51 0 0 1 2.135 0l1.389 1.389z\"></path></svg></span>写回答</button></div><div class=\"QuestionHeaderActions\"><button style=\"margin-right:16px\" type=\"button\" class=\"Button Button--grey Button--withIcon Button--withLabel\"><span style=\"display:inline-flex;align-items:center\">\\u200b<svg class=\"Zi Zi--Invite Button-zi\" fill=\"currentColor\" viewBox=\"0 0 24 24\" width=\"1.2em\" height=\"1.2em\"><path d=\"M4 10V8a1 1 0 1 1 2 0v2h2a1 1 0 0 1 0 2H6v2a1 1 0 0 1-2 0v-2H2a1 1 0 0 1 0-2h2zm10.455 2c-2.21 0-4-1.79-4-4s1.79-4 4-4 4 1.79 4 4-1.79 4-4 4zm-7 6c0-2.66 4.845-4 7.272-4C17.155 14 22 15.34 22 18v1.375c0 .345-.28.625-.625.625H8.08a.625.625 0 0 1-.625-.625V18z\" fill-rule=\"evenodd\"></path></svg></span>邀请回答</button><div class=\"QuestionHeader-Comment\"><button type=\"button\" class=\"Button Button--plain Button--withIcon Button--withLabel\"><span style=\"display:inline-flex;align-items:center\">\\u200b<svg class=\"Zi Zi--Comment Button-zi\" fill=\"currentColor\" viewBox=\"0 0 24 24\" width=\"1.2em\" height=\"1.2em\"><path d=\"M10.241 19.313a.97.97 0 0 0-.77.2 7.908 7.908 0 0 1-3.772 1.482.409.409 0 0 1-.38-.637 5.825 5.825 0 0 0 1.11-2.237.605.605 0 0 0-.227-.59A7.935 7.935 0 0 1 3 11.25C3 6.7 7.03 3 12 3s9 3.7 9 8.25-4.373 9.108-10.759 8.063z\" fill-rule=\"evenodd\"></path></svg></span>添加评论</button></div><div class=\"Popover ShareMenu\"><div class=\"ShareMenu-toggler\" id=\"null-toggle\" aria-haspopup=\"true\" aria-expanded=\"false\" aria-owns=\"null-content\"><button type=\"button\" class=\"Button Button--plain Button--withIcon Button--withLabel\"><span style=\"display:inline-flex;align-items:center\">\\u200b<svg class=\"Zi Zi--Share Button-zi\" fill=\"currentColor\" viewBox=\"0 0 24 24\" width=\"1.2em\" height=\"1.2em\"><path d=\"M2.931 7.89c-1.067.24-1.275 1.669-.318 2.207l5.277 2.908 8.168-4.776c.25-.127.477.198.273.39L9.05 14.66l.927 5.953c.18 1.084 1.593 1.376 2.182.456l9.644-15.242c.584-.892-.212-2.029-1.234-1.796L2.93 7.89z\" fill-rule=\"evenodd\"></path></svg></span>分享</button></div></div><div class=\"Popover\"><button aria-label=\"更多\" type=\"button\" id=\"null-toggle\" aria-haspopup=\"true\" aria-expanded=\"false\" aria-owns=\"null-content\" class=\"Button Button--plain Button--withIcon Button--iconOnly\"><span style=\"display:inline-flex;align-items:center\">\\u200b<svg class=\"Zi Zi--Dots Button-zi\" fill=\"currentColor\" viewBox=\"0 0 24 24\" width=\"1.2em\" height=\"1.2em\"><path d=\"M5 14a2 2 0 1 1 0-4 2 2 0 0 1 0 4zm7 0a2 2 0 1 1 0-4 2 2 0 0 1 0 4zm7 0a2 2 0 1 1 0-4 2 2 0 0 1 0 4z\" fill-rule=\"evenodd\"></path></svg></span></button></div></div><div class=\"QuestionHeader-actions\"></div></div></div></div></div></div><div><div class=\"Sticky\"></div></div></div><div class=\"Question-main\"><div class=\"Question-mainColumn\"><div><div id=\"QuestionAnswers-answers\" class=\"QuestionAnswers-answers\" data-zop-feedlistmap=\"0,0,1,0\"><div class=\"Card AnswersNavWrapper\"><div class=\"ListShortcut\"><div class=\"List\"><div class=\"List-header\"><h4 class=\"List-headerText\"><span>38<!-- --> 个回答</span></h4><div class=\"List-headerOptions\"><div class=\"Popover\"><button role=\"combobox\" aria-expanded=\"false\" id=\"null-toggle\" aria-haspopup=\"true\" aria-owns=\"null-content\" type=\"button\" class=\"Button InputLike InputButton Select-button Select-plainButton Button--plain\">默认排序<svg class=\"Zi Zi--Select Select-arrow\" fill=\"currentColor\" viewBox=\"0 0 24 24\" width=\"24\" height=\"24\"><path d=\"M12 16.183l2.716-2.966a.757.757 0 0 1 1.064.001.738.738 0 0 1 0 1.052l-3.247 3.512a.758.758 0 0 1-1.064 0L8.22 14.27a.738.738 0 0 1 0-1.052.758.758 0 0 1 1.063 0L12 16.183zm0-9.365L9.284 9.782a.758.758 0 0 1-1.064 0 .738.738 0 0 1 0-1.052l3.248-3.512a.758.758 0 0 1 1.065 0L15.78 8.73a.738.738 0 0 1 0 1.052.757.757 0 0 1-1.063.001L12 6.818z\" fill-rule=\"evenodd\"></path></svg></button></div></div></div><div><div class=\"\"><div class=\"List-item\" tabindex=\"0\"><div class=\"ContentItem AnswerItem\" data-za-index=\"0\" data-zop=\"{&quot;authorName&quot;:&quot;包大人&quot;,&quot;itemId&quot;:607394337,&quot;title&quot;:&quot;特征工程到底是什么？&quot;,&quot;type&quot;:&quot;answer&quot;}\" name=\"607394337\" itemProp=\"acceptedAnswer\" itemType=\"http://schema.org/Answer\" itemscope=\"\"><div class=\"ContentItem-meta\"><div class=\"AuthorInfo AnswerItem-authorInfo AnswerItem-authorInfo--related\" itemProp=\"author\" itemscope=\"\" itemType=\"http://schema.org/Person\"><meta itemProp=\"name\" content=\"包大人\"/><meta itemProp=\"image\" content=\"https://pic3.zhimg.com/f15630e4e1a339e31c859146157f7143_l.jpg\"/><meta itemProp=\"url\" content=\"https://www.zhihu.com/people/bao-bao-12-67\"/><meta itemProp=\"zhihu:followerCount\" content=\"5380\"/><span class=\"UserLink AuthorInfo-avatarWrapper\"><div class=\"Popover\"><div id=\"null-toggle\" aria-haspopup=\"true\" aria-expanded=\"false\" aria-owns=\"null-content\"><a class=\"UserLink-link\" data-za-detail-view-element_name=\"User\" target=\"_blank\" href=\"//www.zhihu.com/people/bao-bao-12-67\"><img class=\"Avatar AuthorInfo-avatar\" width=\"38\" height=\"38\" src=\"https://pic3.zhimg.com/f15630e4e1a339e31c859146157f7143_xs.jpg\" srcSet=\"https://pic3.zhimg.com/f15630e4e1a339e31c859146157f7143_l.jpg 2x\" alt=\"包大人\"/></a></div></div></span><div class=\"AuthorInfo-content\"><div class=\"AuthorInfo-head\"><span class=\"UserLink AuthorInfo-name\"><div class=\"Popover\"><div id=\"null-toggle\" aria-haspopup=\"true\" aria-expanded=\"false\" aria-owns=\"null-content\"><a class=\"UserLink-link\" data-za-detail-view-element_name=\"User\" target=\"_blank\" href=\"//www.zhihu.com/people/bao-bao-12-67\">包大人</a></div></div><style data-emotion-css=\"1cd9gw4\">.css-1cd9gw4{margin-left:.3em;}</style><style data-emotion-css=\"1pc1mic\">.css-1pc1mic{box-sizing:border-box;margin:0;min-width:0;color:#175199;display:inline-block;width:18px;height:18px;margin-left:.3em;}</style><a href=\"https://www.zhihu.com/question/48510028\" target=\"_blank\" class=\"css-1pc1mic\" data-tooltip=\"已认证的个人\" aria-label=\"已认证的个人\"><style data-emotion-css=\"18biwo\">.css-18biwo{display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}</style><span class=\"css-18biwo\">\\u200b<style data-emotion-css=\"1ifz0go\">.css-1ifz0go{overflow:visible!important;}</style><svg viewBox=\"0 0 24 24\" class=\"css-1ifz0go\" width=\"18\" height=\"18\"><svg viewBox=\"0 0 24 24\" x=\"-3\" y=\"-3\" fill=\"#FFFFFF\" width=\"30\" height=\"30\"><path d=\"M3.56231227,13.8535307 C2.40051305,12.768677 2.41398885,11.0669203 3.59484487,9.99979213 L3.59222085,9.99654885 C4.26730143,9.45036719 4.79446755,8.21005186 4.7184197,7.34453784 L4.72305873,7.34412719 C4.66942824,5.75539997 5.8824188,4.56066914 7.47188965,4.64242381 L7.47229112,4.6386236 C8.33515314,4.72977993 9.58467253,4.22534048 10.1426329,3.55925173 L10.1462611,3.56228565 C11.2316055,2.40008701 12.9353108,2.41394456 14.0015072,3.59634088 L14.0047263,3.59374004 C14.5498229,4.26841874 15.7896857,4.79521622 16.6545744,4.71844347 L16.6549836,4.72304294 C18.245027,4.66894057 19.4396947,5.88213996 19.3575031,7.47241135 L19.3623099,7.47292747 C19.2704388,8.3358681 19.7742711,9.58421483 20.4407199,10.1424506 L20.437686,10.1460789 C21.5997217,11.2312209 21.5860695,12.9345218 20.4042441,14.0007396 L20.4072865,14.0045125 C19.7325967,14.5495925 19.2055209,15.7896954 19.2815865,16.6561959 L19.2770449,16.6565978 C19.3315454,18.2453037 18.1173775,19.4393568 16.5274188,19.3571512 L16.5269029,19.3619539 C15.6647098,19.270083 14.415408,19.7741709 13.8573671,20.4403558 L13.8537409,20.4373235 C12.76842,21.5995708 11.0650432,21.5864553 9.99899434,20.4039226 L9.99527367,20.406923 C9.45025436,19.7323399 8.21017638,19.2051872 7.34461983,19.2812352 L7.344304,19.2776405 C5.75448683,19.3312904 4.55977145,18.1170085 4.64254978,16.527117 L4.63769921,16.5265942 C4.72957031,15.6644394 4.22547659,14.4151814 3.55928015,13.8571569 L3.56231227,13.8535307 Z\"></path></svg><path d=\"M2.63951518,13.3895441 C3.70763333,14.2842292 4.44777637,16.1226061 4.30075305,17.5023312 L4.32211542,17.3063047 C4.17509209,18.6910561 5.17786655,19.7063729 6.5613937,19.5844846 L6.364106,19.6008202 C7.75140298,19.4789319 9.57474349,20.2554985 10.4468305,21.3349009 L10.3224262,21.1803415 C11.1982831,22.2647703 12.6257916,22.2723098 13.5167278,21.2079863 L13.3898102,21.3600325 C14.2845162,20.2919393 16.1229361,19.5518136 17.5026934,19.6988334 L17.3054057,19.6774716 C18.6914461,19.8244915 19.7067866,18.8217404 19.5836389,17.4395022 L19.6012314,17.6367853 C19.4793403,16.2482641 20.255925,14.4249662 21.3353526,13.5528995 L21.1807897,13.677301 C22.2639871,12.8014646 22.2727834,11.3739894 21.2084351,10.483074 L21.3604848,10.6099886 C20.2923667,9.71530351 19.5522236,7.87818322 19.6992469,6.49720154 L19.6778846,6.69448464 C19.8249079,5.30847665 18.8221335,4.2944164 17.4386063,4.41630468 L17.635894,4.39871256 C16.248597,4.52185742 14.4252565,3.74529084 13.5531695,2.66588842 L13.6775738,2.81919121 C12.8017169,1.73601905 11.3742084,1.72722299 10.4832722,2.79154644 L10.6101898,2.63950024 C9.71548377,3.70759343 7.87706394,4.44771919 6.49730661,4.30195588 L6.69459432,4.32206116 C5.30855394,4.17504128 4.29447,5.17904888 4.41636114,6.56128713 L4.3987686,6.36400404 C4.52065973,7.75126861 3.74407501,9.57456653 2.66464737,10.4478898 L2.81921035,10.3222318 C1.73601288,11.1993248 1.72721662,12.6255433 2.79156494,13.5164587 L2.63951518,13.3895441 Z\" fill=\"#0084FF\"></path><svg class=\"Zi Zi--Check\" fill=\"#fff\" x=\"6\" y=\"6\" viewBox=\"0 0 24 24\" width=\"12\" height=\"12\"><path d=\"M10.229 17.516c-.318.327-.75.484-1.199.484-.453 0-.884-.16-1.202-.488l-4.335-4.47a1.77 1.77 0 0 1 .007-2.459 1.663 1.663 0 0 1 2.397.01l3.137 3.246 9.072-9.329a1.662 1.662 0 0 1 2.397 0c.663.681.663 1.786 0 2.466L10.23 17.516z\" fill-rule=\"evenodd\"></path></svg></svg></span></a></span></div><div class=\"AuthorInfo-detail\"><div class=\"AuthorInfo-badge\"><div class=\"AuthorInfo-badgeText\">Microsoft SDE</div></div></div></div></div><style data-emotion-css=\"tdnwmm\">.css-tdnwmm{margin:10px 0;height:42px;}</style><div class=\"css-tdnwmm\"></div><style data-emotion-css=\"h5al4j\">.css-h5al4j{box-sizing:border-box;margin:0;min-width:0;color:#8590A6;font-size:14px;margin-top:10px;margin-bottom:-4px;}</style><div class=\"css-h5al4j\"><span><span class=\"Voters\"><button type=\"button\" class=\"Button Button--plain\">2,088 人<!-- -->赞同了该回答</button></span></span></div></div><meta itemProp=\"image\"/><meta itemProp=\"upvoteCount\" content=\"2088\"/><meta itemProp=\"url\" content=\"https://www.zhihu.com/question/29316149/answer/607394337\"/><meta itemProp=\"dateCreated\" content=\"2019-02-24T14:35:52.000Z\"/><meta itemProp=\"dateModified\" content=\"2020-03-23T10:13:02.000Z\"/><meta itemProp=\"commentCount\" content=\"39\"/><div class=\"RichContent RichContent--unescapable\"><div class=\"RichContent-inner\"><span class=\"RichText ztext CopyrightRichText-richText\" itemProp=\"text\"><p>2020-03-20 更新，<b>很多资料下载需要翻墙，整理好在下面公众号 i数据智能 里面，回复“特征工程”</b>即可获取：</p><p><a href=\"https://link.zhihu.com/?target=http%3A//weixin.qq.com/r/PkT17dnEw4S8rZ2O9xEs\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">weixin.qq.com/r/PkT17dn</span><span class=\"invisible\">Ew4S8rZ2O9xEs</span><span class=\"ellipsis\"></span></a> (二维码自动识别)</p><p>特征工程是机器学习，甚至是深度学习中最为重要的一部分，<b>也是课本上最不愿意讲的一部分</b>，特征工程往往是打开数据密码的钥匙，<b>是数据科学中最有创造力的一部分</b>。因为往往和具体的数据相结合，很难优雅地系统地讲好。所以课本上会讲一下理论知识比较扎实的<b>归一化</b>，<b>降维</b>等部分，而忽略一些很dirty hand的特征工程技巧。</p><p>什么是特征工程呢？一个非常简单的例子，现在出一非常简答的二分类问题题，请你使用逻辑回归，设计一个身材分类器。输入数据X:身高和体重 ，标签为Y:身材等级（胖，不胖）。显然，不能单纯的根据体重来判断一个人胖不胖，姚明很重，他胖吗？显然不是。针对这个问题，一个非常经典的特征工程是，<b>BMI指数，BMI=体重/(身高^2)。这样，通过BMI指数，</b>就能非常显然地帮助我们，刻画一个人身材如何。<b>甚至，你可以抛弃原始的体重和身高数据。</b></p><p>所以说，特征工程就是<b>通过X，创造新的X&#39;。</b>基本的操作包括<b>，衍生（升维），筛选（降维）。说</b>起来简单，实际中，衍生和筛选都是困难重重，甚至需要非常专业的专家知识。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/50/v2-2fa04777f16f18adaf9a0181fac3fa53_hd.jpg\" data-rawwidth=\"924\" data-rawheight=\"329\" data-size=\"normal\" data-caption=\"\" data-default-watermark-src=\"https://pic2.zhimg.com/50/v2-75b5986059835dcc66aa13c1f823c74b_hd.jpg\" class=\"origin_image zh-lightbox-thumb\" width=\"924\" data-original=\"https://pic1.zhimg.com/v2-2fa04777f16f18adaf9a0181fac3fa53_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;924&#39; height=&#39;329&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"924\" data-rawheight=\"329\" data-size=\"normal\" data-caption=\"\" data-default-watermark-src=\"https://pic2.zhimg.com/50/v2-75b5986059835dcc66aa13c1f823c74b_hd.jpg\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"924\" data-original=\"https://pic1.zhimg.com/v2-2fa04777f16f18adaf9a0181fac3fa53_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/50/v2-2fa04777f16f18adaf9a0181fac3fa53_hd.jpg\"/></figure><p>当然，人类的进化也就是一直在变懒，深度学习的兴起，让很多人相信，deep learning，作为一种强大的<b>自动化特征工程工具，</b>能够自动学习各种低级和高级的特征。至少在视觉领域，似乎没有人愿意提起SIFT算子，仿佛被翻进了历史的另一页。<b>Embedding为代表的表征学习，在大规模稀疏ID问题和多值ID问题上，</b>似乎也能搞定自动化特征工程，于是互大家看到很多，各种千奇百怪的文本分类模型，CTR预估模型，似乎就是Embedding后面一顿盘他，最终转化成分类问题，似乎并不需要特征工程这门祖传的手艺了。</p><p><b>然而真的是这样吗？</b></p><p>讲一个比较复杂的例子，<b>如果你对speech稍有了解，或者</b>做过<b>说话人验证/声纹识别（SVR）任务</b>，你会知道，有一种特征工程叫做<b>MFCC </b>特征，现在解决说话人本身特性的问题，前端还是无法离开MFCC，而我认为MFCC是一种非常有代表性的饱含了专家知识的特征工程，感兴趣的同学可以了解一下：<a href=\"https://link.zhihu.com/?target=https%3A//zh.wikipedia.org/wiki/%25E6%25A2%2585%25E5%25B0%2594%25E9%25A2%2591%25E7%258E%2587%25E5%2580%2592%25E8%25B0%25B1%25E7%25B3%25BB%25E6%2595%25B0\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">MFCC(梅尔频率倒谱系数)</a></p><p>关于特征工程的系统部分前面的回答已经总结的很好了。如果你读过很多博客，书籍，你会发现，特征工程的<b>实践和理论是高度割裂的</b>，在这里，我要推荐一门coursera的课程，<a href=\"https://link.zhihu.com/?target=https%3A//www.coursera.org/specializations/aml%3FsiteID%3DlVarvwc5BD0-AqkGMb7JzoCMW0Np1uLfCA%26utm_campaign%3DlVarvwc5BD0%26utm_content%3D2%26utm_medium%3Dpartners%26utm_source%3Dlinkshare\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Advanced Machine Learning（高级机器学习）</a><b>，</b>有Kazanova大神授课的部分，（顶级Kaggle GM，巅峰TOP3），这里面有很多来源于实践中的很多技巧。</p><p>Kaggle上有一句非常经典的话，<b>数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已</b>，而这恰恰是课堂上最为缺失，一门需要在实践中学习的手艺。</p><p>其实是有一本讲特征工程的书的，不过并不推荐，因为实在是太简单了，可能看了就跟喝了一杯白开水一样。链接在下面</p><a target=\"_blank\" href=\"https://link.zhihu.com/?target=https%3A//perso.limsi.fr/annlor/enseignement/ensiie/Feature_Engineering_for_Machine_Learning.pdf\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\"LinkCard LinkCard--noImage\"><span class=\"LinkCard-content\"><span class=\"LinkCard-text\"><span class=\"LinkCard-title\" data-text=\"true\">Feature_Engineering_for_Machine_Learning.pdf</span><span class=\"LinkCard-meta\"><span style=\"display:inline-flex;align-items:center\">\\u200b<svg class=\"Zi Zi--InsertLink\" fill=\"currentColor\" viewBox=\"0 0 24 24\" width=\"17\" height=\"17\"><path d=\"M6.77 17.23c-.905-.904-.94-2.333-.08-3.193l3.059-3.06-1.192-1.19-3.059 3.058c-1.489 1.489-1.427 3.954.138 5.519s4.03 1.627 5.519.138l3.059-3.059-1.192-1.192-3.059 3.06c-.86.86-2.289.824-3.193-.08zm3.016-8.673l1.192 1.192 3.059-3.06c.86-.86 2.289-.824 3.193.08.905.905.94 2.334.08 3.194l-3.059 3.06 1.192 1.19 3.059-3.058c1.489-1.489 1.427-3.954-.138-5.519s-4.03-1.627-5.519-.138L9.786 8.557zm-1.023 6.68c.33.33.863.343 1.177.029l5.34-5.34c.314-.314.3-.846-.03-1.176-.33-.33-.862-.344-1.176-.03l-5.34 5.34c-.314.314-.3.846.03 1.177z\" fill-rule=\"evenodd\"></path></svg></span>perso.limsi.fr</span></span><span class=\"LinkCard-imageCell\"><div class=\"LinkCard-image LinkCard-image--default\"><svg class=\"Zi Zi--Browser\" fill=\"currentColor\" viewBox=\"0 0 24 24\" width=\"32\" height=\"32\"><path d=\"M11.991 3C7.023 3 3 7.032 3 12s4.023 9 8.991 9C16.968 21 21 16.968 21 12s-4.032-9-9.009-9zm6.237 5.4h-2.655a14.084 14.084 0 0 0-1.242-3.204A7.227 7.227 0 0 1 18.228 8.4zM12 4.836A12.678 12.678 0 0 1 13.719 8.4h-3.438A12.678 12.678 0 0 1 12 4.836zM5.034 13.8A7.418 7.418 0 0 1 4.8 12c0-.621.09-1.224.234-1.8h3.042A14.864 14.864 0 0 0 7.95 12c0 .612.054 1.206.126 1.8H5.034zm.738 1.8h2.655a14.084 14.084 0 0 0 1.242 3.204A7.188 7.188 0 0 1 5.772 15.6zm2.655-7.2H5.772a7.188 7.188 0 0 1 3.897-3.204c-.54.999-.954 2.079-1.242 3.204zM12 19.164a12.678 12.678 0 0 1-1.719-3.564h3.438A12.678 12.678 0 0 1 12 19.164zm2.106-5.364H9.894A13.242 13.242 0 0 1 9.75 12c0-.612.063-1.215.144-1.8h4.212c.081.585.144 1.188.144 1.8 0 .612-.063 1.206-.144 1.8zm.225 5.004c.54-.999.954-2.079 1.242-3.204h2.655a7.227 7.227 0 0 1-3.897 3.204zm1.593-5.004c.072-.594.126-1.188.126-1.8 0-.612-.054-1.206-.126-1.8h3.042c.144.576.234 1.179.234 1.8s-.09 1.224-.234 1.8h-3.042z\"></path></svg></div></span></span></a><p>长这个样子</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/50/v2-50420df02b8ae50c53b642ad411bd2d0_hd.jpg\" data-rawwidth=\"473\" data-rawheight=\"620\" data-size=\"normal\" data-caption=\"\" data-default-watermark-src=\"https://pic2.zhimg.com/50/v2-57367ab87b47d6070e8d8fcee3d88956_hd.jpg\" class=\"origin_image zh-lightbox-thumb\" width=\"473\" data-original=\"https://pic1.zhimg.com/v2-50420df02b8ae50c53b642ad411bd2d0_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;473&#39; height=&#39;620&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"473\" data-rawheight=\"620\" data-size=\"normal\" data-caption=\"\" data-default-watermark-src=\"https://pic2.zhimg.com/50/v2-57367ab87b47d6070e8d8fcee3d88956_hd.jpg\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"473\" data-original=\"https://pic1.zhimg.com/v2-50420df02b8ae50c53b642ad411bd2d0_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/50/v2-50420df02b8ae50c53b642ad411bd2d0_hd.jpg\"/></figure><p>下面有一个非常神奇的slide，非常推荐，非常好，真香。</p><a target=\"_blank\" href=\"https://link.zhihu.com/?target=https%3A//www.slideshare.net/HJvanVeen/feature-engineering-72376750\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\"LinkCard LinkCard--noImage\"><span class=\"LinkCard-content\"><span class=\"LinkCard-text\"><span class=\"LinkCard-title\" data-text=\"true\">feature-engineering</span><span class=\"LinkCard-meta\"><span style=\"display:inline-flex;align-items:center\">\\u200b<svg class=\"Zi Zi--InsertLink\" fill=\"currentColor\" viewBox=\"0 0 24 24\" width=\"17\" height=\"17\"><path d=\"M6.77 17.23c-.905-.904-.94-2.333-.08-3.193l3.059-3.06-1.192-1.19-3.059 3.058c-1.489 1.489-1.427 3.954.138 5.519s4.03 1.627 5.519.138l3.059-3.059-1.192-1.192-3.059 3.06c-.86.86-2.289.824-3.193-.08zm3.016-8.673l1.192 1.192 3.059-3.06c.86-.86 2.289-.824 3.193.08.905.905.94 2.334.08 3.194l-3.059 3.06 1.192 1.19 3.059-3.058c1.489-1.489 1.427-3.954-.138-5.519s-4.03-1.627-5.519-.138L9.786 8.557zm-1.023 6.68c.33.33.863.343 1.177.029l5.34-5.34c.314-.314.3-.846-.03-1.176-.33-.33-.862-.344-1.176-.03l-5.34 5.34c-.314.314-.3.846.03 1.177z\" fill-rule=\"evenodd\"></path></svg></span>www.slideshare.net</span></span><span class=\"LinkCard-imageCell\"><div class=\"LinkCard-image LinkCard-image--default\"><svg class=\"Zi Zi--Browser\" fill=\"currentColor\" viewBox=\"0 0 24 24\" width=\"32\" height=\"32\"><path d=\"M11.991 3C7.023 3 3 7.032 3 12s4.023 9 8.991 9C16.968 21 21 16.968 21 12s-4.032-9-9.009-9zm6.237 5.4h-2.655a14.084 14.084 0 0 0-1.242-3.204A7.227 7.227 0 0 1 18.228 8.4zM12 4.836A12.678 12.678 0 0 1 13.719 8.4h-3.438A12.678 12.678 0 0 1 12 4.836zM5.034 13.8A7.418 7.418 0 0 1 4.8 12c0-.621.09-1.224.234-1.8h3.042A14.864 14.864 0 0 0 7.95 12c0 .612.054 1.206.126 1.8H5.034zm.738 1.8h2.655a14.084 14.084 0 0 0 1.242 3.204A7.188 7.188 0 0 1 5.772 15.6zm2.655-7.2H5.772a7.188 7.188 0 0 1 3.897-3.204c-.54.999-.954 2.079-1.242 3.204zM12 19.164a12.678 12.678 0 0 1-1.719-3.564h3.438A12.678 12.678 0 0 1 12 19.164zm2.106-5.364H9.894A13.242 13.242 0 0 1 9.75 12c0-.612.063-1.215.144-1.8h4.212c.081.585.144 1.188.144 1.8 0 .612-.063 1.206-.144 1.8zm.225 5.004c.54-.999.954-2.079 1.242-3.204h2.655a7.227 7.227 0 0 1-3.897 3.204zm1.593-5.004c.072-.594.126-1.188.126-1.8 0-.612-.054-1.206-.126-1.8h3.042c.144.576.234 1.179.234 1.8s-.09 1.224-.234 1.8h-3.042z\"></path></svg></div></span></span></a><p>基本涵盖了最基本的特征工程方法（套路）。</p><p>大致如下：</p><p><b>1.类别特征</b></p><ul><li>one-hot encoding</li><li>hash encoding</li><li>label encoding</li><li>count encoding</li><li>label-count encoding</li><li>target encoding</li><li>category embedding</li><li>Nan encoding</li><li>polynomial encoding</li><li>expansion encoding</li><li>consolidation encoding</li></ul><p><b>2.数值特征</b></p><ul><li>rounding</li><li>binning</li><li>scaling</li><li>imputation</li><li>interactions</li><li>no linear encoding</li><li>row statistics</li></ul><p><b>4.时间特征</b></p><p><b>5.空间特征</b></p><p><b>6.自然语言处理</b></p><p><b>7.深度学习/NN</b></p><p><b>8.Leakage</b></p><p>当然以上并不全，还有很多非常经典的聚合统计，Graph上的特征工程等等。</p><p>文本相似性是一个非常好的体味特征工程威力，对比深度学习效果的一个问题，不妨去看看。</p><a target=\"_blank\" href=\"https://link.zhihu.com/?target=https%3A//www.kaggle.com/c/quora-question-pairs\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\"LinkCard LinkCard--noImage\"><span class=\"LinkCard-content\"><span class=\"LinkCard-text\"><span class=\"LinkCard-title\" data-text=\"true\">Quora Question Pairs | Kaggle</span><span class=\"LinkCard-meta\"><span style=\"display:inline-flex;align-items:center\">\\u200b<svg class=\"Zi Zi--InsertLink\" fill=\"currentColor\" viewBox=\"0 0 24 24\" width=\"17\" height=\"17\"><path d=\"M6.77 17.23c-.905-.904-.94-2.333-.08-3.193l3.059-3.06-1.192-1.19-3.059 3.058c-1.489 1.489-1.427 3.954.138 5.519s4.03 1.627 5.519.138l3.059-3.059-1.192-1.192-3.059 3.06c-.86.86-2.289.824-3.193-.08zm3.016-8.673l1.192 1.192 3.059-3.06c.86-.86 2.289-.824 3.193.08.905.905.94 2.334.08 3.194l-3.059 3.06 1.192 1.19 3.059-3.058c1.489-1.489 1.427-3.954-.138-5.519s-4.03-1.627-5.519-.138L9.786 8.557zm-1.023 6.68c.33.33.863.343 1.177.029l5.34-5.34c.314-.314.3-.846-.03-1.176-.33-.33-.862-.344-1.176-.03l-5.34 5.34c-.314.314-.3.846.03 1.177z\" fill-rule=\"evenodd\"></path></svg></span>www.kaggle.com</span></span><span class=\"LinkCard-imageCell\"><div class=\"LinkCard-image LinkCard-image--default\"><svg class=\"Zi Zi--Browser\" fill=\"currentColor\" viewBox=\"0 0 24 24\" width=\"32\" height=\"32\"><path d=\"M11.991 3C7.023 3 3 7.032 3 12s4.023 9 8.991 9C16.968 21 21 16.968 21 12s-4.032-9-9.009-9zm6.237 5.4h-2.655a14.084 14.084 0 0 0-1.242-3.204A7.227 7.227 0 0 1 18.228 8.4zM12 4.836A12.678 12.678 0 0 1 13.719 8.4h-3.438A12.678 12.678 0 0 1 12 4.836zM5.034 13.8A7.418 7.418 0 0 1 4.8 12c0-.621.09-1.224.234-1.8h3.042A14.864 14.864 0 0 0 7.95 12c0 .612.054 1.206.126 1.8H5.034zm.738 1.8h2.655a14.084 14.084 0 0 0 1.242 3.204A7.188 7.188 0 0 1 5.772 15.6zm2.655-7.2H5.772a7.188 7.188 0 0 1 3.897-3.204c-.54.999-.954 2.079-1.242 3.204zM12 19.164a12.678 12.678 0 0 1-1.719-3.564h3.438A12.678 12.678 0 0 1 12 19.164zm2.106-5.364H9.894A13.242 13.242 0 0 1 9.75 12c0-.612.063-1.215.144-1.8h4.212c.081.585.144 1.188.144 1.8 0 .612-.063 1.206-.144 1.8zm.225 5.004c.54-.999.954-2.079 1.242-3.204h2.655a7.227 7.227 0 0 1-3.897 3.204zm1.593-5.004c.072-.594.126-1.188.126-1.8 0-.612-.054-1.206-.126-1.8h3.042c.144.576.234 1.179.234 1.8s-.09 1.224-.234 1.8h-3.042z\"></path></svg></div></span></span></a><p>AutoML，抛开比较火热的NAS研究方向，对自动化特征工程也有一些研究，果然人类的本质是在变懒，关于AutoML是如何解决自动化特征工程的，可以看我专栏里的AutoML概览，在很多场景下，尤其是匿名，数据丧失原始含义的情况下，能不能一把梭就看automl或者你神奇的大脑去解谜了。</p><p>再补充一些资料：</p><p>书籍：</p><p><a href=\"https://link.zhihu.com/?target=http%3A//www.amazon.com/dp/0792381963%3Ftag%3Dinspiredalgor-20\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Amazon.com: Feature Extraction, Construction and Selection: A Data Mining Perspective (The Springer International Series in Engineering and Computer Science) (9780792381969): Huan Liu, Hiroshi Motoda: Books</a></p><p><a href=\"https://link.zhihu.com/?target=http%3A//www.amazon.com/dp/3540354875%3Ftag%3Dinspiredalgor-20\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Feature Extraction: Foundations and Applications (Studies in Fuzziness and Soft Computing): Isabelle Guyon, Steve Gunn, Masoud Nikravesh, Lofti A. Zadeh: 9783540354871: Amazon.com: Books</a></p><p><a href=\"https://link.zhihu.com/?target=http%3A//www.amazon.com/dp/0123965497%3Ftag%3Dinspiredalgor-20\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Mark Nixon: 9780123965493: Amazon.com: Books</a></p><p><a href=\"https://link.zhihu.com/?target=http%3A//www.amazon.com/dp/079238198X%3Ftag%3Dinspiredalgor-20\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Huan Liu, Hiroshi Motoda: 9780792381983: Amazon.com: Books</a></p><p><a href=\"https://link.zhihu.com/?target=http%3A//www.amazon.com/dp/1584888784%3Ftag%3Dinspiredalgor-20\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Amazon.com: Computational Methods of Feature Selection (Chapman &amp; Hall/CRC Data Mining and Knowledge Discovery Series) (9781584888789): Huan Liu, Hiroshi Motoda: Books</a></p><p>Slides:</p><p><a href=\"https://link.zhihu.com/?target=http%3A//kti.tugraz.at/staff/denis/courses/kddm1/featureengineering.pdf\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Feature Engineering (PDF), Knowledge Discover and Data Mining 1, by Roman Kern</a></p><p><a href=\"https://link.zhihu.com/?target=http%3A//www.cs.berkeley.edu/~jordan/courses/294-fall09/lectures/feature/slides.pdf\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Feature Engineering and Selection</a></p><p><a href=\"https://link.zhihu.com/?target=http%3A//www.cs.princeton.edu/courses/archive/spring10/cos424/slides/18-feat.pdf\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Feature Engineering</a></p><p><a href=\"https://link.zhihu.com/?target=http%3A//pslcdatashop.org/KDDCup/workshop/papers/kdd2010ntu.pdf\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">KDD CUP 2010年冠军的论文</a></p><p class=\"ztext-empty-paragraph\"><br/></p><p>课程</p><p><a href=\"https://link.zhihu.com/?target=http%3A//www.cs.berkeley.edu/~jordan/courses/294-fall09/lectures/feature/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Feature selection. berkeley</a></p><p><a href=\"https://link.zhihu.com/?target=http%3A//kti.tugraz.at/staff/denis/courses/kddm1/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Knowledge Discovery and Data Mining 1</a></p><p><a href=\"https://link.zhihu.com/?target=https%3A//www.youtube.com/watch%3Fv%3DdrUToKxEAUA\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">油管上CMU授课的特征工程</a></p><p><a href=\"https://link.zhihu.com/?target=http%3A//www.columbia.edu/~rsb2162/FES2013/materials.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">FES.columbia</a></p><p>博客</p><p><a href=\"https://link.zhihu.com/?target=https%3A//machinelearningmastery.com/discover-feature-engineering-how-to-engineer-features-and-how-to-get-good-at-it/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">discover-feature-engineering-how-to-engineer-features-and-how-to-get-good-at-it</a></p></span></div><div><div class=\"ContentItem-time\"><a target=\"_blank\" href=\"//www.zhihu.com/question/29316149/answer/607394337\"><span data-tooltip=\"发布于 2019-02-24 22:35\">编辑于 2020-03-23</span></a></div></div><div class=\"ContentItem-actions RichContent-actions\"><span><button aria-label=\"赞同 2088 \" type=\"button\" class=\"Button VoteButton VoteButton--up\"><span style=\"display:inline-flex;align-items:center\">\\u200b<svg class=\"Zi Zi--TriangleUp VoteButton-TriangleUp\" fill=\"currentColor\" viewBox=\"0 0 24 24\" width=\"10\" height=\"10\"><path d=\"M2 18.242c0-.326.088-.532.237-.896l7.98-13.203C10.572 3.57 11.086 3 12 3c.915 0 1.429.571 1.784 1.143l7.98 13.203c.15.364.236.57.236.896 0 1.386-.875 1.9-1.955 1.9H3.955c-1.08 0-1.955-.517-1.955-1.9z\" fill-rule=\"evenodd\"></path></svg></span>赞同 2088</button><button aria-label=\"反对\" type=\"button\" class=\"Button VoteButton VoteButton--down\"><span style=\"display:inline-flex;align-items:center\">\\u200b<svg class=\"Zi Zi--TriangleDown\" fill=\"currentColor\" viewBox=\"0 0 24 24\" width=\"10\" height=\"10\"><path d=\"M20.044 3H3.956C2.876 3 2 3.517 2 4.9c0 .326.087.533.236.896L10.216 19c.355.571.87 1.143 1.784 1.143s1.429-.572 1.784-1.143l7.98-13.204c.149-.363.236-.57.236-.896 0-1.386-.876-1.9-1.956-1.9z\" fill-rule=\"evenodd\"></path></svg></span></button></span><button type=\"button\" class=\"Button ContentItem-action Button--plain Button--withIcon Button--withLabel\"><span style=\"display:inline-flex;align-items:center\">\\u200b<svg class=\"Zi Zi--Comment Button-zi\" fill=\"currentColor\" viewBox=\"0 0 24 24\" width=\"1.2em\" height=\"1.2em\"><path d=\"M10.241 19.313a.97.97 0 0 0-.77.2 7.908 7.908 0 0 1-3.772 1.482.409.409 0 0 1-.38-.637 5.825 5.825 0 0 0 1.11-2.237.605.605 0 0 0-.227-.59A7.935 7.935 0 0 1 3 11.25C3 6.7 7.03 3 12 3s9 3.7 9 8.25-4.373 9.108-10.759 8.063z\" fill-rule=\"evenodd\"></path></svg></span>39 条评论</button><div class=\"Popover ShareMenu ContentItem-action\"><div class=\"ShareMenu-toggler\" id=\"null-toggle\" aria-haspopup=\"true\" aria-expanded=\"false\" aria-owns=\"null-content\"><button type=\"button\" class=\"Button Button--plain Button--withIcon Button--withLabel\"><span style=\"display:inline-flex;align-items:center\">\\u200b<svg class=\"Zi Zi--Share Button-zi\" fill=\"currentColor\" viewBox=\"0 0 24 24\" width=\"1.2em\" height=\"1.2em\"><path d=\"M2.931 7.89c-1.067.24-1.275 1.669-.318 2.207l5.277 2.908 8.168-4.776c.25-.127.477.198.273.39L9.05 14.66l.927 5.953c.18 1.084 1.593 1.376 2.182.456l9.644-15.242c.584-.892-.212-2.029-1.234-1.796L2.93 7.89z\" fill-rule=\"evenodd\"></path></svg></span>分享</button></div></div><button type=\"button\" class=\"Button ContentItem-action Button--plain Button--withIcon Button--withLabel\"><span style=\"display:inline-flex;align-items:center\">\\u200b<svg class=\"Zi Zi--Star Button-zi\" fill=\"currentColor\" viewBox=\"0 0 24 24\" width=\"1.2em\" height=\"1.2em\"><path d=\"M5.515 19.64l.918-5.355-3.89-3.792c-.926-.902-.639-1.784.64-1.97L8.56 7.74l2.404-4.871c.572-1.16 1.5-1.16 2.072 0L15.44 7.74l5.377.782c1.28.186 1.566 1.068.64 1.97l-3.89 3.793.918 5.354c.219 1.274-.532 1.82-1.676 1.218L12 18.33l-4.808 2.528c-1.145.602-1.896.056-1.677-1.218z\" fill-rule=\"evenodd\"></path></svg></span>收藏</button><button type=\"button\" class=\"Button ContentItem-action Button--plain Button--withIcon Button--withLabel\"><span style=\"display:inline-flex;align-items:center\">\\u200b<svg class=\"Zi Zi--Heart Button-zi\" fill=\"currentColor\" viewBox=\"0 0 24 24\" width=\"1.2em\" height=\"1.2em\"><path d=\"M2 8.437C2 5.505 4.294 3.094 7.207 3 9.243 3 11.092 4.19 12 6c.823-1.758 2.649-3 4.651-3C19.545 3 22 5.507 22 8.432 22 16.24 13.842 21 12 21 10.158 21 2 16.24 2 8.437z\" fill-rule=\"evenodd\"></path></svg></span>喜欢</button><button data-zop-retract-question=\"true\" type=\"button\" class=\"Button ContentItem-action ContentItem-rightButton Button--plain\"><span class=\"RichContent-collapsedText\">收起</span><span style=\"display:inline-flex;align-items:center\">\\u200b<svg class=\"Zi Zi--ArrowDown ContentItem-arrowIcon is-active\" fill=\"currentColor\" viewBox=\"0 0 24 24\" width=\"24\" height=\"24\"><path d=\"M12 13L8.285 9.218a.758.758 0 0 0-1.064 0 .738.738 0 0 0 0 1.052l4.249 4.512a.758.758 0 0 0 1.064 0l4.246-4.512a.738.738 0 0 0 0-1.052.757.757 0 0 0-1.063 0L12.002 13z\" fill-rule=\"evenodd\"></path></svg></span></button></div></div></div></div><div class=\"List-item\" tabindex=\"0\"><div class=\"ContentItem AnswerItem\" data-za-index=\"1\" data-zop=\"{&quot;authorName&quot;:&quot;城东&quot;,&quot;itemId&quot;:110159647,&quot;title&quot;:&quot;特征工程到底是什么？&quot;,&quot;type&quot;:&quot;answer&quot;}\" name=\"110159647\" itemProp=\"suggestedAnswer\" itemType=\"http://schema.org/Answer\" itemscope=\"\"><div class=\"ContentItem-meta\"><div class=\"AuthorInfo AnswerItem-authorInfo AnswerItem-authorInfo--related\" itemProp=\"author\" itemscope=\"\" itemType=\"http://schema.org/Person\"><meta itemProp=\"name\" content=\"城东\"/><meta itemProp=\"image\" content=\"https://pic1.zhimg.com/34d6534efe90a4e6580254dd2ef17903_l.jpg\"/><meta itemProp=\"url\" content=\"https://www.zhihu.com/people/jasonfreak\"/><meta itemProp=\"zhihu:followerCount\" content=\"3329\"/><span class=\"UserLink AuthorInfo-avatarWrapper\"><div class=\"Popover\"><div id=\"null-toggle\" aria-haspopup=\"true\" aria-expanded=\"false\" aria-owns=\"null-content\"><a class=\"UserLink-link\" data-za-detail-view-element_name=\"User\" target=\"_blank\" href=\"//www.zhihu.com/people/jasonfreak\"><img class=\"Avatar AuthorInfo-avatar\" width=\"38\" height=\"38\" src=\"https://pic1.zhimg.com/34d6534efe90a4e6580254dd2ef17903_xs.jpg\" srcSet=\"https://pic1.zhimg.com/34d6534efe90a4e6580254dd2ef17903_l.jpg 2x\" alt=\"城东\"/></a></div></div></span><div class=\"AuthorInfo-content\"><div class=\"AuthorInfo-head\"><span class=\"UserLink AuthorInfo-name\"><div class=\"Popover\"><div id=\"null-toggle\" aria-haspopup=\"true\" aria-expanded=\"false\" aria-owns=\"null-content\"><a class=\"UserLink-link\" data-za-detail-view-element_name=\"User\" target=\"_blank\" href=\"//www.zhihu.com/people/jasonfreak\">城东</a></div></div><style data-emotion-css=\"1cd9gw4\">.css-1cd9gw4{margin-left:.3em;}</style></span></div><div class=\"AuthorInfo-detail\"><div class=\"AuthorInfo-badge\"><div class=\"ztext AuthorInfo-badgeText\">一个懒惰的人，总是想设计更智能的程序来避免做重复性工作</div></div></div></div></div><style data-emotion-css=\"tdnwmm\">.css-tdnwmm{margin:10px 0;height:42px;}</style><div class=\"css-tdnwmm\"></div><style data-emotion-css=\"h5al4j\">.css-h5al4j{box-sizing:border-box;margin:0;min-width:0;color:#8590A6;font-size:14px;margin-top:10px;margin-bottom:-4px;}</style><div class=\"css-h5al4j\"><span><span class=\"Voters\"><button type=\"button\" class=\"Button Button--plain\">2,540 人<!-- -->赞同了该回答</button></span></span></div></div><meta itemProp=\"image\"/><meta itemProp=\"upvoteCount\" content=\"2540\"/><meta itemProp=\"url\" content=\"https://www.zhihu.com/question/29316149/answer/110159647\"/><meta itemProp=\"dateCreated\" content=\"2016-07-09T02:15:16.000Z\"/><meta itemProp=\"dateModified\" content=\"2016-07-18T16:33:38.000Z\"/><meta itemProp=\"commentCount\" content=\"63\"/><div class=\"RichContent RichContent--unescapable\"><div class=\"RichContent-inner\"><span class=\"RichText ztext CopyrightRichText-richText\" itemProp=\"text\">转自我的博文：<br/><a href=\"https://link.zhihu.com/?target=http%3A//www.cnblogs.com/jasonfreak/p/5448385.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">使用sklearn做单机特征工程</a><br/><b>目录</b><p>1 特征工程是什么？<br/>2 数据预处理<br/>\\u3000\\u30002.1 无量纲化<br/>\\u3000\\u3000\\u3000\\u30002.1.1 标准化<br/>\\u3000\\u3000\\u3000\\u30002.1.2 区间缩放法<br/>\\u3000\\u3000\\u3000\\u30002.1.3 标准化与归一化的区别<br/>\\u3000\\u30002.2 对定量特征二值化<br/>\\u3000\\u30002.3 对定性特征哑编码<br/>\\u3000\\u30002.4 缺失值计算<br/>\\u3000\\u30002.5 数据变换<br/>3 特征选择<br/>\\u3000\\u30003.1 Filter<br/>\\u3000\\u3000\\u3000\\u30003.1.1 方差选择法<br/>\\u3000\\u3000\\u3000\\u30003.1.2 相关系数法<br/>\\u3000\\u3000\\u3000\\u30003.1.3 卡方检验<br/>\\u3000\\u3000\\u3000\\u30003.1.4 互信息法<br/>\\u3000\\u30003.2 Wrapper<br/>\\u3000\\u3000\\u3000\\u30003.2.1 递归特征消除法<br/>\\u3000\\u30003.3 Embedded<br/>\\u3000\\u3000\\u3000\\u30003.3.1 基于惩罚项的特征选择法<br/>\\u3000\\u3000\\u3000\\u30003.3.2 基于树模型的特征选择法<br/>4 降维<br/>\\u3000\\u30004.1 主成分分析法（PCA）<br/>\\u3000\\u30004.2 线性判别分析法（LDA）<br/>5 总结<br/>6 参考资料</p><br/><b>1 特征工程是什么？</b><p>\\u3000\\u3000有这么一句话在业界广泛流传：数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已。那特征工程到底是什么呢？顾名思义，其本质是一项工程活动，目的是最大限度地从原始数据中提取特征以供算法和模型使用。通过总结和归纳，人们认为特征工程包括以下方面：</p><figure><noscript><img src=\"https://pic2.zhimg.com/50/20e4522e6104ad71fc543cc21f402b36_hd.jpg\" data-rawwidth=\"875\" data-rawheight=\"967\" class=\"origin_image zh-lightbox-thumb\" width=\"875\" data-original=\"https://pic3.zhimg.com/20e4522e6104ad71fc543cc21f402b36_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;875&#39; height=&#39;967&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"875\" data-rawheight=\"967\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"875\" data-original=\"https://pic3.zhimg.com/20e4522e6104ad71fc543cc21f402b36_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/50/20e4522e6104ad71fc543cc21f402b36_hd.jpg\"/></figure><br/><p>\\u3000\\u3000特征处理是特征工程的核心部分，sklearn提供了较为完整的特征处理方法，包括数据预处理，特征选择，降维等。首次接触到sklearn，通常会被其丰富且方便的算法模型库吸引，但是这里介绍的特征处理库也十分强大！</p><p>\\u3000\\u3000本文中使用sklearn中的<a href=\"https://link.zhihu.com/?target=http%3A//scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html%23sklearn.datasets.load_iris\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">IRIS（鸢尾花）数据集</a>来对特征处理功能进行说明。IRIS数据集由Fisher在1936年整理，包含4个特征（Sepal.Length（花萼长度）、Sepal.Width（花萼宽度）、Petal.Length（花瓣长度）、Petal.Width（花瓣宽度）），特征值都为正浮点数，单位为厘米。目标值为鸢尾花的分类（Iris Setosa（山鸢尾）、Iris Versicolour（杂色鸢尾），Iris Virginica（维吉尼亚鸢尾））。导入IRIS数据集的代码如下：</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"kn\">from</span> <span class=\"nn\">sklearn.datasets</span> <span class=\"kn\">import</span> <span class=\"n\">load_iris</span>\\n \\n<span class=\"c1\">#导入IRIS数据集</span>\\n<span class=\"n\">iris</span> <span class=\"o\">=</span> <span class=\"n\">load_iris</span><span class=\"p\">()</span>\\n\\n<span class=\"c1\">#特征矩阵</span>\\n<span class=\"n\">iris</span><span class=\"o\">.</span><span class=\"n\">data</span>\\n\\n<span class=\"c1\">#目标向量</span>\\n<span class=\"n\">iris</span><span class=\"o\">.</span><span class=\"n\">target</span>\\n</code></pre></div><br/><b>2 数据预处理</b><p>\\u3000\\u3000通过特征提取，我们能得到未经处理的特征，这时的特征可能有以下问题：</p><ul><li>不属于同一量纲：即特征的规格不一样，不能够放在一起比较。无量纲化可以解决这一问题。</li><li>信息冗余：对于某些定量特征，其包含的有效信息为区间划分，例如学习成绩，假若只关心“及格”或不“及格”，那么需要将定量的考分，转换成“1”和“0”表示及格和未及格。二值化可以解决这一问题。</li><li>定性特征不能直接使用：某些机器学习算法和模型只能接受定量特征的输入，那么需要将定性特征转换为定量特征。最简单的方式是为每一种定性值指定一个定量值，但是这种方式过于灵活，增加了调参的工作。<a href=\"https://link.zhihu.com/?target=http%3A//www.ats.ucla.edu/stat/mult_pkg/faq/general/dummy.htm\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">通常使用哑编码的方式将定性特征转换为定量特征</a>：假设有N种定性值，则将这一个特征扩展为N种特征，当原始特征值为第i种定性值时，第i个扩展特征赋值为1，其他扩展特征赋值为0。哑编码的方式相比直接指定的方式，不用增加调参的工作，对于线性模型来说，使用哑编码后的特征可达到非线性的效果。</li><li>存在缺失值：缺失值需要补充。</li><li>信息利用率低：不同的机器学习算法和模型对数据中信息的利用是不同的，之前提到在线性模型中，使用对定性特征哑编码可以达到非线性的效果。类似地，对定量变量多项式化，或者进行其他的转换，都能达到非线性的效果。</li></ul><p>\\u3000\\u3000我们使用sklearn中的preproccessing库来进行数据预处理，可以覆盖以上问题的解决方案。</p><b>2.1 无量纲化</b><p>\\u3000\\u3000无量纲化使不同规格的数据转换到同一规格。常见的无量纲化方法有标准化和区间缩放法。标准化的前提是特征值服从正态分布，标准化后，其转换成标准正态分布。区间缩放法利用了边界值信息，将特征的取值区间缩放到某个特点的范围，例如[0, 1]等。</p><b>2.1.1 标准化</b><p>\\u3000\\u3000标准化需要计算特征的均值和标准差，公式表达为：</p><p>\\u3000\\u3000使用preproccessing库的StandardScaler类对数据进行标准化的代码如下：</p><figure><noscript><img src=\"https://pic4.zhimg.com/50/c7e852db6bd05b7bb1017b5425ffeec1_hd.jpg\" data-rawwidth=\"81\" data-rawheight=\"48\" class=\"content_image\" width=\"81\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;81&#39; height=&#39;48&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"81\" data-rawheight=\"48\" class=\"content_image lazy\" width=\"81\" data-actualsrc=\"https://pic4.zhimg.com/50/c7e852db6bd05b7bb1017b5425ffeec1_hd.jpg\"/></figure><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"kn\">from</span> <span class=\"nn\">sklearn.preprocessing</span> <span class=\"kn\">import</span> <span class=\"n\">StandardScaler</span>\\n \\n<span class=\"c1\">#标准化，返回值为标准化后的数据</span>\\n<span class=\"n\">StandardScaler</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"n\">fit_transform</span><span class=\"p\">(</span><span class=\"n\">iris</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"p\">)</span></code></pre></div><b>2.1.2 区间缩放法</b><p>\\u3000\\u3000区间缩放法的思路有多种，常见的一种为利用两个最值进行缩放，公式表达为：</p><p>\\u3000\\u3000使用preproccessing库的MinMaxScaler类对数据进行区间缩放的代码如下：</p><figure><noscript><img src=\"https://pic2.zhimg.com/50/0f119a8e8f69509c5b95ef6a8a01a809_hd.jpg\" data-rawwidth=\"119\" data-rawheight=\"52\" class=\"content_image\" width=\"119\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;119&#39; height=&#39;52&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"119\" data-rawheight=\"52\" class=\"content_image lazy\" width=\"119\" data-actualsrc=\"https://pic2.zhimg.com/50/0f119a8e8f69509c5b95ef6a8a01a809_hd.jpg\"/></figure><div class=\"highlight\"><pre><code class=\"language-text\">from sklearn.preprocessing import MinMaxScaler\\n\\n#区间缩放，返回值为缩放到[0, 1]区间的数据\\nMinMaxScaler().fit_transform(iris.data)</code></pre></div><b>2.1.3 标准化与归一化的区别</b><p>\\u3000\\u3000简单来说，标准化是依照特征矩阵的列处理数据，其通过求z-score的方法，将样本的特征值转换到同一量纲下。归一化是依照特征矩阵的行处理数据，其目的在于样本向量在点乘运算或其他核函数计算相似性时，拥有统一的标准，也就是说都转化为“单位向量”。规则为l2的归一化公式如下：</p><br/><figure><noscript><img src=\"https://pic3.zhimg.com/50/fbb2fd0a163f2fa211829b735194baac_hd.jpg\" data-rawwidth=\"113\" data-rawheight=\"57\" class=\"content_image\" width=\"113\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;113&#39; height=&#39;57&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"113\" data-rawheight=\"57\" class=\"content_image lazy\" width=\"113\" data-actualsrc=\"https://pic3.zhimg.com/50/fbb2fd0a163f2fa211829b735194baac_hd.jpg\"/></figure><br/><p>\\u3000\\u3000使用preproccessing库的Normalizer类对数据进行归一化的代码如下：</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"kn\">from</span> <span class=\"nn\">sklearn.preprocessing</span> <span class=\"kn\">import</span> <span class=\"n\">Normalizer</span>\\n\\n<span class=\"c1\">#归一化，返回值为归一化后的数据</span>\\n<span class=\"n\">Normalizer</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"n\">fit_transform</span><span class=\"p\">(</span><span class=\"n\">iris</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"p\">)</span></code></pre></div><b>2.2 对定量特征二值化</b><p>\\u3000\\u3000定量特征二值化的核心在于设定一个阈值，大于阈值的赋值为1，小于等于阈值的赋值为0，公式表达如下：</p><figure><noscript><img src=\"https://pic4.zhimg.com/50/11111244c5b69c1af6c034496a2591ad_hd.jpg\" data-rawwidth=\"159\" data-rawheight=\"41\" class=\"content_image\" width=\"159\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;159&#39; height=&#39;41&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"159\" data-rawheight=\"41\" class=\"content_image lazy\" width=\"159\" data-actualsrc=\"https://pic4.zhimg.com/50/11111244c5b69c1af6c034496a2591ad_hd.jpg\"/></figure><p>\\u3000\\u3000使用preproccessing库的Binarizer类对数据进行二值化的代码如下：</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"kn\">from</span> <span class=\"nn\">sklearn.preprocessing</span> <span class=\"kn\">import</span> <span class=\"n\">Binarizer</span>\\n\\n<span class=\"c1\">#二值化，阈值设置为3，返回值为二值化后的数据</span>\\n<span class=\"n\">Binarizer</span><span class=\"p\">(</span><span class=\"n\">threshold</span><span class=\"o\">=</span><span class=\"mi\">3</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">fit_transform</span><span class=\"p\">(</span><span class=\"n\">iris</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"p\">)</span></code></pre></div><b>2.3 对定性特征哑编码</b><p>\\u3000\\u3000由于IRIS数据集的特征皆为定量特征，故使用其目标值进行哑编码（实际上是不需要的）。使用preproccessing库的OneHotEncoder类对数据进行哑编码的代码如下：</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"kn\">from</span> <span class=\"nn\">sklearn.preprocessing</span> <span class=\"kn\">import</span> <span class=\"n\">OneHotEncoder</span>\\n\\n<span class=\"c1\">#哑编码，对IRIS数据集的目标值，返回值为哑编码后的数据</span>\\n<span class=\"n\">OneHotEncoder</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"n\">fit_transform</span><span class=\"p\">(</span><span class=\"n\">iris</span><span class=\"o\">.</span><span class=\"n\">target</span><span class=\"o\">.</span><span class=\"n\">reshape</span><span class=\"p\">((</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">,</span><span class=\"mi\">1</span><span class=\"p\">)))</span></code></pre></div><b>2.4 缺失值计算</b><p>\\u3000\\u3000由于IRIS数据集没有缺失值，故对数据集新增一个样本，4个特征均赋值为NaN，表示数据缺失。使用preproccessing库的Imputer类对数据进行缺失值计算的代码如下：</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"kn\">from</span> <span class=\"nn\">numpy</span> <span class=\"kn\">import</span> <span class=\"n\">vstack</span><span class=\"p\">,</span> <span class=\"n\">array</span><span class=\"p\">,</span> <span class=\"n\">nan</span>\\n<span class=\"kn\">from</span> <span class=\"nn\">sklearn.preprocessing</span> <span class=\"kn\">import</span> <span class=\"n\">Imputer</span>\\n\\n<span class=\"c1\">#缺失值计算，返回值为计算缺失值后的数据</span>\\n<span class=\"c1\">#参数missing_value为缺失值的表示形式，默认为NaN</span>\\n<span class=\"c1\">#参数strategy为缺失值填充方式，默认为mean（均值）</span>\\n<span class=\"n\">Imputer</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"n\">fit_transform</span><span class=\"p\">(</span><span class=\"n\">vstack</span><span class=\"p\">((</span><span class=\"n\">array</span><span class=\"p\">([</span><span class=\"n\">nan</span><span class=\"p\">,</span> <span class=\"n\">nan</span><span class=\"p\">,</span> <span class=\"n\">nan</span><span class=\"p\">,</span> <span class=\"n\">nan</span><span class=\"p\">]),</span> <span class=\"n\">iris</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"p\">)))</span></code></pre></div><b>2.5 数据变换</b><p>\\u3000\\u3000常见的数据变换有基于多项式的、基于指数函数的、基于对数函数的。4个特征，度为2的多项式转换公式如下：</p><figure><noscript><img src=\"https://pic2.zhimg.com/50/d1c57a66fad39df90b87cea330efb3f3_hd.jpg\" data-rawwidth=\"571\" data-rawheight=\"57\" class=\"origin_image zh-lightbox-thumb\" width=\"571\" data-original=\"https://pic3.zhimg.com/d1c57a66fad39df90b87cea330efb3f3_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;571&#39; height=&#39;57&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"571\" data-rawheight=\"57\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"571\" data-original=\"https://pic3.zhimg.com/d1c57a66fad39df90b87cea330efb3f3_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/50/d1c57a66fad39df90b87cea330efb3f3_hd.jpg\"/></figure><p>\\u3000\\u3000使用preproccessing库的PolynomialFeatures类对数据进行多项式转换的代码如下：</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"kn\">from</span> <span class=\"nn\">sklearn.preprocessing</span> <span class=\"kn\">import</span> <span class=\"n\">PolynomialFeatures</span>\\n\\n<span class=\"c1\">#多项式转换</span>\\n<span class=\"c1\">#参数degree为度，默认值为2</span>\\n<span class=\"n\">PolynomialFeatures</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"n\">fit_transform</span><span class=\"p\">(</span><span class=\"n\">iris</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"p\">)</span></code></pre></div><p>\\u3000\\u3000基于单变元函数的数据变换可以使用一个统一的方式完成，使用preproccessing库的FunctionTransformer对数据进行对数函数转换的代码如下：</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"kn\">from</span> <span class=\"nn\">numpy</span> <span class=\"kn\">import</span> <span class=\"n\">log1p</span>\\n<span class=\"kn\">from</span> <span class=\"nn\">sklearn.preprocessing</span> <span class=\"kn\">import</span> <span class=\"n\">FunctionTransformer</span>\\n\\n<span class=\"c1\">#自定义转换函数为对数函数的数据变换</span>\\n<span class=\"c1\">#第一个参数是单变元函数</span>\\n<span class=\"n\">FunctionTransformer</span><span class=\"p\">(</span><span class=\"n\">log1p</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">fit_transform</span><span class=\"p\">(</span><span class=\"n\">iris</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"p\">)</span></code></pre></div><b><br/>3 特征选择</b><p>\\u3000\\u3000当数据预处理完成后，我们需要选择有意义的特征输入机器学习的算法和模型进行训练。通常来说，从两个方面考虑来选择特征：</p><ul><li>特征是否发散：如果一个特征不发散，例如方差接近于0，也就是说样本在这个特征上基本上没有差异，这个特征对于样本的区分并没有什么用。</li><li>特征与目标的相关性：这点比较显见，与目标相关性高的特征，应当优选选择。除方差法外，本文介绍的其他方法均从相关性考虑。</li></ul><p>\\u3000\\u3000根据特征选择的形式又可以将特征选择方法分为3种：</p><ul><li>Filter：过滤法，按照发散性或者相关性对各个特征进行评分，设定阈值或者待选择阈值的个数，选择特征。</li><li>Wrapper：包装法，根据目标函数（通常是预测效果评分），每次选择若干特征，或者排除若干特征。</li><li>Embedded：集成法，先使用某些机器学习的算法和模型进行训练，得到各个特征的权值系数，根据系数从大到小选择特征。类似于Filter方法，但是是通过训练来确定特征的优劣。</li></ul><p>\\u3000\\u3000我们使用sklearn中的feature_selection库来进行特征选择。</p><b>3.1 Filter</b><br/><b>3.1.1 方差选择法</b><p>\\u3000\\u3000使用方差选择法，先要计算各个特征的方差，然后根据阈值，选择方差大于阈值的特征。使用feature_selection库的VarianceThreshold类来选择特征的代码如下：</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"kn\">from</span> <span class=\"nn\">sklearn.feature_selection</span> <span class=\"kn\">import</span> <span class=\"n\">VarianceThreshold</span>\\n\\n<span class=\"c1\">#方差选择法，返回值为特征选择后的数据</span>\\n<span class=\"c1\">#参数threshold为方差的阈值</span>\\n<span class=\"n\">VarianceThreshold</span><span class=\"p\">(</span><span class=\"n\">threshold</span><span class=\"o\">=</span><span class=\"mi\">3</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">fit_transform</span><span class=\"p\">(</span><span class=\"n\">iris</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"p\">)</span></code></pre></div><b>3.1.2 相关系数法</b><p>\\u3000\\u3000使用相关系数法，先要计算各个特征对目标值的相关系数以及相关系数的P值。用feature_selection库的SelectKBest类结合相关系数来选择特征的代码如下：</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"kn\">from</span> <span class=\"nn\">sklearn.feature_selection</span> <span class=\"kn\">import</span> <span class=\"n\">SelectKBest</span>\\n<span class=\"kn\">from</span> <span class=\"nn\">scipy.stats</span> <span class=\"kn\">import</span> <span class=\"n\">pearsonr</span>\\n\\n<span class=\"c1\">#选择K个最好的特征，返回选择特征后的数据</span>\\n<span class=\"c1\">#第一个参数为计算评估特征是否好的函数，该函数输入特征矩阵和目标向量，输出二元组（评分，P值）的数组，数组第i项为第i个特征的评分和P值。在此定义为计算相关系数</span>\\n<span class=\"c1\">#参数k为选择的特征个数</span>\\n<span class=\"n\">SelectKBest</span><span class=\"p\">(</span><span class=\"k\">lambda</span> <span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">Y</span><span class=\"p\">:</span> <span class=\"n\">array</span><span class=\"p\">(</span><span class=\"nb\">map</span><span class=\"p\">(</span><span class=\"k\">lambda</span> <span class=\"n\">x</span><span class=\"p\">:</span><span class=\"n\">pearsonr</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">Y</span><span class=\"p\">),</span> <span class=\"n\">X</span><span class=\"o\">.</span><span class=\"n\">T</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">T</span><span class=\"p\">,</span> <span class=\"n\">k</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">fit_transform</span><span class=\"p\">(</span><span class=\"n\">iris</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"p\">,</span> <span class=\"n\">iris</span><span class=\"o\">.</span><span class=\"n\">target</span><span class=\"p\">)</span></code></pre></div><b>3.1.3 卡方检验</b><p>\\u3000\\u3000经典的卡方检验是检验定性自变量对定性因变量的相关性。假设自变量有N种取值，因变量有M种取值，考虑自变量等于i且因变量等于j的样本频数的观察值与期望的差距，构建统计量：</p><figure><noscript><img src=\"https://pic3.zhimg.com/50/7bc586c806b9b8bf1e74433a2e1976bc_hd.jpg\" data-rawwidth=\"162\" data-rawheight=\"48\" class=\"content_image\" width=\"162\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;162&#39; height=&#39;48&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"162\" data-rawheight=\"48\" class=\"content_image lazy\" width=\"162\" data-actualsrc=\"https://pic3.zhimg.com/50/7bc586c806b9b8bf1e74433a2e1976bc_hd.jpg\"/></figure><p>\\u3000\\u3000不难发现，<a href=\"https://link.zhihu.com/?target=http%3A//wiki.mbalib.com/wiki/%25E5%258D%25A1%25E6%2596%25B9%25E6%25A3%2580%25E9%25AA%258C\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">这个统计量的含义简而言之就是自变量对因变量的相关性</a>。用feature_selection库的SelectKBest类结合卡方检验来选择特征的代码如下：</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"kn\">from</span> <span class=\"nn\">sklearn.feature_selection</span> <span class=\"kn\">import</span> <span class=\"n\">SelectKBest</span>\\n<span class=\"kn\">from</span> <span class=\"nn\">sklearn.feature_selection</span> <span class=\"kn\">import</span> <span class=\"n\">chi2</span>\\n\\n<span class=\"c1\">#选择K个最好的特征，返回选择特征后的数据</span>\\n<span class=\"n\">SelectKBest</span><span class=\"p\">(</span><span class=\"n\">chi2</span><span class=\"p\">,</span> <span class=\"n\">k</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">fit_transform</span><span class=\"p\">(</span><span class=\"n\">iris</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"p\">,</span> <span class=\"n\">iris</span><span class=\"o\">.</span><span class=\"n\">target</span><span class=\"p\">)</span></code></pre></div><b>3.1.4 互信息法</b><p>\\u3000\\u3000经典的互信息也是评价定性自变量对定性因变量的相关性的，互信息计算公式如下：</p><figure><noscript><img src=\"https://pic4.zhimg.com/50/6af9a077b49f587a5d149f5dc51073ba_hd.jpg\" data-rawwidth=\"274\" data-rawheight=\"50\" class=\"content_image\" width=\"274\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;274&#39; height=&#39;50&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"274\" data-rawheight=\"50\" class=\"content_image lazy\" width=\"274\" data-actualsrc=\"https://pic4.zhimg.com/50/6af9a077b49f587a5d149f5dc51073ba_hd.jpg\"/></figure><p>\\u3000\\u3000为了处理定量数据，最大信息系数法被提出，使用feature_selection库的SelectKBest类结合最大信息系数法来选择特征的代码如下：</p><div class=\"highlight\"><pre><code class=\"language-python\"> <span class=\"kn\">from</span> <span class=\"nn\">sklearn.feature_selection</span> <span class=\"kn\">import</span> <span class=\"n\">SelectKBest</span>\\n <span class=\"kn\">from</span> <span class=\"nn\">minepy</span> <span class=\"kn\">import</span> <span class=\"n\">MINE</span>\\n \\n <span class=\"c1\">#由于MINE的设计不是函数式的，定义mic方法将其为函数式的，返回一个二元组，二元组的第2项设置成固定的P值0.5</span>\\n <span class=\"k\">def</span> <span class=\"nf\">mic</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">):</span>\\n     <span class=\"n\">m</span> <span class=\"o\">=</span> <span class=\"n\">MINE</span><span class=\"p\">()</span>\\n     <span class=\"n\">m</span><span class=\"o\">.</span><span class=\"n\">compute_score</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">)</span>\\n     <span class=\"k\">return</span> <span class=\"p\">(</span><span class=\"n\">m</span><span class=\"o\">.</span><span class=\"n\">mic</span><span class=\"p\">(),</span> <span class=\"mf\">0.5</span><span class=\"p\">)</span>\\n\\n<span class=\"c1\">#选择K个最好的特征，返回特征选择后的数据</span>\\n<span class=\"n\">SelectKBest</span><span class=\"p\">(</span><span class=\"k\">lambda</span> <span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">Y</span><span class=\"p\">:</span> <span class=\"n\">array</span><span class=\"p\">(</span><span class=\"nb\">map</span><span class=\"p\">(</span><span class=\"k\">lambda</span> <span class=\"n\">x</span><span class=\"p\">:</span><span class=\"n\">mic</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">Y</span><span class=\"p\">),</span> <span class=\"n\">X</span><span class=\"o\">.</span><span class=\"n\">T</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">T</span><span class=\"p\">,</span> <span class=\"n\">k</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">fit_transform</span><span class=\"p\">(</span><span class=\"n\">iris</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"p\">,</span> <span class=\"n\">iris</span><span class=\"o\">.</span><span class=\"n\">target</span><span class=\"p\">)</span></code></pre></div><b>3.2 Wrapper</b><br/><b>3.2.1 递归特征消除法</b><p>\\u3000\\u3000递归消除特征法使用一个基模型来进行多轮训练，每轮训练后，消除若干权值系数的特征，再基于新的特征集进行下一轮训练。使用feature_selection库的RFE类来选择特征的代码如下：</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"kn\">from</span> <span class=\"nn\">sklearn.feature_selection</span> <span class=\"kn\">import</span> <span class=\"n\">RFE</span>\\n<span class=\"kn\">from</span> <span class=\"nn\">sklearn.linear_model</span> <span class=\"kn\">import</span> <span class=\"n\">LogisticRegression</span>\\n\\n<span class=\"c1\">#递归特征消除法，返回特征选择后的数据</span>\\n<span class=\"c1\">#参数estimator为基模型</span>\\n<span class=\"c1\">#参数n_features_to_select为选择的特征个数</span>\\n<span class=\"n\">RFE</span><span class=\"p\">(</span><span class=\"n\">estimator</span><span class=\"o\">=</span><span class=\"n\">LogisticRegression</span><span class=\"p\">(),</span> <span class=\"n\">n_features_to_select</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">fit_transform</span><span class=\"p\">(</span><span class=\"n\">iris</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"p\">,</span> <span class=\"n\">iris</span><span class=\"o\">.</span><span class=\"n\">target</span><span class=\"p\">)</span></code></pre></div><b>3.3 Embedded</b><br/><b>3.3.1 基于惩罚项的特征选择法</b><p>\\u3000\\u3000使用带惩罚项的基模型，除了筛选出特征外，同时也进行了降维。使用feature_selection库的SelectFromModel类结合带L1惩罚项的逻辑回归模型，来选择特征的代码如下：</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"kn\">from</span> <span class=\"nn\">sklearn.feature_selection</span> <span class=\"kn\">import</span> <span class=\"n\">SelectFromModel</span>\\n<span class=\"kn\">from</span> <span class=\"nn\">sklearn.linear_model</span> <span class=\"kn\">import</span> <span class=\"n\">LogisticRegression</span>\\n\\n<span class=\"c1\">#带L1惩罚项的逻辑回归作为基模型的特征选择</span>\\n<span class=\"n\">SelectFromModel</span><span class=\"p\">(</span><span class=\"n\">LogisticRegression</span><span class=\"p\">(</span><span class=\"n\">penalty</span><span class=\"o\">=</span><span class=\"s2\">&#34;l1&#34;</span><span class=\"p\">,</span> <span class=\"n\">C</span><span class=\"o\">=</span><span class=\"mf\">0.1</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">fit_transform</span><span class=\"p\">(</span><span class=\"n\">iris</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"p\">,</span> <span class=\"n\">iris</span><span class=\"o\">.</span><span class=\"n\">target</span><span class=\"p\">)</span></code></pre></div><p>\\u3000\\u3000实际上，<a href=\"http://www.zhihu.com/question/28641663/answer/41653367\" class=\"internal\">L1惩罚项降维的原理在于保留多个对目标值具有同等相关性的特征中的一个</a>，所以没选到的特征不代表不重要。故，可结合L2惩罚项来优化。具体操作为：若一个特征在L1中的权值为1，选择在L2中权值差别不大且在L1中权值为0的特征构成同类集合，将这一集合中的特征平分L1中的权值，故需要构建一个新的逻辑回归模型：</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"kn\">from</span> <span class=\"nn\">sklearn.linear_model</span> <span class=\"kn\">import</span> <span class=\"n\">LogisticRegression</span>\\n\\n<span class=\"k\">class</span> <span class=\"nc\">LR</span><span class=\"p\">(</span><span class=\"n\">LogisticRegression</span><span class=\"p\">):</span>\\n    <span class=\"k\">def</span> <span class=\"fm\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">threshold</span><span class=\"o\">=</span><span class=\"mf\">0.01</span><span class=\"p\">,</span> <span class=\"n\">dual</span><span class=\"o\">=</span><span class=\"bp\">False</span><span class=\"p\">,</span> <span class=\"n\">tol</span><span class=\"o\">=</span><span class=\"mf\">1e-4</span><span class=\"p\">,</span> <span class=\"n\">C</span><span class=\"o\">=</span><span class=\"mf\">1.0</span><span class=\"p\">,</span>\\n                 <span class=\"n\">fit_intercept</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">,</span> <span class=\"n\">intercept_scaling</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">class_weight</span><span class=\"o\">=</span><span class=\"bp\">None</span><span class=\"p\">,</span>\\n                 <span class=\"n\">random_state</span><span class=\"o\">=</span><span class=\"bp\">None</span><span class=\"p\">,</span> <span class=\"n\">solver</span><span class=\"o\">=</span><span class=\"s1\">&#39;liblinear&#39;</span><span class=\"p\">,</span> <span class=\"n\">max_iter</span><span class=\"o\">=</span><span class=\"mi\">100</span><span class=\"p\">,</span>\\n                 <span class=\"n\">multi_class</span><span class=\"o\">=</span><span class=\"s1\">&#39;ovr&#39;</span><span class=\"p\">,</span> <span class=\"n\">verbose</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">warm_start</span><span class=\"o\">=</span><span class=\"bp\">False</span><span class=\"p\">,</span> <span class=\"n\">n_jobs</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">):</span>\\n\\n        <span class=\"c1\">#权值相近的阈值</span>\\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">threshold</span> <span class=\"o\">=</span> <span class=\"n\">threshold</span>\\n        <span class=\"n\">LogisticRegression</span><span class=\"o\">.</span><span class=\"fm\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">penalty</span><span class=\"o\">=</span><span class=\"s1\">&#39;l1&#39;</span><span class=\"p\">,</span> <span class=\"n\">dual</span><span class=\"o\">=</span><span class=\"n\">dual</span><span class=\"p\">,</span> <span class=\"n\">tol</span><span class=\"o\">=</span><span class=\"n\">tol</span><span class=\"p\">,</span> <span class=\"n\">C</span><span class=\"o\">=</span><span class=\"n\">C</span><span class=\"p\">,</span>\\n                 <span class=\"n\">fit_intercept</span><span class=\"o\">=</span><span class=\"n\">fit_intercept</span><span class=\"p\">,</span> <span class=\"n\">intercept_scaling</span><span class=\"o\">=</span><span class=\"n\">intercept_scaling</span><span class=\"p\">,</span> <span class=\"n\">class_weight</span><span class=\"o\">=</span><span class=\"n\">class_weight</span><span class=\"p\">,</span>\\n                 <span class=\"n\">random_state</span><span class=\"o\">=</span><span class=\"n\">random_state</span><span class=\"p\">,</span> <span class=\"n\">solver</span><span class=\"o\">=</span><span class=\"n\">solver</span><span class=\"p\">,</span> <span class=\"n\">max_iter</span><span class=\"o\">=</span><span class=\"n\">max_iter</span><span class=\"p\">,</span>\\n                 <span class=\"n\">multi_class</span><span class=\"o\">=</span><span class=\"n\">multi_class</span><span class=\"p\">,</span> <span class=\"n\">verbose</span><span class=\"o\">=</span><span class=\"n\">verbose</span><span class=\"p\">,</span> <span class=\"n\">warm_start</span><span class=\"o\">=</span><span class=\"n\">warm_start</span><span class=\"p\">,</span> <span class=\"n\">n_jobs</span><span class=\"o\">=</span><span class=\"n\">n_jobs</span><span class=\"p\">)</span>\\n        <span class=\"c1\">#使用同样的参数创建L2逻辑回归</span>\\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">l2</span> <span class=\"o\">=</span> <span class=\"n\">LogisticRegression</span><span class=\"p\">(</span><span class=\"n\">penalty</span><span class=\"o\">=</span><span class=\"s1\">&#39;l2&#39;</span><span class=\"p\">,</span> <span class=\"n\">dual</span><span class=\"o\">=</span><span class=\"n\">dual</span><span class=\"p\">,</span> <span class=\"n\">tol</span><span class=\"o\">=</span><span class=\"n\">tol</span><span class=\"p\">,</span> <span class=\"n\">C</span><span class=\"o\">=</span><span class=\"n\">C</span><span class=\"p\">,</span> <span class=\"n\">fit_intercept</span><span class=\"o\">=</span><span class=\"n\">fit_intercept</span><span class=\"p\">,</span> <span class=\"n\">intercept_scaling</span><span class=\"o\">=</span><span class=\"n\">intercept_scaling</span><span class=\"p\">,</span> <span class=\"n\">class_weight</span> <span class=\"o\">=</span> <span class=\"n\">class_weight</span><span class=\"p\">,</span> <span class=\"n\">random_state</span><span class=\"o\">=</span><span class=\"n\">random_state</span><span class=\"p\">,</span> <span class=\"n\">solver</span><span class=\"o\">=</span><span class=\"n\">solver</span><span class=\"p\">,</span> <span class=\"n\">max_iter</span><span class=\"o\">=</span><span class=\"n\">max_iter</span><span class=\"p\">,</span> <span class=\"n\">multi_class</span><span class=\"o\">=</span><span class=\"n\">multi_class</span><span class=\"p\">,</span> <span class=\"n\">verbose</span><span class=\"o\">=</span><span class=\"n\">verbose</span><span class=\"p\">,</span> <span class=\"n\">warm_start</span><span class=\"o\">=</span><span class=\"n\">warm_start</span><span class=\"p\">,</span> <span class=\"n\">n_jobs</span><span class=\"o\">=</span><span class=\"n\">n_jobs</span><span class=\"p\">)</span>\\n\\n    <span class=\"k\">def</span> <span class=\"nf\">fit</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">,</span> <span class=\"n\">sample_weight</span><span class=\"o\">=</span><span class=\"bp\">None</span><span class=\"p\">):</span>\\n        <span class=\"c1\">#训练L1逻辑回归</span>\\n        <span class=\"nb\">super</span><span class=\"p\">(</span><span class=\"n\">LR</span><span class=\"p\">,</span> <span class=\"bp\">self</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">fit</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">,</span> <span class=\"n\">sample_weight</span><span class=\"o\">=</span><span class=\"n\">sample_weight</span><span class=\"p\">)</span>\\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">coef_old_</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">coef_</span><span class=\"o\">.</span><span class=\"n\">copy</span><span class=\"p\">()</span>\\n        <span class=\"c1\">#训练L2逻辑回归</span>\\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">l2</span><span class=\"o\">.</span><span class=\"n\">fit</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">,</span> <span class=\"n\">sample_weight</span><span class=\"o\">=</span><span class=\"n\">sample_weight</span><span class=\"p\">)</span>\\n\\n        <span class=\"n\">cntOfRow</span><span class=\"p\">,</span> <span class=\"n\">cntOfCol</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">coef_</span><span class=\"o\">.</span><span class=\"n\">shape</span>\\n        <span class=\"c1\">#权值系数矩阵的行数对应目标值的种类数目</span>\\n        <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">cntOfRow</span><span class=\"p\">):</span>\\n            <span class=\"k\">for</span> <span class=\"n\">j</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">cntOfCol</span><span class=\"p\">):</span>\\n                <span class=\"n\">coef</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">coef_</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">][</span><span class=\"n\">j</span><span class=\"p\">]</span>\\n                <span class=\"c1\">#L1逻辑回归的权值系数不为0</span>\\n                <span class=\"k\">if</span> <span class=\"n\">coef</span> <span class=\"o\">!=</span> <span class=\"mi\">0</span><span class=\"p\">:</span>\\n                    <span class=\"n\">idx</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">j</span><span class=\"p\">]</span>\\n                    <span class=\"c1\">#对应在L2逻辑回归中的权值系数</span>\\n                    <span class=\"n\">coef1</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">l2</span><span class=\"o\">.</span><span class=\"n\">coef_</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">][</span><span class=\"n\">j</span><span class=\"p\">]</span>\\n                    <span class=\"k\">for</span> <span class=\"n\">k</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">cntOfCol</span><span class=\"p\">):</span>\\n                        <span class=\"n\">coef2</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">l2</span><span class=\"o\">.</span><span class=\"n\">coef_</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">][</span><span class=\"n\">k</span><span class=\"p\">]</span>\\n                        <span class=\"c1\">#在L2逻辑回归中，权值系数之差小于设定的阈值，且在L1中对应的权值为0</span>\\n                        <span class=\"k\">if</span> <span class=\"nb\">abs</span><span class=\"p\">(</span><span class=\"n\">coef1</span><span class=\"o\">-</span><span class=\"n\">coef2</span><span class=\"p\">)</span> <span class=\"o\">&lt;</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">threshold</span> <span class=\"ow\">and</span> <span class=\"n\">j</span> <span class=\"o\">!=</span> <span class=\"n\">k</span> <span class=\"ow\">and</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">coef_</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">][</span><span class=\"n\">k</span><span class=\"p\">]</span> <span class=\"o\">==</span> <span class=\"mi\">0</span><span class=\"p\">:</span>\\n                            <span class=\"n\">idx</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">k</span><span class=\"p\">)</span>\\n                    <span class=\"c1\">#计算这一类特征的权值系数均值</span>\\n                    <span class=\"n\">mean</span> <span class=\"o\">=</span> <span class=\"n\">coef</span> <span class=\"o\">/</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">idx</span><span class=\"p\">)</span>\\n                    <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">coef_</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">][</span><span class=\"n\">idx</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">mean</span>\\n        <span class=\"k\">return</span> <span class=\"bp\">self</span></code></pre></div><p>\\u3000\\u3000使用feature_selection库的SelectFromModel类结合带L1以及L2惩罚项的逻辑回归模型，来选择特征的代码如下：</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"kn\">from</span> <span class=\"nn\">sklearn.feature_selection</span> <span class=\"kn\">import</span> <span class=\"n\">SelectFromModel</span>\\n \\n<span class=\"c1\">#带L1和L2惩罚项的逻辑回归作为基模型的特征选择</span>\\n<span class=\"c1\">#参数threshold为权值系数之差的阈值</span>\\n<span class=\"n\">SelectFromModel</span><span class=\"p\">(</span><span class=\"n\">LR</span><span class=\"p\">(</span><span class=\"n\">threshold</span><span class=\"o\">=</span><span class=\"mf\">0.5</span><span class=\"p\">,</span> <span class=\"n\">C</span><span class=\"o\">=</span><span class=\"mf\">0.1</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">fit_transform</span><span class=\"p\">(</span><span class=\"n\">iris</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"p\">,</span> <span class=\"n\">iris</span><span class=\"o\">.</span><span class=\"n\">target</span><span class=\"p\">)</span></code></pre></div><b>3.3.2 基于树模型的特征选择法</b><p>\\u3000\\u3000树模型中GBDT也可用来作为基模型进行特征选择，使用feature_selection库的SelectFromModel类结合GBDT模型，来选择特征的代码如下：</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"kn\">from</span> <span class=\"nn\">sklearn.feature_selection</span> <span class=\"kn\">import</span> <span class=\"n\">SelectFromModel</span>\\n<span class=\"kn\">from</span> <span class=\"nn\">sklearn.ensemble</span> <span class=\"kn\">import</span> <span class=\"n\">GradientBoostingClassifier</span>\\n\\n<span class=\"c1\">#GBDT作为基模型的特征选择</span>\\n<span class=\"n\">SelectFromModel</span><span class=\"p\">(</span><span class=\"n\">GradientBoostingClassifier</span><span class=\"p\">())</span><span class=\"o\">.</span><span class=\"n\">fit_transform</span><span class=\"p\">(</span><span class=\"n\">iris</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"p\">,</span> <span class=\"n\">iris</span><span class=\"o\">.</span><span class=\"n\">target</span><span class=\"p\">)</span></code></pre></div><b><br/>4 降维</b><p>\\u3000\\u3000当特征选择完成后，可以直接训练模型了，但是可能由于特征矩阵过大，导致计算量大，训练时间长的问题，因此降低特征矩阵维度也是必不可少的。常见的降维方法除了以上提到的基于L1惩罚项的模型以外，另外还有主成分分析法（PCA）和线性判别分析（LDA），线性判别分析本身也是一个分类模型。PCA和LDA有很多的相似点，其本质是要将原始的样本映射到维度更低的样本空间中，但是PCA和LDA的映射目标不一样：<a href=\"https://link.zhihu.com/?target=http%3A//www.cnblogs.com/LeftNotEasy/archive/2011/01/08/lda-and-pca-machine-learning.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">PCA是为了让映射后的样本具有最大的发散性；而LDA是为了让映射后的样本有最好的分类性能</a>。所以说PCA是一种无监督的降维方法，而LDA是一种有监督的降维方法。</p><b>4.1 主成分分析法（PCA）</b><p>\\u3000\\u3000使用decomposition库的PCA类选择特征的代码如下：</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"kn\">from</span> <span class=\"nn\">sklearn.decomposition</span> <span class=\"kn\">import</span> <span class=\"n\">PCA</span>\\n\\n<span class=\"c1\">#主成分分析法，返回降维后的数据</span>\\n<span class=\"c1\">#参数n_components为主成分数目</span>\\n<span class=\"n\">PCA</span><span class=\"p\">(</span><span class=\"n\">n_components</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">fit_transform</span><span class=\"p\">(</span><span class=\"n\">iris</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"p\">)</span></code></pre></div><b>4.2 线性判别分析法（LDA）</b><p>\\u3000\\u3000使用lda库的LDA类选择特征的代码如下：</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"kn\">from</span> <span class=\"nn\">sklearn.lda</span> <span class=\"kn\">import</span> <span class=\"n\">LDA</span>\\n\\n<span class=\"c1\">#线性判别分析法，返回降维后的数据</span>\\n<span class=\"c1\">#参数n_components为降维后的维数</span>\\n<span class=\"n\">LDA</span><span class=\"p\">(</span><span class=\"n\">n_components</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">fit_transform</span><span class=\"p\">(</span><span class=\"n\">iris</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"p\">,</span> <span class=\"n\">iris</span><span class=\"o\">.</span><span class=\"n\">target</span><span class=\"p\">)</span></code></pre></div><b><br/><br/>5 总结</b><p>\\u3000\\u3000再让我们回归一下本文开始的特征工程的思维导图，我们可以使用sklearn完成几乎所有特征处理的工作，而且不管是数据预处理，还是特征选择，抑或降维，它们都是通过某个类的方法fit_transform完成的，fit_transform要不只带一个参数：特征矩阵，要不带两个参数：特征矩阵加目标向量。这些难道都是巧合吗？还是故意设计成这样？方法fit_transform中有fit这一单词，它和训练模型的fit方法有关联吗？接下来，我将在<a href=\"https://link.zhihu.com/?target=http%3A//www.cnblogs.com/jasonfreak/p/5448462.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">《使用sklearn优雅地进行数据挖掘》</a>中阐述其中的奥妙！</p><b><br/><br/>6 参考资料</b><ol><li><a href=\"https://link.zhihu.com/?target=http%3A//www.ats.ucla.edu/stat/mult_pkg/faq/general/dummy.htm\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">FAQ: What is dummy coding?</a></li><li><a href=\"https://link.zhihu.com/?target=http%3A//scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html%23sklearn.datasets.load_iris\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">IRIS（鸢尾花）数据集</a></li><li><a href=\"https://link.zhihu.com/?target=http%3A//wiki.mbalib.com/wiki/%25E5%258D%25A1%25E6%2596%25B9%25E6%25A3%2580%25E9%25AA%258C\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">卡方检验</a></li><li><a href=\"https://link.zhihu.com/?target=http%3A//dataunion.org/14072.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">干货：结合Scikit-learn介绍几种常用的特征选择方法</a></li><li><a href=\"http://www.zhihu.com/question/28641663/answer/41653367\" class=\"internal\">机器学习中，有哪些特征选择的工程方法？</a></li><li><a href=\"https://link.zhihu.com/?target=http%3A//www.cnblogs.com/LeftNotEasy/archive/2011/01/08/lda-and-pca-machine-learning.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">机器学习中的数学(4)-线性判别分析（LDA）, 主成分分析(PCA)</a></li></ol></span></div><div><div class=\"ContentItem-time\"><a target=\"_blank\" href=\"//www.zhihu.com/question/29316149/answer/110159647\"><span data-tooltip=\"发布于 2016-07-09 10:15\">编辑于 2016-07-19</span></a></div></div><div class=\"ContentItem-actions RichContent-actions\"><span><button aria-label=\"赞同 2540 \" type=\"button\" class=\"Button VoteButton VoteButton--up\"><span style=\"display:inline-flex;align-items:center\">\\u200b<svg class=\"Zi Zi--TriangleUp VoteButton-TriangleUp\" fill=\"currentColor\" viewBox=\"0 0 24 24\" width=\"10\" height=\"10\"><path d=\"M2 18.242c0-.326.088-.532.237-.896l7.98-13.203C10.572 3.57 11.086 3 12 3c.915 0 1.429.571 1.784 1.143l7.98 13.203c.15.364.236.57.236.896 0 1.386-.875 1.9-1.955 1.9H3.955c-1.08 0-1.955-.517-1.955-1.9z\" fill-rule=\"evenodd\"></path></svg></span>赞同 2540</button><button aria-label=\"反对\" type=\"button\" class=\"Button VoteButton VoteButton--down\"><span style=\"display:inline-flex;align-items:center\">\\u200b<svg class=\"Zi Zi--TriangleDown\" fill=\"currentColor\" viewBox=\"0 0 24 24\" width=\"10\" height=\"10\"><path d=\"M20.044 3H3.956C2.876 3 2 3.517 2 4.9c0 .326.087.533.236.896L10.216 19c.355.571.87 1.143 1.784 1.143s1.429-.572 1.784-1.143l7.98-13.204c.149-.363.236-.57.236-.896 0-1.386-.876-1.9-1.956-1.9z\" fill-rule=\"evenodd\"></path></svg></span></button></span><button type=\"button\" class=\"Button ContentItem-action Button--plain Button--withIcon Button--withLabel\"><span style=\"display:inline-flex;align-items:center\">\\u200b<svg class=\"Zi Zi--Comment Button-zi\" fill=\"currentColor\" viewBox=\"0 0 24 24\" width=\"1.2em\" height=\"1.2em\"><path d=\"M10.241 19.313a.97.97 0 0 0-.77.2 7.908 7.908 0 0 1-3.772 1.482.409.409 0 0 1-.38-.637 5.825 5.825 0 0 0 1.11-2.237.605.605 0 0 0-.227-.59A7.935 7.935 0 0 1 3 11.25C3 6.7 7.03 3 12 3s9 3.7 9 8.25-4.373 9.108-10.759 8.063z\" fill-rule=\"evenodd\"></path></svg></span>63 条评论</button><div class=\"Popover ShareMenu ContentItem-action\"><div class=\"ShareMenu-toggler\" id=\"null-toggle\" aria-haspopup=\"true\" aria-expanded=\"false\" aria-owns=\"null-content\"><button type=\"button\" class=\"Button Button--plain Button--withIcon Button--withLabel\"><span style=\"display:inline-flex;align-items:center\">\\u200b<svg class=\"Zi Zi--Share Button-zi\" fill=\"currentColor\" viewBox=\"0 0 24 24\" width=\"1.2em\" height=\"1.2em\"><path d=\"M2.931 7.89c-1.067.24-1.275 1.669-.318 2.207l5.277 2.908 8.168-4.776c.25-.127.477.198.273.39L9.05 14.66l.927 5.953c.18 1.084 1.593 1.376 2.182.456l9.644-15.242c.584-.892-.212-2.029-1.234-1.796L2.93 7.89z\" fill-rule=\"evenodd\"></path></svg></span>分享</button></div></div><button type=\"button\" class=\"Button ContentItem-action Button--plain Button--withIcon Button--withLabel\"><span style=\"display:inline-flex;align-items:center\">\\u200b<svg class=\"Zi Zi--Star Button-zi\" fill=\"currentColor\" viewBox=\"0 0 24 24\" width=\"1.2em\" height=\"1.2em\"><path d=\"M5.515 19.64l.918-5.355-3.89-3.792c-.926-.902-.639-1.784.64-1.97L8.56 7.74l2.404-4.871c.572-1.16 1.5-1.16 2.072 0L15.44 7.74l5.377.782c1.28.186 1.566 1.068.64 1.97l-3.89 3.793.918 5.354c.219 1.274-.532 1.82-1.676 1.218L12 18.33l-4.808 2.528c-1.145.602-1.896.056-1.677-1.218z\" fill-rule=\"evenodd\"></path></svg></span>收藏</button><button type=\"button\" class=\"Button ContentItem-action Button--plain Button--withIcon Button--withLabel\"><span style=\"display:inline-flex;align-items:center\">\\u200b<svg class=\"Zi Zi--Heart Button-zi\" fill=\"currentColor\" viewBox=\"0 0 24 24\" width=\"1.2em\" height=\"1.2em\"><path d=\"M2 8.437C2 5.505 4.294 3.094 7.207 3 9.243 3 11.092 4.19 12 6c.823-1.758 2.649-3 4.651-3C19.545 3 22 5.507 22 8.432 22 16.24 13.842 21 12 21 10.158 21 2 16.24 2 8.437z\" fill-rule=\"evenodd\"></path></svg></span>喜欢</button><button data-zop-retract-question=\"true\" type=\"button\" class=\"Button ContentItem-action ContentItem-rightButton Button--plain\"><span class=\"RichContent-collapsedText\">收起</span><span style=\"display:inline-flex;align-items:center\">\\u200b<svg class=\"Zi Zi--ArrowDown ContentItem-arrowIcon is-active\" fill=\"currentColor\" viewBox=\"0 0 24 24\" width=\"24\" height=\"24\"><path d=\"M12 13L8.285 9.218a.758.758 0 0 0-1.064 0 .738.738 0 0 0 0 1.052l4.249 4.512a.758.758 0 0 0 1.064 0l4.246-4.512a.738.738 0 0 0 0-1.052.757.757 0 0 0-1.063 0L12.002 13z\" fill-rule=\"evenodd\"></path></svg></span></button></div></div></div></div><div></div></div></div></div></div></div></div></div></div></div></div></main><div data-zop-usertoken=\"{}\"></div></div></div><script nonce=\"f9cc5f30-ae62-4f3a-8440-9c3520ff9733\" async=\"\" src=\"https://www.googletagmanager.com/gtag/js?id=UA-149949619-1\"></script><script nonce=\"f9cc5f30-ae62-4f3a-8440-9c3520ff9733\">function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag(\"js\",new Date),gtag(\"config\",\"UA-149949619-1\");</script><script id=\"js-clientConfig\" type=\"text/json\">{\"host\":\"zhihu.com\",\"protocol\":\"https:\",\"wwwHost\":\"www.zhihu.com\",\"fetchRoot\":{\"www\":\"https:\\\\u002F\\\\u002Fwww.zhihu.com\",\"api\":\"https:\\\\u002F\\\\u002Fapi.zhihu.com\",\"zhuanlan\":\"https:\\\\u002F\\\\u002Fzhuanlan.zhihu.com\"}}</script><script id=\"js-initialData\" type=\"text/json\">{\"initialState\":{\"common\":{\"ask\":{}},\"loading\":{\"global\":{\"count\":0},\"local\":{\"question\\\\u002Fget\\\\u002F\":false,\"question\\\\u002FgetAnswers\\\\u002F29316149\":false}},\"club\":{\"tags\":{},\"admins\":{\"data\":[]},\"members\":{\"data\":[]},\"explore\":{},\"profile\":{},\"checkin\":{},\"comments\":{\"paging\":{},\"loading\":{},\"ids\":{}},\"postList\":{\"paging\":{},\"loading\":{},\"ids\":{}},\"recommend\":{\"data\":[]}},\"entities\":{\"users\":{},\"questions\":{\"29316149\":{\"type\":\"question\",\"id\":29316149,\"title\":\"特征工程到底是什么？\",\"questionType\":\"normal\",\"created\":1428220876,\"updatedTime\":1428220876,\"url\":\"https:\\\\u002F\\\\u002Fwww.zhihu.com\\\\u002Fapi\\\\u002Fv4\\\\u002Fquestions\\\\u002F29316149\",\"isMuted\":false,\"isVisible\":true,\"isNormal\":true,\"isEditable\":false,\"adminClosedComment\":false,\"hasPublishingDraft\":false,\"answerCount\":38,\"visitCount\":403390,\"commentCount\":0,\"followerCount\":6022,\"collapsedAnswerCount\":0,\"excerpt\":\"大概知道一些，但是不是很清楚领域内的知识是怎么帮助特征工程的呢？如果有例子就更好了。\",\"commentPermission\":\"all\",\"detail\":\"大概知道一些，但是不是很清楚领域内的知识是怎么帮助特征工程的呢？如果有例子就更好了。\",\"editableDetail\":\"大概知道一些，但是不是很清楚领域内的知识是怎么帮助特征工程的呢？如果有例子就更好了。\",\"status\":{\"isLocked\":false,\"isClose\":false,\"isEvaluate\":false,\"isSuggest\":false},\"relationship\":{\"isAuthor\":false,\"isFollowing\":false,\"isAnonymous\":false,\"canLock\":false,\"canStickAnswers\":false,\"canCollapseAnswers\":false},\"topics\":[{\"id\":\"19553534\",\"type\":\"topic\",\"url\":\"https:\\\\u002F\\\\u002Fwww.zhihu.com\\\\u002Fapi\\\\u002Fv4\\\\u002Ftopics\\\\u002F19553534\",\"name\":\"数据挖掘\",\"avatarUrl\":\"https:\\\\u002F\\\\u002Fpic4.zhimg.com\\\\u002F50\\\\u002Fv2-ffec50bd1604c0d53c73bd484ff05bb3_qhd.jpg\",\"topicType\":\"CHANNEL\"},{\"id\":\"19559424\",\"type\":\"topic\",\"url\":\"https:\\\\u002F\\\\u002Fwww.zhihu.com\\\\u002Fapi\\\\u002Fv4\\\\u002Ftopics\\\\u002F19559424\",\"name\":\"数据分析\",\"avatarUrl\":\"https:\\\\u002F\\\\u002Fpic2.zhimg.com\\\\u002F50\\\\u002Fv2-20382e770da3bff41c52c04e821ec6cc_qhd.jpg\",\"topicType\":\"CHANNEL\"},{\"id\":\"19563024\",\"type\":\"topic\",\"url\":\"https:\\\\u002F\\\\u002Fwww.zhihu.com\\\\u002Fapi\\\\u002Fv4\\\\u002Ftopics\\\\u002F19563024\",\"name\":\"推荐系统\",\"avatarUrl\":\"https:\\\\u002F\\\\u002Fpic2.zhimg.com\\\\u002F50\\\\u002Fv2-dbd37a24b50740e0a32e121cab8b612e_qhd.jpg\",\"topicType\":\"NORMAL\"},{\"id\":\"19740929\",\"type\":\"topic\",\"url\":\"https:\\\\u002F\\\\u002Fwww.zhihu.com\\\\u002Fapi\\\\u002Fv4\\\\u002Ftopics\\\\u002F19740929\",\"name\":\"大数据\",\"avatarUrl\":\"https:\\\\u002F\\\\u002Fpic1.zhimg.com\\\\u002F50\\\\u002Fv2-fa204023265c107d93668eb54c6f43ef_qhd.jpg\",\"topicType\":\"CHANNEL\"},{\"id\":\"19809410\",\"type\":\"topic\",\"url\":\"https:\\\\u002F\\\\u002Fwww.zhihu.com\\\\u002Fapi\\\\u002Fv4\\\\u002Ftopics\\\\u002F19809410\",\"name\":\"特征选择\",\"avatarUrl\":\"https:\\\\u002F\\\\u002Fpic3.zhimg.com\\\\u002Fe82bab09c_l.jpg\",\"topicType\":\"NORMAL\"}],\"author\":{\"id\":\"807114adabb9060957fef1f06de47c2c\",\"urlToken\":\"bean-mr-51\",\"name\":\"bean mr\",\"avatarUrl\":\"https:\\\\u002F\\\\u002Fpic1.zhimg.com\\\\u002Fda8e974dc_l.jpg\",\"avatarUrlTemplate\":\"https:\\\\u002F\\\\u002Fpic2.zhimg.com\\\\u002Fda8e974dc.jpg\",\"isOrg\":false,\"type\":\"people\",\"url\":\"https:\\\\u002F\\\\u002Fwww.zhihu.com\\\\u002Fapi\\\\u002Fv4\\\\u002Fpeople\\\\u002F807114adabb9060957fef1f06de47c2c\",\"userType\":\"people\",\"headline\":\"是知也\",\"badge\":[],\"badgeV2\":{\"title\":\"\",\"mergedBadges\":[],\"detailBadges\":[]},\"gender\":1,\"isAdvertiser\":false,\"isPrivacy\":false},\"canComment\":{\"status\":true,\"reason\":\"\"},\"reviewInfo\":{\"type\":\"\",\"tips\":\"\",\"editTips\":\"\",\"isReviewing\":false,\"editIsReviewing\":false},\"relatedCards\":[],\"muteInfo\":{\"type\":\"\"},\"showAuthor\":false,\"isLabeled\":false,\"showEncourageAuthor\":false,\"voteupCount\":18,\"visibleOnlyToAuthor\":false}},\"answers\":{\"82949813\":{\"id\":82949813,\"type\":\"answer\",\"answerType\":\"normal\",\"question\":{\"type\":\"question\",\"id\":29316149,\"title\":\"特征工程到底是什么？\",\"questionType\":\"normal\",\"created\":1428220876,\"updatedTime\":1428220876,\"url\":\"https:\\\\u002F\\\\u002Fwww.zhihu.com\\\\u002Fapi\\\\u002Fv4\\\\u002Fquestions\\\\u002F29316149\",\"relationship\":{}},\"author\":{\"id\":\"0cabf9a69e4e6a7775a57d48f1f68149\",\"urlToken\":\"zr9558\",\"name\":\"张戎\",\"avatarUrl\":\"https:\\\\u002F\\\\u002Fpic4.zhimg.com\\\\u002F132f5d0d10e0c5ad4079633336b33eb8_l.jpg\",\"avatarUrlTemplate\":\"https:\\\\u002F\\\\u002Fpic4.zhimg.com\\\\u002F132f5d0d10e0c5ad4079633336b33eb8.jpg\",\"isOrg\":false,\"type\":\"people\",\"url\":\"https:\\\\u002F\\\\u002Fwww.zhihu.com\\\\u002Fapi\\\\u002Fv4\\\\u002Fpeople\\\\u002F0cabf9a69e4e6a7775a57d48f1f68149\",\"userType\":\"people\",\"headline\":\"追求卓越 成功就会在不经意间追上你\",\"badge\":[{\"type\":\"identity\",\"description\":\"新加坡国立大学 数学博士\",\"topics\":[]},{\"type\":\"best_answerer\",\"description\":\"优秀回答者\",\"topics\":[{\"id\":\"19554091\",\"type\":\"topic\",\"url\":\"https:\\\\u002F\\\\u002Fwww.zhihu.com\\\\u002Fapi\\\\u002Fv4\\\\u002Ftopics\\\\u002F19554091\",\"name\":\"数学\",\"avatarUrl\":\"https:\\\\u002F\\\\u002Fpic1.zhimg.com\\\\u002F50\\\\u002Fv2-351d57389cf50b002a20606caac645cf_qhd.jpg\",\"topicType\":\"CHANNEL\"}]}],\"badgeV2\":{\"title\":\"数学话题下的优秀回答者\",\"mergedBadges\":[{\"type\":\"best\",\"detailType\":\"best_answerer\",\"title\":\"优秀回答者\",\"description\":\"数学话题下的优秀回答者\",\"url\":\"https:\\\\u002F\\\\u002Fwww.zhihu.com\\\\u002Ftopic\\\\u002F20054793\",\"sources\":[{\"id\":\"19554091\",\"token\":\"19554091\",\"type\":\"topic\",\"url\":\"https:\\\\u002F\\\\u002Fwww.zhihu.com\\\\u002Ftopic\\\\u002F19554091\",\"name\":\"数学\",\"avatarPath\":\"v2-351d57389cf50b002a20606caac645cf\",\"avatarUrl\":\"https:\\\\u002F\\\\u002Fpic1.zhimg.com\\\\u002Fv2-351d57389cf50b002a20606caac645cf_hd.jpg\",\"description\":\"\"}]},{\"type\":\"identity\",\"detailType\":\"identity_people\",\"title\":\"已认证的个人\",\"description\":\"新加坡国立大学 数学博士\",\"url\":\"https:\\\\u002F\\\\u002Fwww.zhihu.com\\\\u002Faccount\\\\u002Fverification\\\\u002Fintro\",\"sources\":[]}],\"detailBadges\":[]},\"gender\":1,\"isAdvertiser\":false,\"followerCount\":61936,\"isFollowed\":false,\"isPrivacy\":false},\"url\":\"https:\\\\u002F\\\\u002Fwww.zhihu.com\\\\u002Fapi\\\\u002Fv4\\\\u002Fanswers\\\\u002F82949813\",\"isCollapsed\":false,\"createdTime\":1453526475,\"updatedTime\":1453526475,\"extras\":\"\",\"isCopyable\":false,\"isNormal\":true,\"voteupCount\":275,\"commentCount\":17,\"isSticky\":false,\"adminClosedComment\":false,\"commentPermission\":\"all\",\"canComment\":{\"reason\":\"\",\"status\":true},\"reshipmentSettings\":\"disallowed\",\"content\":\"特征工程是一个非常重要的课题，是机器学习中不可缺少的一部分，但是它几乎很少出现于机器学习书本里面的某一章。在机器学习方面的成功很大程度上在于如果使用特征工程。\\\\u003Cbr\\\\u002F\\\\u003E\\\\u003Cbr\\\\u002F\\\\u003E（I）特征工程可以解决什么样的问题？\\\\u003Cbr\\\\u002F\\\\u003E\\\\u003Cbr\\\\u002F\\\\u003E在机器学习中，经常是用一个预测模型（线性回归，逻辑回归，SVD等）和一堆原始数据来得到一些预测的结果，人们需要做的是从这堆原始数据中去提炼较优的结果，然后做到最优的预测。这个就包括两个方面，第一就是如何选择和使用各种模型，第二就是怎么样去使用这些原始的数据才能达到最优的效果。那么怎么样才能够获得最优的结果呢？贴上一句经典的话就是：\\\\u003Cbr\\\\u002F\\\\u003E\\\\u003Cbr\\\\u002F\\\\u003EActually the sucess of all Machine Learning algorithms depends on how you present the data. \\\\u003Cbr\\\\u002F\\\\u003E------ Mohammad Pezeshki\\\\u003Cbr\\\\u002F\\\\u003E\\\\u003Cbr\\\\u002F\\\\u003E直接翻译过来便是：事实上所有机器学习算法上面的成功都在于你怎么样去展示这些数据。由此可见特征工程在实际的机器学习中的重要性，从数据里面提取出来的特征好坏与否就会直接影响模型的效果。从某些层面上来说，所使用的特征越好，得到的效果就会越好。所需要的特征就是可以借此来描述已知数据的内在关系。总结一下就是：\\\\u003Cbr\\\\u002F\\\\u003E\\\\u003Cbr\\\\u002F\\\\u003EBetter feature means flexibility. Better feature means simpler models. Better feature means better results.\\\\u003Cbr\\\\u002F\\\\u003E\\\\u003Cbr\\\\u002F\\\\u003E有的时候，可以使用一些不是最优的模型来训练数据，如果特征选择得好的话，依然可以得到一个不错的结果。很多机器学习的模型都能够从数据中选择出不错的结构，从而进行良好的预测。一个优秀的特征具有极强的灵活性，可以使用不那么复杂的，运算速度快，容易理解和维护的模型来得到不错的结果。\\\\u003Cbr\\\\u002F\\\\u003E\\\\u003Cbr\\\\u002F\\\\u003E（II）什么才是特征工程？\\\\u003Cbr\\\\u002F\\\\u003E\\\\u003Cbr\\\\u002F\\\\u003EFeature Engineering is the process of transforming raw data into features that better represent the underlying problem to the predictive models, resulting in improved model accuracy on unseen data.  \\\\u003Cbr\\\\u002F\\\\u003E------ Jason Brownlee\\\\u003Cbr\\\\u002F\\\\u003E\\\\u003Cbr\\\\u002F\\\\u003EFeature Engineering is manually designing what the input x&#39;s should be. \\\\u003Cbr\\\\u002F\\\\u003E------ Tomasz Malisiewicz\\\\u003Cbr\\\\u002F\\\\u003E\\\\u003Cbr\\\\u002F\\\\u003E从这个概念可以看出，特征工程其实是一个如何展示和表现数据的问题，在实际工作中需要把数据以一种“良好”的方式展示出来，使得能够使用各种各样的机器学习模型来得到更好的效果。如何从原始数据中去除不佳的数据，展示合适的数据就成为了特征工程的关键问题。\\\\u003Cbr\\\\u002F\\\\u003E\\\\u003Cbr\\\\u002F\\\\u003E（III）特征有用性的预估\\\\u003Cbr\\\\u002F\\\\u003E\\\\u003Cbr\\\\u002F\\\\u003E每次构造了一个特征，都需要从各个方面去证明该特征的有效性。一个特征是否重要主要在于该特征与要预测的东西是否是高度相关的，如果是高度相关，那么该特征就是十分重要的。比如常用的工具就是统计学里面的相关系数。\\\\u003Cbr\\\\u002F\\\\u003E\\\\u003Cbr\\\\u002F\\\\u003E（IV）特征的构造过程 \\\\u003Cbr\\\\u002F\\\\u003E\\\\u003Cbr\\\\u002F\\\\u003E在实际工作中首先肯定要确定具体的问题，然后就是数据的选择和准备过程，再就是模型的准备和计算工作，最后才是展示数据的预测结果。构造特征的一般步骤：\\\\u003Cbr\\\\u002F\\\\u003E［1］任务的确定：根据具体的业务确定需要解决的问题；\\\\u003Cbr\\\\u002F\\\\u003E\\\\u003Cbr\\\\u002F\\\\u003E［2］数据的选择：整合数据，收集数据；\\\\u003Cbr\\\\u002F\\\\u003E\\\\u003Cbr\\\\u002F\\\\u003E［3］预处理数据：设计数据展现的格式，清洗数据，选择合适的样本使得机器学习模型能够使用它。比方说一些年龄特征是空值或者负数或者大于200等，或者说某个页面的播放数据大于曝光数据，这些就是数据的不合理，需要在使用之前把这一批数据排除掉。\\\\u003Cbr\\\\u002F\\\\u003E\\\\u003Cbr\\\\u002F\\\\u003E［4］特征的构造：转化数据，使之成为有效的特征。常用的方法是标准化，归一化，特征的离散化等。\\\\u003Cbr\\\\u002F\\\\u003E（4.1）标准化（Standardization）：比方说有一些数字的单位是千克，有一些数字的单位是克，这个时候需要统一单位。如果没有标准化，两个变量混在一起搞，那么肯定就会不合适。\\\\u003Cbr\\\\u002F\\\\u003E（4.2）归一化（Normalization）：归一化是因为在特征会在不同的尺度下有不同的表现形式，归一化会使得各个特征能够同时以恰当的方式表现。比方说某个专辑的点击播放率一般不会超过0.2，但是专辑的播放次数可能会达到几千次，所以说为了能够在模型里面得到更合适结果，需要先把一些特征在尺度上进行归一化，然后进行模型训练。\\\\u003Cbr\\\\u002F\\\\u003E（4.3）特征的离散化（Discretization）：离散化是指把特征进行必要的离散处理，比方说年龄特征是一个连续的特征，但是把年龄层分成5－18岁（中小学生），19－23岁（大学生），24－29岁（工作前几年），30－40岁（成家立业），40－60岁（中年人）从某些层面来说比连续的年龄数据（比如说某人年龄是20岁1月3日之类的）更容易理解不同年龄层人的特性。典型的离散化步骤：对特征做排序－&gt; 选择合适的分割点－&gt; 作出区间的分割 －&gt;  作出区间分割－&gt; 查看是否能够达到停止条件。\\\\u003Cbr\\\\u002F\\\\u003E\\\\u003Cbr\\\\u002F\\\\u003E［5］模型的使用：创造模型，选择合适的模型，用合适的模型来进行预测，用各种统计指标来判断该特征是否合适；\\\\u003Cbr\\\\u002F\\\\u003E\\\\u003Cbr\\\\u002F\\\\u003E［6］上线的效果：通过在线测试来看效果。\\\\u003Cbr\\\\u002F\\\\u003E\\\\u003Cbr\\\\u002F\\\\u003E数据的转换（Transforming Data）就是把数据从原始的数据状态转换成适合模型计算的状态，从某些层面上来说，“数据转换“和”特征构造“的过程几乎是一致的。\\\\u003Cbr\\\\u002F\\\\u003E\\\\u003Cbr\\\\u002F\\\\u003E（V）特征工程的迭代过程\\\\u003Cbr\\\\u002F\\\\u003E\\\\u003Cbr\\\\u002F\\\\u003E特征工程的迭代步骤：\\\\u003Cbr\\\\u002F\\\\u003E［1］选择特征：需要进行头脑风暴（brainstorm）。通过具体的问题分析，查看大量的数据，从数据中查看出可以提取出数据的关键；\\\\u003Cbr\\\\u002F\\\\u003E［2］设计特征：这个需要具体问题具体分析，可以自动进行特征提取工作，也可以进行手工进行特征的构造工作，甚至混合两种方法；\\\\u003Cbr\\\\u002F\\\\u003E［3］选择特征：使用不同的特征构造方法，来从多个层面来判断这个特征的选择是否合适；\\\\u003Cbr\\\\u002F\\\\u003E［4］计算模型：通过模型计算得到模型在该特征上所提升的准确率。\\\\u003Cbr\\\\u002F\\\\u003E［5］上线测试：通过在线测试的效果来判断特征是否有效。\",\"editableContent\":\"\",\"excerpt\":\"特征工程是一个非常重要的课题，是机器学习中不可缺少的一部分，但是它几乎很少出现于机器学习书本里面的某一章。在机器学习方面的成功很大程度上在于如果使用特征工程。 （I）特征工程可以解决什么样的问题？ 在机器学习中，经常是用一个预测模型（线性回归，逻辑回归，SVD等）和一堆原始数据来得到一些预测的结果，人们需要做的是从这堆原始数据中去提炼较优的结果，然后做到最优的预测。这个就包括两个方面，第一就是如何选择…\",\"collapsedBy\":\"nobody\",\"collapseReason\":\"\",\"annotationAction\":null,\"markInfos\":[],\"relevantInfo\":{\"isRelevant\":false,\"relevantType\":\"\",\"relevantText\":\"\"},\"suggestEdit\":{\"reason\":\"\",\"status\":false,\"tip\":\"\",\"title\":\"\",\"unnormalDetails\":{\"status\":\"\",\"description\":\"\",\"reason\":\"\",\"reasonId\":0,\"note\":\"\"},\"url\":\"\"},\"isLabeled\":true,\"rewardInfo\":{\"canOpenReward\":false,\"isRewardable\":false,\"rewardMemberCount\":0,\"rewardTotalMoney\":0,\"tagline\":\"\"},\"relationship\":{\"isAuthor\":false,\"isAuthorized\":false,\"isNothelp\":false,\"isThanked\":false,\"isRecognized\":false,\"voting\":0,\"upvotedFollowees\":[]},\"adAnswer\":null},\"110159647\":{\"id\":110159647,\"type\":\"answer\",\"answerType\":\"normal\",\"question\":{\"type\":\"question\",\"id\":29316149,\"title\":\"特征工程到底是什么？\",\"questionType\":\"normal\",\"created\":1428220876,\"updatedTime\":1428220876,\"url\":\"https:\\\\u002F\\\\u002Fwww.zhihu.com\\\\u002Fapi\\\\u002Fv4\\\\u002Fquestions\\\\u002F29316149\",\"relationship\":{}},\"author\":{\"id\":\"034d67956246877e7c376f67e2d10546\",\"urlToken\":\"jasonfreak\",\"name\":\"城东\",\"avatarUrl\":\"https:\\\\u002F\\\\u002Fpic1.zhimg.com\\\\u002F34d6534efe90a4e6580254dd2ef17903_l.jpg\",\"avatarUrlTemplate\":\"https:\\\\u002F\\\\u002Fpic2.zhimg.com\\\\u002F34d6534efe90a4e6580254dd2ef17903.jpg\",\"isOrg\":false,\"type\":\"people\",\"url\":\"https:\\\\u002F\\\\u002Fwww.zhihu.com\\\\u002Fapi\\\\u002Fv4\\\\u002Fpeople\\\\u002F034d67956246877e7c376f67e2d10546\",\"userType\":\"people\",\"headline\":\"一个懒惰的人，总是想设计更智能的程序来避免做重复性工作\",\"badge\":[],\"badgeV2\":{\"title\":\"\",\"mergedBadges\":[],\"detailBadges\":[]},\"gender\":1,\"isAdvertiser\":false,\"followerCount\":3329,\"isFollowed\":false,\"isPrivacy\":false},\"url\":\"https:\\\\u002F\\\\u002Fwww.zhihu.com\\\\u002Fapi\\\\u002Fv4\\\\u002Fanswers\\\\u002F110159647\",\"isCollapsed\":false,\"createdTime\":1468030516,\"updatedTime\":1468859618,\"extras\":\"\",\"isCopyable\":true,\"isNormal\":true,\"voteupCount\":2540,\"commentCount\":63,\"isSticky\":false,\"adminClosedComment\":false,\"commentPermission\":\"all\",\"canComment\":{\"reason\":\"\",\"status\":true},\"reshipmentSettings\":\"allowed\",\"content\":\"转自我的博文：\\\\u003Cbr\\\\u002F\\\\u003E\\\\u003Ca href=\\\\\"https:\\\\u002F\\\\u002Flink.zhihu.com\\\\u002F?target=http%3A\\\\u002F\\\\u002Fwww.cnblogs.com\\\\u002Fjasonfreak\\\\u002Fp\\\\u002F5448385.html\\\\\" class=\\\\\" wrap external\\\\\" target=\\\\\"_blank\\\\\" rel=\\\\\"nofollow noreferrer\\\\\"\\\\u003E使用sklearn做单机特征工程\\\\u003C\\\\u002Fa\\\\u003E\\\\u003Cbr\\\\u002F\\\\u003E\\\\u003Cb\\\\u003E目录\\\\u003C\\\\u002Fb\\\\u003E\\\\u003Cp\\\\u003E1 特征工程是什么？\\\\u003Cbr\\\\u002F\\\\u003E2 数据预处理\\\\u003Cbr\\\\u002F\\\\u003E\\u3000\\u30002.1 无量纲化\\\\u003Cbr\\\\u002F\\\\u003E\\u3000\\u3000\\u3000\\u30002.1.1 标准化\\\\u003Cbr\\\\u002F\\\\u003E\\u3000\\u3000\\u3000\\u30002.1.2 区间缩放法\\\\u003Cbr\\\\u002F\\\\u003E\\u3000\\u3000\\u3000\\u30002.1.3 标准化与归一化的区别\\\\u003Cbr\\\\u002F\\\\u003E\\u3000\\u30002.2 对定量特征二值化\\\\u003Cbr\\\\u002F\\\\u003E\\u3000\\u30002.3 对定性特征哑编码\\\\u003Cbr\\\\u002F\\\\u003E\\u3000\\u30002.4 缺失值计算\\\\u003Cbr\\\\u002F\\\\u003E\\u3000\\u30002.5 数据变换\\\\u003Cbr\\\\u002F\\\\u003E3 特征选择\\\\u003Cbr\\\\u002F\\\\u003E\\u3000\\u30003.1 Filter\\\\u003Cbr\\\\u002F\\\\u003E\\u3000\\u3000\\u3000\\u30003.1.1 方差选择法\\\\u003Cbr\\\\u002F\\\\u003E\\u3000\\u3000\\u3000\\u30003.1.2 相关系数法\\\\u003Cbr\\\\u002F\\\\u003E\\u3000\\u3000\\u3000\\u30003.1.3 卡方检验\\\\u003Cbr\\\\u002F\\\\u003E\\u3000\\u3000\\u3000\\u30003.1.4 互信息法\\\\u003Cbr\\\\u002F\\\\u003E\\u3000\\u30003.2 Wrapper\\\\u003Cbr\\\\u002F\\\\u003E\\u3000\\u3000\\u3000\\u30003.2.1 递归特征消除法\\\\u003Cbr\\\\u002F\\\\u003E\\u3000\\u30003.3 Embedded\\\\u003Cbr\\\\u002F\\\\u003E\\u3000\\u3000\\u3000\\u30003.3.1 基于惩罚项的特征选择法\\\\u003Cbr\\\\u002F\\\\u003E\\u3000\\u3000\\u3000\\u30003.3.2 基于树模型的特征选择法\\\\u003Cbr\\\\u002F\\\\u003E4 降维\\\\u003Cbr\\\\u002F\\\\u003E\\u3000\\u30004.1 主成分分析法（PCA）\\\\u003Cbr\\\\u002F\\\\u003E\\u3000\\u30004.2 线性判别分析法（LDA）\\\\u003Cbr\\\\u002F\\\\u003E5 总结\\\\u003Cbr\\\\u002F\\\\u003E6 参考资料\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Cbr\\\\u002F\\\\u003E\\\\u003Cb\\\\u003E1 特征工程是什么？\\\\u003C\\\\u002Fb\\\\u003E\\\\u003Cp\\\\u003E\\u3000\\u3000有这么一句话在业界广泛流传：数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已。那特征工程到底是什么呢？顾名思义，其本质是一项工程活动，目的是最大限度地从原始数据中提取特征以供算法和模型使用。通过总结和归纳，人们认为特征工程包括以下方面：\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Cfigure\\\\u003E\\\\u003Cnoscript\\\\u003E\\\\u003Cimg src=\\\\\"https:\\\\u002F\\\\u002Fpic2.zhimg.com\\\\u002F50\\\\u002F20e4522e6104ad71fc543cc21f402b36_hd.jpg\\\\\" data-rawwidth=\\\\\"875\\\\\" data-rawheight=\\\\\"967\\\\\" class=\\\\\"origin_image zh-lightbox-thumb\\\\\" width=\\\\\"875\\\\\" data-original=\\\\\"https:\\\\u002F\\\\u002Fpic3.zhimg.com\\\\u002F20e4522e6104ad71fc543cc21f402b36_r.jpg\\\\\"\\\\u002F\\\\u003E\\\\u003C\\\\u002Fnoscript\\\\u003E\\\\u003Cimg src=\\\\\"data:image\\\\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\\\\u002F\\\\u002Fwww.w3.org\\\\u002F2000\\\\u002Fsvg&#39; width=&#39;875&#39; height=&#39;967&#39;&gt;&lt;\\\\u002Fsvg&gt;\\\\\" data-rawwidth=\\\\\"875\\\\\" data-rawheight=\\\\\"967\\\\\" class=\\\\\"origin_image zh-lightbox-thumb lazy\\\\\" width=\\\\\"875\\\\\" data-original=\\\\\"https:\\\\u002F\\\\u002Fpic3.zhimg.com\\\\u002F20e4522e6104ad71fc543cc21f402b36_r.jpg\\\\\" data-actualsrc=\\\\\"https:\\\\u002F\\\\u002Fpic2.zhimg.com\\\\u002F50\\\\u002F20e4522e6104ad71fc543cc21f402b36_hd.jpg\\\\\"\\\\u002F\\\\u003E\\\\u003C\\\\u002Ffigure\\\\u003E\\\\u003Cbr\\\\u002F\\\\u003E\\\\u003Cp\\\\u003E\\u3000\\u3000特征处理是特征工程的核心部分，sklearn提供了较为完整的特征处理方法，包括数据预处理，特征选择，降维等。首次接触到sklearn，通常会被其丰富且方便的算法模型库吸引，但是这里介绍的特征处理库也十分强大！\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Cp\\\\u003E\\u3000\\u3000本文中使用sklearn中的\\\\u003Ca href=\\\\\"https:\\\\u002F\\\\u002Flink.zhihu.com\\\\u002F?target=http%3A\\\\u002F\\\\u002Fscikit-learn.org\\\\u002Fstable\\\\u002Fmodules\\\\u002Fgenerated\\\\u002Fsklearn.datasets.load_iris.html%23sklearn.datasets.load_iris\\\\\" class=\\\\\" wrap external\\\\\" target=\\\\\"_blank\\\\\" rel=\\\\\"nofollow noreferrer\\\\\"\\\\u003EIRIS（鸢尾花）数据集\\\\u003C\\\\u002Fa\\\\u003E来对特征处理功能进行说明。IRIS数据集由Fisher在1936年整理，包含4个特征（Sepal.Length（花萼长度）、Sepal.Width（花萼宽度）、Petal.Length（花瓣长度）、Petal.Width（花瓣宽度）），特征值都为正浮点数，单位为厘米。目标值为鸢尾花的分类（Iris Setosa（山鸢尾）、Iris Versicolour（杂色鸢尾），Iris Virginica（维吉尼亚鸢尾））。导入IRIS数据集的代码如下：\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Cdiv class=\\\\\"highlight\\\\\"\\\\u003E\\\\u003Cpre\\\\u003E\\\\u003Ccode class=\\\\\"language-python\\\\\"\\\\u003E\\\\u003Cspan class=\\\\\"kn\\\\\"\\\\u003Efrom\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"nn\\\\\"\\\\u003Esklearn.datasets\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"kn\\\\\"\\\\u003Eimport\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Eload_iris\\\\u003C\\\\u002Fspan\\\\u003E\\\\n \\\\n\\\\u003Cspan class=\\\\\"c1\\\\\"\\\\u003E#导入IRIS数据集\\\\u003C\\\\u002Fspan\\\\u003E\\\\n\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Eiris\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E=\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Eload_iris\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E()\\\\u003C\\\\u002Fspan\\\\u003E\\\\n\\\\n\\\\u003Cspan class=\\\\\"c1\\\\\"\\\\u003E#特征矩阵\\\\u003C\\\\u002Fspan\\\\u003E\\\\n\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Eiris\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E.\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Edata\\\\u003C\\\\u002Fspan\\\\u003E\\\\n\\\\n\\\\u003Cspan class=\\\\\"c1\\\\\"\\\\u003E#目标向量\\\\u003C\\\\u002Fspan\\\\u003E\\\\n\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Eiris\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E.\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Etarget\\\\u003C\\\\u002Fspan\\\\u003E\\\\n\\\\u003C\\\\u002Fcode\\\\u003E\\\\u003C\\\\u002Fpre\\\\u003E\\\\u003C\\\\u002Fdiv\\\\u003E\\\\u003Cbr\\\\u002F\\\\u003E\\\\u003Cb\\\\u003E2 数据预处理\\\\u003C\\\\u002Fb\\\\u003E\\\\u003Cp\\\\u003E\\u3000\\u3000通过特征提取，我们能得到未经处理的特征，这时的特征可能有以下问题：\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Cul\\\\u003E\\\\u003Cli\\\\u003E不属于同一量纲：即特征的规格不一样，不能够放在一起比较。无量纲化可以解决这一问题。\\\\u003C\\\\u002Fli\\\\u003E\\\\u003Cli\\\\u003E信息冗余：对于某些定量特征，其包含的有效信息为区间划分，例如学习成绩，假若只关心“及格”或不“及格”，那么需要将定量的考分，转换成“1”和“0”表示及格和未及格。二值化可以解决这一问题。\\\\u003C\\\\u002Fli\\\\u003E\\\\u003Cli\\\\u003E定性特征不能直接使用：某些机器学习算法和模型只能接受定量特征的输入，那么需要将定性特征转换为定量特征。最简单的方式是为每一种定性值指定一个定量值，但是这种方式过于灵活，增加了调参的工作。\\\\u003Ca href=\\\\\"https:\\\\u002F\\\\u002Flink.zhihu.com\\\\u002F?target=http%3A\\\\u002F\\\\u002Fwww.ats.ucla.edu\\\\u002Fstat\\\\u002Fmult_pkg\\\\u002Ffaq\\\\u002Fgeneral\\\\u002Fdummy.htm\\\\\" class=\\\\\" wrap external\\\\\" target=\\\\\"_blank\\\\\" rel=\\\\\"nofollow noreferrer\\\\\"\\\\u003E通常使用哑编码的方式将定性特征转换为定量特征\\\\u003C\\\\u002Fa\\\\u003E：假设有N种定性值，则将这一个特征扩展为N种特征，当原始特征值为第i种定性值时，第i个扩展特征赋值为1，其他扩展特征赋值为0。哑编码的方式相比直接指定的方式，不用增加调参的工作，对于线性模型来说，使用哑编码后的特征可达到非线性的效果。\\\\u003C\\\\u002Fli\\\\u003E\\\\u003Cli\\\\u003E存在缺失值：缺失值需要补充。\\\\u003C\\\\u002Fli\\\\u003E\\\\u003Cli\\\\u003E信息利用率低：不同的机器学习算法和模型对数据中信息的利用是不同的，之前提到在线性模型中，使用对定性特征哑编码可以达到非线性的效果。类似地，对定量变量多项式化，或者进行其他的转换，都能达到非线性的效果。\\\\u003C\\\\u002Fli\\\\u003E\\\\u003C\\\\u002Ful\\\\u003E\\\\u003Cp\\\\u003E\\u3000\\u3000我们使用sklearn中的preproccessing库来进行数据预处理，可以覆盖以上问题的解决方案。\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Cb\\\\u003E2.1 无量纲化\\\\u003C\\\\u002Fb\\\\u003E\\\\u003Cp\\\\u003E\\u3000\\u3000无量纲化使不同规格的数据转换到同一规格。常见的无量纲化方法有标准化和区间缩放法。标准化的前提是特征值服从正态分布，标准化后，其转换成标准正态分布。区间缩放法利用了边界值信息，将特征的取值区间缩放到某个特点的范围，例如[0, 1]等。\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Cb\\\\u003E2.1.1 标准化\\\\u003C\\\\u002Fb\\\\u003E\\\\u003Cp\\\\u003E\\u3000\\u3000标准化需要计算特征的均值和标准差，公式表达为：\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Cp\\\\u003E\\u3000\\u3000使用preproccessing库的StandardScaler类对数据进行标准化的代码如下：\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Cfigure\\\\u003E\\\\u003Cnoscript\\\\u003E\\\\u003Cimg src=\\\\\"https:\\\\u002F\\\\u002Fpic4.zhimg.com\\\\u002F50\\\\u002Fc7e852db6bd05b7bb1017b5425ffeec1_hd.jpg\\\\\" data-rawwidth=\\\\\"81\\\\\" data-rawheight=\\\\\"48\\\\\" class=\\\\\"content_image\\\\\" width=\\\\\"81\\\\\"\\\\u002F\\\\u003E\\\\u003C\\\\u002Fnoscript\\\\u003E\\\\u003Cimg src=\\\\\"data:image\\\\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\\\\u002F\\\\u002Fwww.w3.org\\\\u002F2000\\\\u002Fsvg&#39; width=&#39;81&#39; height=&#39;48&#39;&gt;&lt;\\\\u002Fsvg&gt;\\\\\" data-rawwidth=\\\\\"81\\\\\" data-rawheight=\\\\\"48\\\\\" class=\\\\\"content_image lazy\\\\\" width=\\\\\"81\\\\\" data-actualsrc=\\\\\"https:\\\\u002F\\\\u002Fpic4.zhimg.com\\\\u002F50\\\\u002Fc7e852db6bd05b7bb1017b5425ffeec1_hd.jpg\\\\\"\\\\u002F\\\\u003E\\\\u003C\\\\u002Ffigure\\\\u003E\\\\u003Cdiv class=\\\\\"highlight\\\\\"\\\\u003E\\\\u003Cpre\\\\u003E\\\\u003Ccode class=\\\\\"language-python\\\\\"\\\\u003E\\\\u003Cspan class=\\\\\"kn\\\\\"\\\\u003Efrom\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"nn\\\\\"\\\\u003Esklearn.preprocessing\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"kn\\\\\"\\\\u003Eimport\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003EStandardScaler\\\\u003C\\\\u002Fspan\\\\u003E\\\\n \\\\n\\\\u003Cspan class=\\\\\"c1\\\\\"\\\\u003E#标准化，返回值为标准化后的数据\\\\u003C\\\\u002Fspan\\\\u003E\\\\n\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003EStandardScaler\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E()\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E.\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Efit_transform\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E(\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Eiris\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E.\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Edata\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E)\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003C\\\\u002Fcode\\\\u003E\\\\u003C\\\\u002Fpre\\\\u003E\\\\u003C\\\\u002Fdiv\\\\u003E\\\\u003Cb\\\\u003E2.1.2 区间缩放法\\\\u003C\\\\u002Fb\\\\u003E\\\\u003Cp\\\\u003E\\u3000\\u3000区间缩放法的思路有多种，常见的一种为利用两个最值进行缩放，公式表达为：\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Cp\\\\u003E\\u3000\\u3000使用preproccessing库的MinMaxScaler类对数据进行区间缩放的代码如下：\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Cfigure\\\\u003E\\\\u003Cnoscript\\\\u003E\\\\u003Cimg src=\\\\\"https:\\\\u002F\\\\u002Fpic2.zhimg.com\\\\u002F50\\\\u002F0f119a8e8f69509c5b95ef6a8a01a809_hd.jpg\\\\\" data-rawwidth=\\\\\"119\\\\\" data-rawheight=\\\\\"52\\\\\" class=\\\\\"content_image\\\\\" width=\\\\\"119\\\\\"\\\\u002F\\\\u003E\\\\u003C\\\\u002Fnoscript\\\\u003E\\\\u003Cimg src=\\\\\"data:image\\\\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\\\\u002F\\\\u002Fwww.w3.org\\\\u002F2000\\\\u002Fsvg&#39; width=&#39;119&#39; height=&#39;52&#39;&gt;&lt;\\\\u002Fsvg&gt;\\\\\" data-rawwidth=\\\\\"119\\\\\" data-rawheight=\\\\\"52\\\\\" class=\\\\\"content_image lazy\\\\\" width=\\\\\"119\\\\\" data-actualsrc=\\\\\"https:\\\\u002F\\\\u002Fpic2.zhimg.com\\\\u002F50\\\\u002F0f119a8e8f69509c5b95ef6a8a01a809_hd.jpg\\\\\"\\\\u002F\\\\u003E\\\\u003C\\\\u002Ffigure\\\\u003E\\\\u003Cdiv class=\\\\\"highlight\\\\\"\\\\u003E\\\\u003Cpre\\\\u003E\\\\u003Ccode class=\\\\\"language-text\\\\\"\\\\u003Efrom sklearn.preprocessing import MinMaxScaler\\\\n\\\\n#区间缩放，返回值为缩放到[0, 1]区间的数据\\\\nMinMaxScaler().fit_transform(iris.data)\\\\u003C\\\\u002Fcode\\\\u003E\\\\u003C\\\\u002Fpre\\\\u003E\\\\u003C\\\\u002Fdiv\\\\u003E\\\\u003Cb\\\\u003E2.1.3 标准化与归一化的区别\\\\u003C\\\\u002Fb\\\\u003E\\\\u003Cp\\\\u003E\\u3000\\u3000简单来说，标准化是依照特征矩阵的列处理数据，其通过求z-score的方法，将样本的特征值转换到同一量纲下。归一化是依照特征矩阵的行处理数据，其目的在于样本向量在点乘运算或其他核函数计算相似性时，拥有统一的标准，也就是说都转化为“单位向量”。规则为l2的归一化公式如下：\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Cbr\\\\u002F\\\\u003E\\\\u003Cfigure\\\\u003E\\\\u003Cnoscript\\\\u003E\\\\u003Cimg src=\\\\\"https:\\\\u002F\\\\u002Fpic3.zhimg.com\\\\u002F50\\\\u002Ffbb2fd0a163f2fa211829b735194baac_hd.jpg\\\\\" data-rawwidth=\\\\\"113\\\\\" data-rawheight=\\\\\"57\\\\\" class=\\\\\"content_image\\\\\" width=\\\\\"113\\\\\"\\\\u002F\\\\u003E\\\\u003C\\\\u002Fnoscript\\\\u003E\\\\u003Cimg src=\\\\\"data:image\\\\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\\\\u002F\\\\u002Fwww.w3.org\\\\u002F2000\\\\u002Fsvg&#39; width=&#39;113&#39; height=&#39;57&#39;&gt;&lt;\\\\u002Fsvg&gt;\\\\\" data-rawwidth=\\\\\"113\\\\\" data-rawheight=\\\\\"57\\\\\" class=\\\\\"content_image lazy\\\\\" width=\\\\\"113\\\\\" data-actualsrc=\\\\\"https:\\\\u002F\\\\u002Fpic3.zhimg.com\\\\u002F50\\\\u002Ffbb2fd0a163f2fa211829b735194baac_hd.jpg\\\\\"\\\\u002F\\\\u003E\\\\u003C\\\\u002Ffigure\\\\u003E\\\\u003Cbr\\\\u002F\\\\u003E\\\\u003Cp\\\\u003E\\u3000\\u3000使用preproccessing库的Normalizer类对数据进行归一化的代码如下：\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Cdiv class=\\\\\"highlight\\\\\"\\\\u003E\\\\u003Cpre\\\\u003E\\\\u003Ccode class=\\\\\"language-python\\\\\"\\\\u003E\\\\u003Cspan class=\\\\\"kn\\\\\"\\\\u003Efrom\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"nn\\\\\"\\\\u003Esklearn.preprocessing\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"kn\\\\\"\\\\u003Eimport\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003ENormalizer\\\\u003C\\\\u002Fspan\\\\u003E\\\\n\\\\n\\\\u003Cspan class=\\\\\"c1\\\\\"\\\\u003E#归一化，返回值为归一化后的数据\\\\u003C\\\\u002Fspan\\\\u003E\\\\n\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003ENormalizer\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E()\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E.\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Efit_transform\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E(\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Eiris\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E.\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Edata\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E)\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003C\\\\u002Fcode\\\\u003E\\\\u003C\\\\u002Fpre\\\\u003E\\\\u003C\\\\u002Fdiv\\\\u003E\\\\u003Cb\\\\u003E2.2 对定量特征二值化\\\\u003C\\\\u002Fb\\\\u003E\\\\u003Cp\\\\u003E\\u3000\\u3000定量特征二值化的核心在于设定一个阈值，大于阈值的赋值为1，小于等于阈值的赋值为0，公式表达如下：\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Cfigure\\\\u003E\\\\u003Cnoscript\\\\u003E\\\\u003Cimg src=\\\\\"https:\\\\u002F\\\\u002Fpic4.zhimg.com\\\\u002F50\\\\u002F11111244c5b69c1af6c034496a2591ad_hd.jpg\\\\\" data-rawwidth=\\\\\"159\\\\\" data-rawheight=\\\\\"41\\\\\" class=\\\\\"content_image\\\\\" width=\\\\\"159\\\\\"\\\\u002F\\\\u003E\\\\u003C\\\\u002Fnoscript\\\\u003E\\\\u003Cimg src=\\\\\"data:image\\\\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\\\\u002F\\\\u002Fwww.w3.org\\\\u002F2000\\\\u002Fsvg&#39; width=&#39;159&#39; height=&#39;41&#39;&gt;&lt;\\\\u002Fsvg&gt;\\\\\" data-rawwidth=\\\\\"159\\\\\" data-rawheight=\\\\\"41\\\\\" class=\\\\\"content_image lazy\\\\\" width=\\\\\"159\\\\\" data-actualsrc=\\\\\"https:\\\\u002F\\\\u002Fpic4.zhimg.com\\\\u002F50\\\\u002F11111244c5b69c1af6c034496a2591ad_hd.jpg\\\\\"\\\\u002F\\\\u003E\\\\u003C\\\\u002Ffigure\\\\u003E\\\\u003Cp\\\\u003E\\u3000\\u3000使用preproccessing库的Binarizer类对数据进行二值化的代码如下：\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Cdiv class=\\\\\"highlight\\\\\"\\\\u003E\\\\u003Cpre\\\\u003E\\\\u003Ccode class=\\\\\"language-python\\\\\"\\\\u003E\\\\u003Cspan class=\\\\\"kn\\\\\"\\\\u003Efrom\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"nn\\\\\"\\\\u003Esklearn.preprocessing\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"kn\\\\\"\\\\u003Eimport\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003EBinarizer\\\\u003C\\\\u002Fspan\\\\u003E\\\\n\\\\n\\\\u003Cspan class=\\\\\"c1\\\\\"\\\\u003E#二值化，阈值设置为3，返回值为二值化后的数据\\\\u003C\\\\u002Fspan\\\\u003E\\\\n\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003EBinarizer\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E(\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Ethreshold\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E=\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"mi\\\\\"\\\\u003E3\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E)\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E.\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Efit_transform\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E(\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Eiris\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E.\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Edata\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E)\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003C\\\\u002Fcode\\\\u003E\\\\u003C\\\\u002Fpre\\\\u003E\\\\u003C\\\\u002Fdiv\\\\u003E\\\\u003Cb\\\\u003E2.3 对定性特征哑编码\\\\u003C\\\\u002Fb\\\\u003E\\\\u003Cp\\\\u003E\\u3000\\u3000由于IRIS数据集的特征皆为定量特征，故使用其目标值进行哑编码（实际上是不需要的）。使用preproccessing库的OneHotEncoder类对数据进行哑编码的代码如下：\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Cdiv class=\\\\\"highlight\\\\\"\\\\u003E\\\\u003Cpre\\\\u003E\\\\u003Ccode class=\\\\\"language-python\\\\\"\\\\u003E\\\\u003Cspan class=\\\\\"kn\\\\\"\\\\u003Efrom\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"nn\\\\\"\\\\u003Esklearn.preprocessing\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"kn\\\\\"\\\\u003Eimport\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003EOneHotEncoder\\\\u003C\\\\u002Fspan\\\\u003E\\\\n\\\\n\\\\u003Cspan class=\\\\\"c1\\\\\"\\\\u003E#哑编码，对IRIS数据集的目标值，返回值为哑编码后的数据\\\\u003C\\\\u002Fspan\\\\u003E\\\\n\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003EOneHotEncoder\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E()\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E.\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Efit_transform\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E(\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Eiris\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E.\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Etarget\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E.\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Ereshape\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E((\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E-\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"mi\\\\\"\\\\u003E1\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E,\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"mi\\\\\"\\\\u003E1\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E)))\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003C\\\\u002Fcode\\\\u003E\\\\u003C\\\\u002Fpre\\\\u003E\\\\u003C\\\\u002Fdiv\\\\u003E\\\\u003Cb\\\\u003E2.4 缺失值计算\\\\u003C\\\\u002Fb\\\\u003E\\\\u003Cp\\\\u003E\\u3000\\u3000由于IRIS数据集没有缺失值，故对数据集新增一个样本，4个特征均赋值为NaN，表示数据缺失。使用preproccessing库的Imputer类对数据进行缺失值计算的代码如下：\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Cdiv class=\\\\\"highlight\\\\\"\\\\u003E\\\\u003Cpre\\\\u003E\\\\u003Ccode class=\\\\\"language-python\\\\\"\\\\u003E\\\\u003Cspan class=\\\\\"kn\\\\\"\\\\u003Efrom\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"nn\\\\\"\\\\u003Enumpy\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"kn\\\\\"\\\\u003Eimport\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Evstack\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E,\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Earray\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E,\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Enan\\\\u003C\\\\u002Fspan\\\\u003E\\\\n\\\\u003Cspan class=\\\\\"kn\\\\\"\\\\u003Efrom\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"nn\\\\\"\\\\u003Esklearn.preprocessing\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"kn\\\\\"\\\\u003Eimport\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003EImputer\\\\u003C\\\\u002Fspan\\\\u003E\\\\n\\\\n\\\\u003Cspan class=\\\\\"c1\\\\\"\\\\u003E#缺失值计算，返回值为计算缺失值后的数据\\\\u003C\\\\u002Fspan\\\\u003E\\\\n\\\\u003Cspan class=\\\\\"c1\\\\\"\\\\u003E#参数missing_value为缺失值的表示形式，默认为NaN\\\\u003C\\\\u002Fspan\\\\u003E\\\\n\\\\u003Cspan class=\\\\\"c1\\\\\"\\\\u003E#参数strategy为缺失值填充方式，默认为mean（均值）\\\\u003C\\\\u002Fspan\\\\u003E\\\\n\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003EImputer\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E()\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E.\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Efit_transform\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E(\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Evstack\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E((\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Earray\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E([\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Enan\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E,\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Enan\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E,\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Enan\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E,\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Enan\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E]),\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Eiris\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E.\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Edata\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E)))\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003C\\\\u002Fcode\\\\u003E\\\\u003C\\\\u002Fpre\\\\u003E\\\\u003C\\\\u002Fdiv\\\\u003E\\\\u003Cb\\\\u003E2.5 数据变换\\\\u003C\\\\u002Fb\\\\u003E\\\\u003Cp\\\\u003E\\u3000\\u3000常见的数据变换有基于多项式的、基于指数函数的、基于对数函数的。4个特征，度为2的多项式转换公式如下：\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Cfigure\\\\u003E\\\\u003Cnoscript\\\\u003E\\\\u003Cimg src=\\\\\"https:\\\\u002F\\\\u002Fpic2.zhimg.com\\\\u002F50\\\\u002Fd1c57a66fad39df90b87cea330efb3f3_hd.jpg\\\\\" data-rawwidth=\\\\\"571\\\\\" data-rawheight=\\\\\"57\\\\\" class=\\\\\"origin_image zh-lightbox-thumb\\\\\" width=\\\\\"571\\\\\" data-original=\\\\\"https:\\\\u002F\\\\u002Fpic3.zhimg.com\\\\u002Fd1c57a66fad39df90b87cea330efb3f3_r.jpg\\\\\"\\\\u002F\\\\u003E\\\\u003C\\\\u002Fnoscript\\\\u003E\\\\u003Cimg src=\\\\\"data:image\\\\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\\\\u002F\\\\u002Fwww.w3.org\\\\u002F2000\\\\u002Fsvg&#39; width=&#39;571&#39; height=&#39;57&#39;&gt;&lt;\\\\u002Fsvg&gt;\\\\\" data-rawwidth=\\\\\"571\\\\\" data-rawheight=\\\\\"57\\\\\" class=\\\\\"origin_image zh-lightbox-thumb lazy\\\\\" width=\\\\\"571\\\\\" data-original=\\\\\"https:\\\\u002F\\\\u002Fpic3.zhimg.com\\\\u002Fd1c57a66fad39df90b87cea330efb3f3_r.jpg\\\\\" data-actualsrc=\\\\\"https:\\\\u002F\\\\u002Fpic2.zhimg.com\\\\u002F50\\\\u002Fd1c57a66fad39df90b87cea330efb3f3_hd.jpg\\\\\"\\\\u002F\\\\u003E\\\\u003C\\\\u002Ffigure\\\\u003E\\\\u003Cp\\\\u003E\\u3000\\u3000使用preproccessing库的PolynomialFeatures类对数据进行多项式转换的代码如下：\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Cdiv class=\\\\\"highlight\\\\\"\\\\u003E\\\\u003Cpre\\\\u003E\\\\u003Ccode class=\\\\\"language-python\\\\\"\\\\u003E\\\\u003Cspan class=\\\\\"kn\\\\\"\\\\u003Efrom\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"nn\\\\\"\\\\u003Esklearn.preprocessing\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"kn\\\\\"\\\\u003Eimport\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003EPolynomialFeatures\\\\u003C\\\\u002Fspan\\\\u003E\\\\n\\\\n\\\\u003Cspan class=\\\\\"c1\\\\\"\\\\u003E#多项式转换\\\\u003C\\\\u002Fspan\\\\u003E\\\\n\\\\u003Cspan class=\\\\\"c1\\\\\"\\\\u003E#参数degree为度，默认值为2\\\\u003C\\\\u002Fspan\\\\u003E\\\\n\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003EPolynomialFeatures\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E()\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E.\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Efit_transform\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E(\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Eiris\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E.\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Edata\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E)\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003C\\\\u002Fcode\\\\u003E\\\\u003C\\\\u002Fpre\\\\u003E\\\\u003C\\\\u002Fdiv\\\\u003E\\\\u003Cp\\\\u003E\\u3000\\u3000基于单变元函数的数据变换可以使用一个统一的方式完成，使用preproccessing库的FunctionTransformer对数据进行对数函数转换的代码如下：\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Cdiv class=\\\\\"highlight\\\\\"\\\\u003E\\\\u003Cpre\\\\u003E\\\\u003Ccode class=\\\\\"language-python\\\\\"\\\\u003E\\\\u003Cspan class=\\\\\"kn\\\\\"\\\\u003Efrom\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"nn\\\\\"\\\\u003Enumpy\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"kn\\\\\"\\\\u003Eimport\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Elog1p\\\\u003C\\\\u002Fspan\\\\u003E\\\\n\\\\u003Cspan class=\\\\\"kn\\\\\"\\\\u003Efrom\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"nn\\\\\"\\\\u003Esklearn.preprocessing\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"kn\\\\\"\\\\u003Eimport\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003EFunctionTransformer\\\\u003C\\\\u002Fspan\\\\u003E\\\\n\\\\n\\\\u003Cspan class=\\\\\"c1\\\\\"\\\\u003E#自定义转换函数为对数函数的数据变换\\\\u003C\\\\u002Fspan\\\\u003E\\\\n\\\\u003Cspan class=\\\\\"c1\\\\\"\\\\u003E#第一个参数是单变元函数\\\\u003C\\\\u002Fspan\\\\u003E\\\\n\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003EFunctionTransformer\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E(\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Elog1p\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E)\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E.\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Efit_transform\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E(\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Eiris\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E.\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Edata\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E)\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003C\\\\u002Fcode\\\\u003E\\\\u003C\\\\u002Fpre\\\\u003E\\\\u003C\\\\u002Fdiv\\\\u003E\\\\u003Cb\\\\u003E\\\\u003Cbr\\\\u002F\\\\u003E3 特征选择\\\\u003C\\\\u002Fb\\\\u003E\\\\u003Cp\\\\u003E\\u3000\\u3000当数据预处理完成后，我们需要选择有意义的特征输入机器学习的算法和模型进行训练。通常来说，从两个方面考虑来选择特征：\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Cul\\\\u003E\\\\u003Cli\\\\u003E特征是否发散：如果一个特征不发散，例如方差接近于0，也就是说样本在这个特征上基本上没有差异，这个特征对于样本的区分并没有什么用。\\\\u003C\\\\u002Fli\\\\u003E\\\\u003Cli\\\\u003E特征与目标的相关性：这点比较显见，与目标相关性高的特征，应当优选选择。除方差法外，本文介绍的其他方法均从相关性考虑。\\\\u003C\\\\u002Fli\\\\u003E\\\\u003C\\\\u002Ful\\\\u003E\\\\u003Cp\\\\u003E\\u3000\\u3000根据特征选择的形式又可以将特征选择方法分为3种：\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Cul\\\\u003E\\\\u003Cli\\\\u003EFilter：过滤法，按照发散性或者相关性对各个特征进行评分，设定阈值或者待选择阈值的个数，选择特征。\\\\u003C\\\\u002Fli\\\\u003E\\\\u003Cli\\\\u003EWrapper：包装法，根据目标函数（通常是预测效果评分），每次选择若干特征，或者排除若干特征。\\\\u003C\\\\u002Fli\\\\u003E\\\\u003Cli\\\\u003EEmbedded：集成法，先使用某些机器学习的算法和模型进行训练，得到各个特征的权值系数，根据系数从大到小选择特征。类似于Filter方法，但是是通过训练来确定特征的优劣。\\\\u003C\\\\u002Fli\\\\u003E\\\\u003C\\\\u002Ful\\\\u003E\\\\u003Cp\\\\u003E\\u3000\\u3000我们使用sklearn中的feature_selection库来进行特征选择。\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Cb\\\\u003E3.1 Filter\\\\u003C\\\\u002Fb\\\\u003E\\\\u003Cbr\\\\u002F\\\\u003E\\\\u003Cb\\\\u003E3.1.1 方差选择法\\\\u003C\\\\u002Fb\\\\u003E\\\\u003Cp\\\\u003E\\u3000\\u3000使用方差选择法，先要计算各个特征的方差，然后根据阈值，选择方差大于阈值的特征。使用feature_selection库的VarianceThreshold类来选择特征的代码如下：\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Cdiv class=\\\\\"highlight\\\\\"\\\\u003E\\\\u003Cpre\\\\u003E\\\\u003Ccode class=\\\\\"language-python\\\\\"\\\\u003E\\\\u003Cspan class=\\\\\"kn\\\\\"\\\\u003Efrom\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"nn\\\\\"\\\\u003Esklearn.feature_selection\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"kn\\\\\"\\\\u003Eimport\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003EVarianceThreshold\\\\u003C\\\\u002Fspan\\\\u003E\\\\n\\\\n\\\\u003Cspan class=\\\\\"c1\\\\\"\\\\u003E#方差选择法，返回值为特征选择后的数据\\\\u003C\\\\u002Fspan\\\\u003E\\\\n\\\\u003Cspan class=\\\\\"c1\\\\\"\\\\u003E#参数threshold为方差的阈值\\\\u003C\\\\u002Fspan\\\\u003E\\\\n\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003EVarianceThreshold\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E(\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Ethreshold\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E=\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"mi\\\\\"\\\\u003E3\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E)\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E.\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Efit_transform\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E(\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Eiris\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E.\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Edata\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E)\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003C\\\\u002Fcode\\\\u003E\\\\u003C\\\\u002Fpre\\\\u003E\\\\u003C\\\\u002Fdiv\\\\u003E\\\\u003Cb\\\\u003E3.1.2 相关系数法\\\\u003C\\\\u002Fb\\\\u003E\\\\u003Cp\\\\u003E\\u3000\\u3000使用相关系数法，先要计算各个特征对目标值的相关系数以及相关系数的P值。用feature_selection库的SelectKBest类结合相关系数来选择特征的代码如下：\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Cdiv class=\\\\\"highlight\\\\\"\\\\u003E\\\\u003Cpre\\\\u003E\\\\u003Ccode class=\\\\\"language-python\\\\\"\\\\u003E\\\\u003Cspan class=\\\\\"kn\\\\\"\\\\u003Efrom\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"nn\\\\\"\\\\u003Esklearn.feature_selection\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"kn\\\\\"\\\\u003Eimport\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003ESelectKBest\\\\u003C\\\\u002Fspan\\\\u003E\\\\n\\\\u003Cspan class=\\\\\"kn\\\\\"\\\\u003Efrom\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"nn\\\\\"\\\\u003Escipy.stats\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"kn\\\\\"\\\\u003Eimport\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Epearsonr\\\\u003C\\\\u002Fspan\\\\u003E\\\\n\\\\n\\\\u003Cspan class=\\\\\"c1\\\\\"\\\\u003E#选择K个最好的特征，返回选择特征后的数据\\\\u003C\\\\u002Fspan\\\\u003E\\\\n\\\\u003Cspan class=\\\\\"c1\\\\\"\\\\u003E#第一个参数为计算评估特征是否好的函数，该函数输入特征矩阵和目标向量，输出二元组（评分，P值）的数组，数组第i项为第i个特征的评分和P值。在此定义为计算相关系数\\\\u003C\\\\u002Fspan\\\\u003E\\\\n\\\\u003Cspan class=\\\\\"c1\\\\\"\\\\u003E#参数k为选择的特征个数\\\\u003C\\\\u002Fspan\\\\u003E\\\\n\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003ESelectKBest\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E(\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"k\\\\\"\\\\u003Elambda\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003EX\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E,\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003EY\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E:\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Earray\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E(\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"nb\\\\\"\\\\u003Emap\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E(\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"k\\\\\"\\\\u003Elambda\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Ex\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E:\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Epearsonr\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E(\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Ex\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E,\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003EY\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E),\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003EX\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E.\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003ET\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E))\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E.\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003ET\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E,\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Ek\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E=\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"mi\\\\\"\\\\u003E2\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E)\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E.\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Efit_transform\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E(\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Eiris\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E.\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Edata\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E,\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Eiris\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E.\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Etarget\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E)\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003C\\\\u002Fcode\\\\u003E\\\\u003C\\\\u002Fpre\\\\u003E\\\\u003C\\\\u002Fdiv\\\\u003E\\\\u003Cb\\\\u003E3.1.3 卡方检验\\\\u003C\\\\u002Fb\\\\u003E\\\\u003Cp\\\\u003E\\u3000\\u3000经典的卡方检验是检验定性自变量对定性因变量的相关性。假设自变量有N种取值，因变量有M种取值，考虑自变量等于i且因变量等于j的样本频数的观察值与期望的差距，构建统计量：\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Cfigure\\\\u003E\\\\u003Cnoscript\\\\u003E\\\\u003Cimg src=\\\\\"https:\\\\u002F\\\\u002Fpic3.zhimg.com\\\\u002F50\\\\u002F7bc586c806b9b8bf1e74433a2e1976bc_hd.jpg\\\\\" data-rawwidth=\\\\\"162\\\\\" data-rawheight=\\\\\"48\\\\\" class=\\\\\"content_image\\\\\" width=\\\\\"162\\\\\"\\\\u002F\\\\u003E\\\\u003C\\\\u002Fnoscript\\\\u003E\\\\u003Cimg src=\\\\\"data:image\\\\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\\\\u002F\\\\u002Fwww.w3.org\\\\u002F2000\\\\u002Fsvg&#39; width=&#39;162&#39; height=&#39;48&#39;&gt;&lt;\\\\u002Fsvg&gt;\\\\\" data-rawwidth=\\\\\"162\\\\\" data-rawheight=\\\\\"48\\\\\" class=\\\\\"content_image lazy\\\\\" width=\\\\\"162\\\\\" data-actualsrc=\\\\\"https:\\\\u002F\\\\u002Fpic3.zhimg.com\\\\u002F50\\\\u002F7bc586c806b9b8bf1e74433a2e1976bc_hd.jpg\\\\\"\\\\u002F\\\\u003E\\\\u003C\\\\u002Ffigure\\\\u003E\\\\u003Cp\\\\u003E\\u3000\\u3000不难发现，\\\\u003Ca href=\\\\\"https:\\\\u002F\\\\u002Flink.zhihu.com\\\\u002F?target=http%3A\\\\u002F\\\\u002Fwiki.mbalib.com\\\\u002Fwiki\\\\u002F%25E5%258D%25A1%25E6%2596%25B9%25E6%25A3%2580%25E9%25AA%258C\\\\\" class=\\\\\" wrap external\\\\\" target=\\\\\"_blank\\\\\" rel=\\\\\"nofollow noreferrer\\\\\"\\\\u003E这个统计量的含义简而言之就是自变量对因变量的相关性\\\\u003C\\\\u002Fa\\\\u003E。用feature_selection库的SelectKBest类结合卡方检验来选择特征的代码如下：\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Cdiv class=\\\\\"highlight\\\\\"\\\\u003E\\\\u003Cpre\\\\u003E\\\\u003Ccode class=\\\\\"language-python\\\\\"\\\\u003E\\\\u003Cspan class=\\\\\"kn\\\\\"\\\\u003Efrom\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"nn\\\\\"\\\\u003Esklearn.feature_selection\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"kn\\\\\"\\\\u003Eimport\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003ESelectKBest\\\\u003C\\\\u002Fspan\\\\u003E\\\\n\\\\u003Cspan class=\\\\\"kn\\\\\"\\\\u003Efrom\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"nn\\\\\"\\\\u003Esklearn.feature_selection\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"kn\\\\\"\\\\u003Eimport\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Echi2\\\\u003C\\\\u002Fspan\\\\u003E\\\\n\\\\n\\\\u003Cspan class=\\\\\"c1\\\\\"\\\\u003E#选择K个最好的特征，返回选择特征后的数据\\\\u003C\\\\u002Fspan\\\\u003E\\\\n\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003ESelectKBest\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E(\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Echi2\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E,\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Ek\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E=\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"mi\\\\\"\\\\u003E2\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E)\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E.\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Efit_transform\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E(\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Eiris\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E.\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Edata\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E,\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Eiris\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E.\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Etarget\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E)\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003C\\\\u002Fcode\\\\u003E\\\\u003C\\\\u002Fpre\\\\u003E\\\\u003C\\\\u002Fdiv\\\\u003E\\\\u003Cb\\\\u003E3.1.4 互信息法\\\\u003C\\\\u002Fb\\\\u003E\\\\u003Cp\\\\u003E\\u3000\\u3000经典的互信息也是评价定性自变量对定性因变量的相关性的，互信息计算公式如下：\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Cfigure\\\\u003E\\\\u003Cnoscript\\\\u003E\\\\u003Cimg src=\\\\\"https:\\\\u002F\\\\u002Fpic4.zhimg.com\\\\u002F50\\\\u002F6af9a077b49f587a5d149f5dc51073ba_hd.jpg\\\\\" data-rawwidth=\\\\\"274\\\\\" data-rawheight=\\\\\"50\\\\\" class=\\\\\"content_image\\\\\" width=\\\\\"274\\\\\"\\\\u002F\\\\u003E\\\\u003C\\\\u002Fnoscript\\\\u003E\\\\u003Cimg src=\\\\\"data:image\\\\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\\\\u002F\\\\u002Fwww.w3.org\\\\u002F2000\\\\u002Fsvg&#39; width=&#39;274&#39; height=&#39;50&#39;&gt;&lt;\\\\u002Fsvg&gt;\\\\\" data-rawwidth=\\\\\"274\\\\\" data-rawheight=\\\\\"50\\\\\" class=\\\\\"content_image lazy\\\\\" width=\\\\\"274\\\\\" data-actualsrc=\\\\\"https:\\\\u002F\\\\u002Fpic4.zhimg.com\\\\u002F50\\\\u002F6af9a077b49f587a5d149f5dc51073ba_hd.jpg\\\\\"\\\\u002F\\\\u003E\\\\u003C\\\\u002Ffigure\\\\u003E\\\\u003Cp\\\\u003E\\u3000\\u3000为了处理定量数据，最大信息系数法被提出，使用feature_selection库的SelectKBest类结合最大信息系数法来选择特征的代码如下：\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Cdiv class=\\\\\"highlight\\\\\"\\\\u003E\\\\u003Cpre\\\\u003E\\\\u003Ccode class=\\\\\"language-python\\\\\"\\\\u003E \\\\u003Cspan class=\\\\\"kn\\\\\"\\\\u003Efrom\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"nn\\\\\"\\\\u003Esklearn.feature_selection\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"kn\\\\\"\\\\u003Eimport\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003ESelectKBest\\\\u003C\\\\u002Fspan\\\\u003E\\\\n \\\\u003Cspan class=\\\\\"kn\\\\\"\\\\u003Efrom\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"nn\\\\\"\\\\u003Eminepy\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"kn\\\\\"\\\\u003Eimport\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003EMINE\\\\u003C\\\\u002Fspan\\\\u003E\\\\n \\\\n \\\\u003Cspan class=\\\\\"c1\\\\\"\\\\u003E#由于MINE的设计不是函数式的，定义mic方法将其为函数式的，返回一个二元组，二元组的第2项设置成固定的P值0.5\\\\u003C\\\\u002Fspan\\\\u003E\\\\n \\\\u003Cspan class=\\\\\"k\\\\\"\\\\u003Edef\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"nf\\\\\"\\\\u003Emic\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E(\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Ex\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E,\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Ey\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E):\\\\u003C\\\\u002Fspan\\\\u003E\\\\n     \\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Em\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E=\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003EMINE\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E()\\\\u003C\\\\u002Fspan\\\\u003E\\\\n     \\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Em\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E.\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Ecompute_score\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E(\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Ex\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E,\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Ey\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E)\\\\u003C\\\\u002Fspan\\\\u003E\\\\n     \\\\u003Cspan class=\\\\\"k\\\\\"\\\\u003Ereturn\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E(\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Em\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E.\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Emic\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E(),\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"mf\\\\\"\\\\u003E0.5\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E)\\\\u003C\\\\u002Fspan\\\\u003E\\\\n\\\\n\\\\u003Cspan class=\\\\\"c1\\\\\"\\\\u003E#选择K个最好的特征，返回特征选择后的数据\\\\u003C\\\\u002Fspan\\\\u003E\\\\n\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003ESelectKBest\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E(\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"k\\\\\"\\\\u003Elambda\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003EX\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E,\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003EY\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E:\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Earray\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E(\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"nb\\\\\"\\\\u003Emap\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E(\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"k\\\\\"\\\\u003Elambda\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Ex\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E:\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Emic\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E(\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Ex\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E,\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003EY\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E),\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003EX\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E.\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003ET\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E))\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E.\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003ET\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E,\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Ek\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E=\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"mi\\\\\"\\\\u003E2\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E)\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E.\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Efit_transform\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E(\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Eiris\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E.\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Edata\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E,\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Eiris\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E.\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Etarget\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E)\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003C\\\\u002Fcode\\\\u003E\\\\u003C\\\\u002Fpre\\\\u003E\\\\u003C\\\\u002Fdiv\\\\u003E\\\\u003Cb\\\\u003E3.2 Wrapper\\\\u003C\\\\u002Fb\\\\u003E\\\\u003Cbr\\\\u002F\\\\u003E\\\\u003Cb\\\\u003E3.2.1 递归特征消除法\\\\u003C\\\\u002Fb\\\\u003E\\\\u003Cp\\\\u003E\\u3000\\u3000递归消除特征法使用一个基模型来进行多轮训练，每轮训练后，消除若干权值系数的特征，再基于新的特征集进行下一轮训练。使用feature_selection库的RFE类来选择特征的代码如下：\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Cdiv class=\\\\\"highlight\\\\\"\\\\u003E\\\\u003Cpre\\\\u003E\\\\u003Ccode class=\\\\\"language-python\\\\\"\\\\u003E\\\\u003Cspan class=\\\\\"kn\\\\\"\\\\u003Efrom\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"nn\\\\\"\\\\u003Esklearn.feature_selection\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"kn\\\\\"\\\\u003Eimport\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003ERFE\\\\u003C\\\\u002Fspan\\\\u003E\\\\n\\\\u003Cspan class=\\\\\"kn\\\\\"\\\\u003Efrom\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"nn\\\\\"\\\\u003Esklearn.linear_model\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"kn\\\\\"\\\\u003Eimport\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003ELogisticRegression\\\\u003C\\\\u002Fspan\\\\u003E\\\\n\\\\n\\\\u003Cspan class=\\\\\"c1\\\\\"\\\\u003E#递归特征消除法，返回特征选择后的数据\\\\u003C\\\\u002Fspan\\\\u003E\\\\n\\\\u003Cspan class=\\\\\"c1\\\\\"\\\\u003E#参数estimator为基模型\\\\u003C\\\\u002Fspan\\\\u003E\\\\n\\\\u003Cspan class=\\\\\"c1\\\\\"\\\\u003E#参数n_features_to_select为选择的特征个数\\\\u003C\\\\u002Fspan\\\\u003E\\\\n\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003ERFE\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E(\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Eestimator\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E=\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003ELogisticRegression\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E(),\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003En_features_to_select\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E=\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"mi\\\\\"\\\\u003E2\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E)\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E.\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Efit_transform\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E(\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Eiris\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E.\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Edata\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E,\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Eiris\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E.\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Etarget\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E)\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003C\\\\u002Fcode\\\\u003E\\\\u003C\\\\u002Fpre\\\\u003E\\\\u003C\\\\u002Fdiv\\\\u003E\\\\u003Cb\\\\u003E3.3 Embedded\\\\u003C\\\\u002Fb\\\\u003E\\\\u003Cbr\\\\u002F\\\\u003E\\\\u003Cb\\\\u003E3.3.1 基于惩罚项的特征选择法\\\\u003C\\\\u002Fb\\\\u003E\\\\u003Cp\\\\u003E\\u3000\\u3000使用带惩罚项的基模型，除了筛选出特征外，同时也进行了降维。使用feature_selection库的SelectFromModel类结合带L1惩罚项的逻辑回归模型，来选择特征的代码如下：\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Cdiv class=\\\\\"highlight\\\\\"\\\\u003E\\\\u003Cpre\\\\u003E\\\\u003Ccode class=\\\\\"language-python\\\\\"\\\\u003E\\\\u003Cspan class=\\\\\"kn\\\\\"\\\\u003Efrom\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"nn\\\\\"\\\\u003Esklearn.feature_selection\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"kn\\\\\"\\\\u003Eimport\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003ESelectFromModel\\\\u003C\\\\u002Fspan\\\\u003E\\\\n\\\\u003Cspan class=\\\\\"kn\\\\\"\\\\u003Efrom\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"nn\\\\\"\\\\u003Esklearn.linear_model\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"kn\\\\\"\\\\u003Eimport\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003ELogisticRegression\\\\u003C\\\\u002Fspan\\\\u003E\\\\n\\\\n\\\\u003Cspan class=\\\\\"c1\\\\\"\\\\u003E#带L1惩罚项的逻辑回归作为基模型的特征选择\\\\u003C\\\\u002Fspan\\\\u003E\\\\n\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003ESelectFromModel\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E(\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003ELogisticRegression\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E(\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Epenalty\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E=\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"s2\\\\\"\\\\u003E&#34;l1&#34;\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E,\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003EC\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E=\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"mf\\\\\"\\\\u003E0.1\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E))\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E.\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Efit_transform\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E(\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Eiris\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E.\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Edata\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E,\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Eiris\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E.\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Etarget\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E)\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003C\\\\u002Fcode\\\\u003E\\\\u003C\\\\u002Fpre\\\\u003E\\\\u003C\\\\u002Fdiv\\\\u003E\\\\u003Cp\\\\u003E\\u3000\\u3000实际上，\\\\u003Ca href=\\\\\"http:\\\\u002F\\\\u002Fwww.zhihu.com\\\\u002Fquestion\\\\u002F28641663\\\\u002Fanswer\\\\u002F41653367\\\\\" class=\\\\\"internal\\\\\"\\\\u003EL1惩罚项降维的原理在于保留多个对目标值具有同等相关性的特征中的一个\\\\u003C\\\\u002Fa\\\\u003E，所以没选到的特征不代表不重要。故，可结合L2惩罚项来优化。具体操作为：若一个特征在L1中的权值为1，选择在L2中权值差别不大且在L1中权值为0的特征构成同类集合，将这一集合中的特征平分L1中的权值，故需要构建一个新的逻辑回归模型：\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Cdiv class=\\\\\"highlight\\\\\"\\\\u003E\\\\u003Cpre\\\\u003E\\\\u003Ccode class=\\\\\"language-python\\\\\"\\\\u003E\\\\u003Cspan class=\\\\\"kn\\\\\"\\\\u003Efrom\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"nn\\\\\"\\\\u003Esklearn.linear_model\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"kn\\\\\"\\\\u003Eimport\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003ELogisticRegression\\\\u003C\\\\u002Fspan\\\\u003E\\\\n\\\\n\\\\u003Cspan class=\\\\\"k\\\\\"\\\\u003Eclass\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"nc\\\\\"\\\\u003ELR\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E(\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003ELogisticRegression\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E):\\\\u003C\\\\u002Fspan\\\\u003E\\\\n    \\\\u003Cspan class=\\\\\"k\\\\\"\\\\u003Edef\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"fm\\\\\"\\\\u003E__init__\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E(\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"bp\\\\\"\\\\u003Eself\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E,\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Ethreshold\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E=\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"mf\\\\\"\\\\u003E0.01\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E,\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Edual\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E=\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"bp\\\\\"\\\\u003EFalse\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E,\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Etol\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E=\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"mf\\\\\"\\\\u003E1e-4\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E,\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003EC\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E=\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"mf\\\\\"\\\\u003E1.0\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E,\\\\u003C\\\\u002Fspan\\\\u003E\\\\n                 \\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Efit_intercept\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E=\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"bp\\\\\"\\\\u003ETrue\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E,\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Eintercept_scaling\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E=\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"mi\\\\\"\\\\u003E1\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E,\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Eclass_weight\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E=\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"bp\\\\\"\\\\u003ENone\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E,\\\\u003C\\\\u002Fspan\\\\u003E\\\\n                 \\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Erandom_state\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E=\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"bp\\\\\"\\\\u003ENone\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E,\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Esolver\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E=\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"s1\\\\\"\\\\u003E&#39;liblinear&#39;\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E,\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Emax_iter\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E=\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"mi\\\\\"\\\\u003E100\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E,\\\\u003C\\\\u002Fspan\\\\u003E\\\\n                 \\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Emulti_class\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E=\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"s1\\\\\"\\\\u003E&#39;ovr&#39;\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E,\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Everbose\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E=\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"mi\\\\\"\\\\u003E0\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E,\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Ewarm_start\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E=\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"bp\\\\\"\\\\u003EFalse\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E,\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003En_jobs\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E=\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"mi\\\\\"\\\\u003E1\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E):\\\\u003C\\\\u002Fspan\\\\u003E\\\\n\\\\n        \\\\u003Cspan class=\\\\\"c1\\\\\"\\\\u003E#权值相近的阈值\\\\u003C\\\\u002Fspan\\\\u003E\\\\n        \\\\u003Cspan class=\\\\\"bp\\\\\"\\\\u003Eself\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E.\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Ethreshold\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E=\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Ethreshold\\\\u003C\\\\u002Fspan\\\\u003E\\\\n        \\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003ELogisticRegression\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E.\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"fm\\\\\"\\\\u003E__init__\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E(\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"bp\\\\\"\\\\u003Eself\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E,\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Epenalty\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E=\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"s1\\\\\"\\\\u003E&#39;l1&#39;\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E,\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Edual\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E=\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Edual\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E,\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Etol\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E=\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Etol\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E,\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003EC\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E=\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003EC\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E,\\\\u003C\\\\u002Fspan\\\\u003E\\\\n                 \\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Efit_intercept\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E=\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Efit_intercept\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E,\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Eintercept_scaling\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E=\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Eintercept_scaling\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E,\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Eclass_weight\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E=\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Eclass_weight\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E,\\\\u003C\\\\u002Fspan\\\\u003E\\\\n                 \\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Erandom_state\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E=\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Erandom_state\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E,\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Esolver\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E=\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Esolver\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E,\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Emax_iter\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E=\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Emax_iter\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E,\\\\u003C\\\\u002Fspan\\\\u003E\\\\n                 \\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Emulti_class\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E=\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Emulti_class\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E,\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Everbose\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E=\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Everbose\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E,\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Ewarm_start\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E=\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Ewarm_start\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E,\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003En_jobs\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E=\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003En_jobs\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E)\\\\u003C\\\\u002Fspan\\\\u003E\\\\n        \\\\u003Cspan class=\\\\\"c1\\\\\"\\\\u003E#使用同样的参数创建L2逻辑回归\\\\u003C\\\\u002Fspan\\\\u003E\\\\n        \\\\u003Cspan class=\\\\\"bp\\\\\"\\\\u003Eself\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E.\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003El2\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E=\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003ELogisticRegression\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E(\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Epenalty\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E=\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"s1\\\\\"\\\\u003E&#39;l2&#39;\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E,\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Edual\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E=\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Edual\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E,\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Etol\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E=\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Etol\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E,\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003EC\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E=\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003EC\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E,\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Efit_intercept\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E=\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Efit_intercept\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E,\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Eintercept_scaling\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E=\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Eintercept_scaling\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E,\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Eclass_weight\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E=\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Eclass_weight\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E,\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Erandom_state\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E=\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Erandom_state\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E,\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Esolver\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E=\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Esolver\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E,\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Emax_iter\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E=\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Emax_iter\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E,\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Emulti_class\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E=\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Emulti_class\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E,\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Everbose\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E=\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Everbose\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E,\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Ewarm_start\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E=\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Ewarm_start\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E,\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003En_jobs\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E=\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003En_jobs\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E)\\\\u003C\\\\u002Fspan\\\\u003E\\\\n\\\\n    \\\\u003Cspan class=\\\\\"k\\\\\"\\\\u003Edef\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"nf\\\\\"\\\\u003Efit\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E(\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"bp\\\\\"\\\\u003Eself\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E,\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003EX\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E,\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Ey\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E,\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Esample_weight\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E=\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"bp\\\\\"\\\\u003ENone\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E):\\\\u003C\\\\u002Fspan\\\\u003E\\\\n        \\\\u003Cspan class=\\\\\"c1\\\\\"\\\\u003E#训练L1逻辑回归\\\\u003C\\\\u002Fspan\\\\u003E\\\\n        \\\\u003Cspan class=\\\\\"nb\\\\\"\\\\u003Esuper\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E(\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003ELR\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E,\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"bp\\\\\"\\\\u003Eself\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E)\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E.\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Efit\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E(\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003EX\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E,\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Ey\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E,\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Esample_weight\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E=\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Esample_weight\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E)\\\\u003C\\\\u002Fspan\\\\u003E\\\\n        \\\\u003Cspan class=\\\\\"bp\\\\\"\\\\u003Eself\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E.\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Ecoef_old_\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E=\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"bp\\\\\"\\\\u003Eself\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E.\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Ecoef_\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E.\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Ecopy\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E()\\\\u003C\\\\u002Fspan\\\\u003E\\\\n        \\\\u003Cspan class=\\\\\"c1\\\\\"\\\\u003E#训练L2逻辑回归\\\\u003C\\\\u002Fspan\\\\u003E\\\\n        \\\\u003Cspan class=\\\\\"bp\\\\\"\\\\u003Eself\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E.\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003El2\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E.\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Efit\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E(\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003EX\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E,\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Ey\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E,\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Esample_weight\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E=\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Esample_weight\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E)\\\\u003C\\\\u002Fspan\\\\u003E\\\\n\\\\n        \\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003EcntOfRow\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E,\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003EcntOfCol\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E=\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"bp\\\\\"\\\\u003Eself\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E.\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Ecoef_\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E.\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Eshape\\\\u003C\\\\u002Fspan\\\\u003E\\\\n        \\\\u003Cspan class=\\\\\"c1\\\\\"\\\\u003E#权值系数矩阵的行数对应目标值的种类数目\\\\u003C\\\\u002Fspan\\\\u003E\\\\n        \\\\u003Cspan class=\\\\\"k\\\\\"\\\\u003Efor\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Ei\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"ow\\\\\"\\\\u003Ein\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"nb\\\\\"\\\\u003Erange\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E(\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003EcntOfRow\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E):\\\\u003C\\\\u002Fspan\\\\u003E\\\\n            \\\\u003Cspan class=\\\\\"k\\\\\"\\\\u003Efor\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Ej\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"ow\\\\\"\\\\u003Ein\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"nb\\\\\"\\\\u003Erange\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E(\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003EcntOfCol\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E):\\\\u003C\\\\u002Fspan\\\\u003E\\\\n                \\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Ecoef\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E=\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"bp\\\\\"\\\\u003Eself\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E.\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Ecoef_\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E[\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Ei\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E][\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Ej\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E]\\\\u003C\\\\u002Fspan\\\\u003E\\\\n                \\\\u003Cspan class=\\\\\"c1\\\\\"\\\\u003E#L1逻辑回归的权值系数不为0\\\\u003C\\\\u002Fspan\\\\u003E\\\\n                \\\\u003Cspan class=\\\\\"k\\\\\"\\\\u003Eif\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Ecoef\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E!=\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"mi\\\\\"\\\\u003E0\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E:\\\\u003C\\\\u002Fspan\\\\u003E\\\\n                    \\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Eidx\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E=\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E[\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Ej\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E]\\\\u003C\\\\u002Fspan\\\\u003E\\\\n                    \\\\u003Cspan class=\\\\\"c1\\\\\"\\\\u003E#对应在L2逻辑回归中的权值系数\\\\u003C\\\\u002Fspan\\\\u003E\\\\n                    \\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Ecoef1\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E=\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"bp\\\\\"\\\\u003Eself\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E.\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003El2\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E.\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Ecoef_\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E[\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Ei\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E][\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Ej\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E]\\\\u003C\\\\u002Fspan\\\\u003E\\\\n                    \\\\u003Cspan class=\\\\\"k\\\\\"\\\\u003Efor\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Ek\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"ow\\\\\"\\\\u003Ein\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"nb\\\\\"\\\\u003Erange\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E(\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003EcntOfCol\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E):\\\\u003C\\\\u002Fspan\\\\u003E\\\\n                        \\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Ecoef2\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E=\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"bp\\\\\"\\\\u003Eself\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E.\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003El2\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E.\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Ecoef_\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E[\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Ei\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E][\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Ek\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E]\\\\u003C\\\\u002Fspan\\\\u003E\\\\n                        \\\\u003Cspan class=\\\\\"c1\\\\\"\\\\u003E#在L2逻辑回归中，权值系数之差小于设定的阈值，且在L1中对应的权值为0\\\\u003C\\\\u002Fspan\\\\u003E\\\\n                        \\\\u003Cspan class=\\\\\"k\\\\\"\\\\u003Eif\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"nb\\\\\"\\\\u003Eabs\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E(\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Ecoef1\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E-\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Ecoef2\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E)\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E&lt;\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"bp\\\\\"\\\\u003Eself\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E.\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Ethreshold\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"ow\\\\\"\\\\u003Eand\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Ej\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E!=\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Ek\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"ow\\\\\"\\\\u003Eand\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"bp\\\\\"\\\\u003Eself\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E.\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Ecoef_\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E[\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Ei\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E][\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Ek\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E]\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E==\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"mi\\\\\"\\\\u003E0\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E:\\\\u003C\\\\u002Fspan\\\\u003E\\\\n                            \\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Eidx\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E.\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Eappend\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E(\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Ek\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E)\\\\u003C\\\\u002Fspan\\\\u003E\\\\n                    \\\\u003Cspan class=\\\\\"c1\\\\\"\\\\u003E#计算这一类特征的权值系数均值\\\\u003C\\\\u002Fspan\\\\u003E\\\\n                    \\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Emean\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E=\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Ecoef\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E\\\\u002F\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"nb\\\\\"\\\\u003Elen\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E(\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Eidx\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E)\\\\u003C\\\\u002Fspan\\\\u003E\\\\n                    \\\\u003Cspan class=\\\\\"bp\\\\\"\\\\u003Eself\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E.\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Ecoef_\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E[\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Ei\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E][\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Eidx\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E]\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E=\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Emean\\\\u003C\\\\u002Fspan\\\\u003E\\\\n        \\\\u003Cspan class=\\\\\"k\\\\\"\\\\u003Ereturn\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"bp\\\\\"\\\\u003Eself\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003C\\\\u002Fcode\\\\u003E\\\\u003C\\\\u002Fpre\\\\u003E\\\\u003C\\\\u002Fdiv\\\\u003E\\\\u003Cp\\\\u003E\\u3000\\u3000使用feature_selection库的SelectFromModel类结合带L1以及L2惩罚项的逻辑回归模型，来选择特征的代码如下：\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Cdiv class=\\\\\"highlight\\\\\"\\\\u003E\\\\u003Cpre\\\\u003E\\\\u003Ccode class=\\\\\"language-python\\\\\"\\\\u003E\\\\u003Cspan class=\\\\\"kn\\\\\"\\\\u003Efrom\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"nn\\\\\"\\\\u003Esklearn.feature_selection\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"kn\\\\\"\\\\u003Eimport\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003ESelectFromModel\\\\u003C\\\\u002Fspan\\\\u003E\\\\n \\\\n\\\\u003Cspan class=\\\\\"c1\\\\\"\\\\u003E#带L1和L2惩罚项的逻辑回归作为基模型的特征选择\\\\u003C\\\\u002Fspan\\\\u003E\\\\n\\\\u003Cspan class=\\\\\"c1\\\\\"\\\\u003E#参数threshold为权值系数之差的阈值\\\\u003C\\\\u002Fspan\\\\u003E\\\\n\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003ESelectFromModel\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E(\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003ELR\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E(\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Ethreshold\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E=\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"mf\\\\\"\\\\u003E0.5\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E,\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003EC\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E=\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"mf\\\\\"\\\\u003E0.1\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E))\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E.\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Efit_transform\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E(\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Eiris\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E.\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Edata\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E,\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Eiris\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E.\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Etarget\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E)\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003C\\\\u002Fcode\\\\u003E\\\\u003C\\\\u002Fpre\\\\u003E\\\\u003C\\\\u002Fdiv\\\\u003E\\\\u003Cb\\\\u003E3.3.2 基于树模型的特征选择法\\\\u003C\\\\u002Fb\\\\u003E\\\\u003Cp\\\\u003E\\u3000\\u3000树模型中GBDT也可用来作为基模型进行特征选择，使用feature_selection库的SelectFromModel类结合GBDT模型，来选择特征的代码如下：\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Cdiv class=\\\\\"highlight\\\\\"\\\\u003E\\\\u003Cpre\\\\u003E\\\\u003Ccode class=\\\\\"language-python\\\\\"\\\\u003E\\\\u003Cspan class=\\\\\"kn\\\\\"\\\\u003Efrom\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"nn\\\\\"\\\\u003Esklearn.feature_selection\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"kn\\\\\"\\\\u003Eimport\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003ESelectFromModel\\\\u003C\\\\u002Fspan\\\\u003E\\\\n\\\\u003Cspan class=\\\\\"kn\\\\\"\\\\u003Efrom\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"nn\\\\\"\\\\u003Esklearn.ensemble\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"kn\\\\\"\\\\u003Eimport\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003EGradientBoostingClassifier\\\\u003C\\\\u002Fspan\\\\u003E\\\\n\\\\n\\\\u003Cspan class=\\\\\"c1\\\\\"\\\\u003E#GBDT作为基模型的特征选择\\\\u003C\\\\u002Fspan\\\\u003E\\\\n\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003ESelectFromModel\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E(\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003EGradientBoostingClassifier\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E())\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E.\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Efit_transform\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E(\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Eiris\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E.\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Edata\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E,\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Eiris\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E.\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Etarget\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E)\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003C\\\\u002Fcode\\\\u003E\\\\u003C\\\\u002Fpre\\\\u003E\\\\u003C\\\\u002Fdiv\\\\u003E\\\\u003Cb\\\\u003E\\\\u003Cbr\\\\u002F\\\\u003E4 降维\\\\u003C\\\\u002Fb\\\\u003E\\\\u003Cp\\\\u003E\\u3000\\u3000当特征选择完成后，可以直接训练模型了，但是可能由于特征矩阵过大，导致计算量大，训练时间长的问题，因此降低特征矩阵维度也是必不可少的。常见的降维方法除了以上提到的基于L1惩罚项的模型以外，另外还有主成分分析法（PCA）和线性判别分析（LDA），线性判别分析本身也是一个分类模型。PCA和LDA有很多的相似点，其本质是要将原始的样本映射到维度更低的样本空间中，但是PCA和LDA的映射目标不一样：\\\\u003Ca href=\\\\\"https:\\\\u002F\\\\u002Flink.zhihu.com\\\\u002F?target=http%3A\\\\u002F\\\\u002Fwww.cnblogs.com\\\\u002FLeftNotEasy\\\\u002Farchive\\\\u002F2011\\\\u002F01\\\\u002F08\\\\u002Flda-and-pca-machine-learning.html\\\\\" class=\\\\\" wrap external\\\\\" target=\\\\\"_blank\\\\\" rel=\\\\\"nofollow noreferrer\\\\\"\\\\u003EPCA是为了让映射后的样本具有最大的发散性；而LDA是为了让映射后的样本有最好的分类性能\\\\u003C\\\\u002Fa\\\\u003E。所以说PCA是一种无监督的降维方法，而LDA是一种有监督的降维方法。\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Cb\\\\u003E4.1 主成分分析法（PCA）\\\\u003C\\\\u002Fb\\\\u003E\\\\u003Cp\\\\u003E\\u3000\\u3000使用decomposition库的PCA类选择特征的代码如下：\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Cdiv class=\\\\\"highlight\\\\\"\\\\u003E\\\\u003Cpre\\\\u003E\\\\u003Ccode class=\\\\\"language-python\\\\\"\\\\u003E\\\\u003Cspan class=\\\\\"kn\\\\\"\\\\u003Efrom\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"nn\\\\\"\\\\u003Esklearn.decomposition\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"kn\\\\\"\\\\u003Eimport\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003EPCA\\\\u003C\\\\u002Fspan\\\\u003E\\\\n\\\\n\\\\u003Cspan class=\\\\\"c1\\\\\"\\\\u003E#主成分分析法，返回降维后的数据\\\\u003C\\\\u002Fspan\\\\u003E\\\\n\\\\u003Cspan class=\\\\\"c1\\\\\"\\\\u003E#参数n_components为主成分数目\\\\u003C\\\\u002Fspan\\\\u003E\\\\n\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003EPCA\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E(\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003En_components\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E=\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"mi\\\\\"\\\\u003E2\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E)\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E.\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Efit_transform\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E(\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Eiris\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E.\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Edata\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E)\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003C\\\\u002Fcode\\\\u003E\\\\u003C\\\\u002Fpre\\\\u003E\\\\u003C\\\\u002Fdiv\\\\u003E\\\\u003Cb\\\\u003E4.2 线性判别分析法（LDA）\\\\u003C\\\\u002Fb\\\\u003E\\\\u003Cp\\\\u003E\\u3000\\u3000使用lda库的LDA类选择特征的代码如下：\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Cdiv class=\\\\\"highlight\\\\\"\\\\u003E\\\\u003Cpre\\\\u003E\\\\u003Ccode class=\\\\\"language-python\\\\\"\\\\u003E\\\\u003Cspan class=\\\\\"kn\\\\\"\\\\u003Efrom\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"nn\\\\\"\\\\u003Esklearn.lda\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"kn\\\\\"\\\\u003Eimport\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003ELDA\\\\u003C\\\\u002Fspan\\\\u003E\\\\n\\\\n\\\\u003Cspan class=\\\\\"c1\\\\\"\\\\u003E#线性判别分析法，返回降维后的数据\\\\u003C\\\\u002Fspan\\\\u003E\\\\n\\\\u003Cspan class=\\\\\"c1\\\\\"\\\\u003E#参数n_components为降维后的维数\\\\u003C\\\\u002Fspan\\\\u003E\\\\n\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003ELDA\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E(\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003En_components\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E=\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"mi\\\\\"\\\\u003E2\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E)\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E.\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Efit_transform\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E(\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Eiris\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E.\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Edata\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E,\\\\u003C\\\\u002Fspan\\\\u003E \\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Eiris\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"o\\\\\"\\\\u003E.\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"n\\\\\"\\\\u003Etarget\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"p\\\\\"\\\\u003E)\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003C\\\\u002Fcode\\\\u003E\\\\u003C\\\\u002Fpre\\\\u003E\\\\u003C\\\\u002Fdiv\\\\u003E\\\\u003Cb\\\\u003E\\\\u003Cbr\\\\u002F\\\\u003E\\\\u003Cbr\\\\u002F\\\\u003E5 总结\\\\u003C\\\\u002Fb\\\\u003E\\\\u003Cp\\\\u003E\\u3000\\u3000再让我们回归一下本文开始的特征工程的思维导图，我们可以使用sklearn完成几乎所有特征处理的工作，而且不管是数据预处理，还是特征选择，抑或降维，它们都是通过某个类的方法fit_transform完成的，fit_transform要不只带一个参数：特征矩阵，要不带两个参数：特征矩阵加目标向量。这些难道都是巧合吗？还是故意设计成这样？方法fit_transform中有fit这一单词，它和训练模型的fit方法有关联吗？接下来，我将在\\\\u003Ca href=\\\\\"https:\\\\u002F\\\\u002Flink.zhihu.com\\\\u002F?target=http%3A\\\\u002F\\\\u002Fwww.cnblogs.com\\\\u002Fjasonfreak\\\\u002Fp\\\\u002F5448462.html\\\\\" class=\\\\\" wrap external\\\\\" target=\\\\\"_blank\\\\\" rel=\\\\\"nofollow noreferrer\\\\\"\\\\u003E《使用sklearn优雅地进行数据挖掘》\\\\u003C\\\\u002Fa\\\\u003E中阐述其中的奥妙！\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Cb\\\\u003E\\\\u003Cbr\\\\u002F\\\\u003E\\\\u003Cbr\\\\u002F\\\\u003E6 参考资料\\\\u003C\\\\u002Fb\\\\u003E\\\\u003Col\\\\u003E\\\\u003Cli\\\\u003E\\\\u003Ca href=\\\\\"https:\\\\u002F\\\\u002Flink.zhihu.com\\\\u002F?target=http%3A\\\\u002F\\\\u002Fwww.ats.ucla.edu\\\\u002Fstat\\\\u002Fmult_pkg\\\\u002Ffaq\\\\u002Fgeneral\\\\u002Fdummy.htm\\\\\" class=\\\\\" wrap external\\\\\" target=\\\\\"_blank\\\\\" rel=\\\\\"nofollow noreferrer\\\\\"\\\\u003EFAQ: What is dummy coding?\\\\u003C\\\\u002Fa\\\\u003E\\\\u003C\\\\u002Fli\\\\u003E\\\\u003Cli\\\\u003E\\\\u003Ca href=\\\\\"https:\\\\u002F\\\\u002Flink.zhihu.com\\\\u002F?target=http%3A\\\\u002F\\\\u002Fscikit-learn.org\\\\u002Fstable\\\\u002Fmodules\\\\u002Fgenerated\\\\u002Fsklearn.datasets.load_iris.html%23sklearn.datasets.load_iris\\\\\" class=\\\\\" wrap external\\\\\" target=\\\\\"_blank\\\\\" rel=\\\\\"nofollow noreferrer\\\\\"\\\\u003EIRIS（鸢尾花）数据集\\\\u003C\\\\u002Fa\\\\u003E\\\\u003C\\\\u002Fli\\\\u003E\\\\u003Cli\\\\u003E\\\\u003Ca href=\\\\\"https:\\\\u002F\\\\u002Flink.zhihu.com\\\\u002F?target=http%3A\\\\u002F\\\\u002Fwiki.mbalib.com\\\\u002Fwiki\\\\u002F%25E5%258D%25A1%25E6%2596%25B9%25E6%25A3%2580%25E9%25AA%258C\\\\\" class=\\\\\" wrap external\\\\\" target=\\\\\"_blank\\\\\" rel=\\\\\"nofollow noreferrer\\\\\"\\\\u003E卡方检验\\\\u003C\\\\u002Fa\\\\u003E\\\\u003C\\\\u002Fli\\\\u003E\\\\u003Cli\\\\u003E\\\\u003Ca href=\\\\\"https:\\\\u002F\\\\u002Flink.zhihu.com\\\\u002F?target=http%3A\\\\u002F\\\\u002Fdataunion.org\\\\u002F14072.html\\\\\" class=\\\\\" wrap external\\\\\" target=\\\\\"_blank\\\\\" rel=\\\\\"nofollow noreferrer\\\\\"\\\\u003E干货：结合Scikit-learn介绍几种常用的特征选择方法\\\\u003C\\\\u002Fa\\\\u003E\\\\u003C\\\\u002Fli\\\\u003E\\\\u003Cli\\\\u003E\\\\u003Ca href=\\\\\"http:\\\\u002F\\\\u002Fwww.zhihu.com\\\\u002Fquestion\\\\u002F28641663\\\\u002Fanswer\\\\u002F41653367\\\\\" class=\\\\\"internal\\\\\"\\\\u003E机器学习中，有哪些特征选择的工程方法？\\\\u003C\\\\u002Fa\\\\u003E\\\\u003C\\\\u002Fli\\\\u003E\\\\u003Cli\\\\u003E\\\\u003Ca href=\\\\\"https:\\\\u002F\\\\u002Flink.zhihu.com\\\\u002F?target=http%3A\\\\u002F\\\\u002Fwww.cnblogs.com\\\\u002FLeftNotEasy\\\\u002Farchive\\\\u002F2011\\\\u002F01\\\\u002F08\\\\u002Flda-and-pca-machine-learning.html\\\\\" class=\\\\\" wrap external\\\\\" target=\\\\\"_blank\\\\\" rel=\\\\\"nofollow noreferrer\\\\\"\\\\u003E机器学习中的数学(4)-线性判别分析（LDA）, 主成分分析(PCA)\\\\u003C\\\\u002Fa\\\\u003E\\\\u003C\\\\u002Fli\\\\u003E\\\\u003C\\\\u002Fol\\\\u003E\",\"editableContent\":\"\",\"excerpt\":\"转自我的博文： 使用sklearn做单机特征工程 目录1 特征工程是什么？ 2 数据预处理 2.1 无量纲化 2.1.1 标准化 2.1.2 区间缩放法 2.1.3 标准化与归一化的区别 2.2 对定量特征二值化 2.3 对定性特征哑编码 2.4 缺失值计算 2.5 数据变换 3 特征选择 3.1 Filter 3.1.1 方差选择法 3.1.2 相关系数法 3.1.3 卡方检验 3.1.4 互信息法 3.2 Wrapper 3.2.1 递归特征消除法 3.3 Embedded 3.3.1 基于惩罚项的特征选择法 3.3.2 基于树模型的特…\",\"collapsedBy\":\"nobody\",\"collapseReason\":\"\",\"annotationAction\":null,\"markInfos\":[],\"relevantInfo\":{\"isRelevant\":false,\"relevantType\":\"\",\"relevantText\":\"\"},\"suggestEdit\":{\"reason\":\"\",\"status\":false,\"tip\":\"\",\"title\":\"\",\"unnormalDetails\":{\"status\":\"\",\"description\":\"\",\"reason\":\"\",\"reasonId\":0,\"note\":\"\"},\"url\":\"\"},\"isLabeled\":true,\"rewardInfo\":{\"canOpenReward\":false,\"isRewardable\":false,\"rewardMemberCount\":0,\"rewardTotalMoney\":0,\"tagline\":\"\"},\"relationship\":{\"isAuthor\":false,\"isAuthorized\":false,\"isNothelp\":false,\"isThanked\":false,\"isRecognized\":false,\"voting\":0,\"upvotedFollowees\":[]},\"adAnswer\":null},\"434884138\":{\"id\":434884138,\"type\":\"answer\",\"answerType\":\"normal\",\"question\":{\"type\":\"question\",\"id\":29316149,\"title\":\"特征工程到底是什么？\",\"questionType\":\"normal\",\"created\":1428220876,\"updatedTime\":1428220876,\"url\":\"https:\\\\u002F\\\\u002Fwww.zhihu.com\\\\u002Fapi\\\\u002Fv4\\\\u002Fquestions\\\\u002F29316149\",\"relationship\":{}},\"author\":{\"id\":\"1d2ffd0f597bba0da689f1fd46464a04\",\"urlToken\":\"jovialcai\",\"name\":\"JovialCai\",\"avatarUrl\":\"https:\\\\u002F\\\\u002Fpic2.zhimg.com\\\\u002Fv2-ea305ea7f9d50f49f26db38eb8b500d1_l.jpg\",\"avatarUrlTemplate\":\"https:\\\\u002F\\\\u002Fpic1.zhimg.com\\\\u002Fv2-ea305ea7f9d50f49f26db38eb8b500d1.jpg\",\"isOrg\":false,\"type\":\"people\",\"url\":\"https:\\\\u002F\\\\u002Fwww.zhihu.com\\\\u002Fapi\\\\u002Fv4\\\\u002Fpeople\\\\u002F1d2ffd0f597bba0da689f1fd46464a04\",\"userType\":\"people\",\"headline\":\"Data Scientist\",\"badge\":[],\"badgeV2\":{\"title\":\"\",\"mergedBadges\":[],\"detailBadges\":[]},\"gender\":1,\"isAdvertiser\":false,\"followerCount\":635,\"isFollowed\":false,\"isPrivacy\":false},\"url\":\"https:\\\\u002F\\\\u002Fwww.zhihu.com\\\\u002Fapi\\\\u002Fv4\\\\u002Fanswers\\\\u002F434884138\",\"isCollapsed\":false,\"createdTime\":1530792277,\"updatedTime\":1536136473,\"extras\":\"\",\"isCopyable\":true,\"isNormal\":true,\"voteupCount\":65,\"commentCount\":16,\"isSticky\":false,\"adminClosedComment\":false,\"commentPermission\":\"all\",\"canComment\":{\"reason\":\"\",\"status\":true},\"reshipmentSettings\":\"allowed\",\"content\":\"\\\\u003Cp\\\\u003E原文已发表于 \\\\u003Ca class=\\\\\"member_mention\\\\\" href=\\\\\"https:\\\\u002F\\\\u002Fwww.zhihu.com\\\\u002Fpeople\\\\u002F91e764c1f5a0d30696ad863dbe23cd9a\\\\\" data-hash=\\\\\"91e764c1f5a0d30696ad863dbe23cd9a\\\\\" data-hovercard=\\\\\"p$b$91e764c1f5a0d30696ad863dbe23cd9a\\\\\"\\\\u003E@京东金融\\\\u003C\\\\u002Fa\\\\u003E 。\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Cp\\\\u003E前面的很多高赞答案已经把特征工程的方法论总结的很到位了，我这里结合自己的工作经验补充一些具体的例子。\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Cp class=\\\\\"ztext-empty-paragraph\\\\\"\\\\u003E\\\\u003Cbr\\\\u002F\\\\u003E\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Cp\\\\u003E#################################\\\\u003Cbr\\\\u002F\\\\u003E找特征这件事，Andrew Ng在深度学习网课中提到过，原课件见第3课结构化机器学习项目中的2.9和2.10两节，笔记整理如下：\\\\u003Cbr\\\\u002F\\\\u003E\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Cfigure data-size=\\\\\"normal\\\\\"\\\\u003E\\\\u003Cnoscript\\\\u003E\\\\u003Cimg src=\\\\\"https:\\\\u002F\\\\u002Fpic3.zhimg.com\\\\u002F50\\\\u002Fv2-c23def3546dbce6c831a39881408e6bd_hd.jpg\\\\\" data-caption=\\\\\"\\\\\" data-size=\\\\\"normal\\\\\" data-rawwidth=\\\\\"758\\\\\" data-rawheight=\\\\\"262\\\\\" class=\\\\\"origin_image zh-lightbox-thumb\\\\\" width=\\\\\"758\\\\\" data-original=\\\\\"https:\\\\u002F\\\\u002Fpic2.zhimg.com\\\\u002Fv2-c23def3546dbce6c831a39881408e6bd_r.jpg\\\\\"\\\\u002F\\\\u003E\\\\u003C\\\\u002Fnoscript\\\\u003E\\\\u003Cimg src=\\\\\"data:image\\\\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\\\\u002F\\\\u002Fwww.w3.org\\\\u002F2000\\\\u002Fsvg&#39; width=&#39;758&#39; height=&#39;262&#39;&gt;&lt;\\\\u002Fsvg&gt;\\\\\" data-caption=\\\\\"\\\\\" data-size=\\\\\"normal\\\\\" data-rawwidth=\\\\\"758\\\\\" data-rawheight=\\\\\"262\\\\\" class=\\\\\"origin_image zh-lightbox-thumb lazy\\\\\" width=\\\\\"758\\\\\" data-original=\\\\\"https:\\\\u002F\\\\u002Fpic2.zhimg.com\\\\u002Fv2-c23def3546dbce6c831a39881408e6bd_r.jpg\\\\\" data-actualsrc=\\\\\"https:\\\\u002F\\\\u002Fpic3.zhimg.com\\\\u002F50\\\\u002Fv2-c23def3546dbce6c831a39881408e6bd_hd.jpg\\\\\"\\\\u002F\\\\u003E\\\\u003C\\\\u002Ffigure\\\\u003E\\\\u003Cfigure data-size=\\\\\"normal\\\\\"\\\\u003E\\\\u003Cnoscript\\\\u003E\\\\u003Cimg src=\\\\\"https:\\\\u002F\\\\u002Fpic3.zhimg.com\\\\u002F50\\\\u002Fv2-299e4f00b5ee90b3b08114e3713754b4_hd.jpg\\\\\" data-caption=\\\\\"\\\\\" data-size=\\\\\"normal\\\\\" data-rawwidth=\\\\\"748\\\\\" data-rawheight=\\\\\"299\\\\\" class=\\\\\"origin_image zh-lightbox-thumb\\\\\" width=\\\\\"748\\\\\" data-original=\\\\\"https:\\\\u002F\\\\u002Fpic1.zhimg.com\\\\u002Fv2-299e4f00b5ee90b3b08114e3713754b4_r.jpg\\\\\"\\\\u002F\\\\u003E\\\\u003C\\\\u002Fnoscript\\\\u003E\\\\u003Cimg src=\\\\\"data:image\\\\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\\\\u002F\\\\u002Fwww.w3.org\\\\u002F2000\\\\u002Fsvg&#39; width=&#39;748&#39; height=&#39;299&#39;&gt;&lt;\\\\u002Fsvg&gt;\\\\\" data-caption=\\\\\"\\\\\" data-size=\\\\\"normal\\\\\" data-rawwidth=\\\\\"748\\\\\" data-rawheight=\\\\\"299\\\\\" class=\\\\\"origin_image zh-lightbox-thumb lazy\\\\\" width=\\\\\"748\\\\\" data-original=\\\\\"https:\\\\u002F\\\\u002Fpic1.zhimg.com\\\\u002Fv2-299e4f00b5ee90b3b08114e3713754b4_r.jpg\\\\\" data-actualsrc=\\\\\"https:\\\\u002F\\\\u002Fpic3.zhimg.com\\\\u002F50\\\\u002Fv2-299e4f00b5ee90b3b08114e3713754b4_hd.jpg\\\\\"\\\\u002F\\\\u003E\\\\u003C\\\\u002Ffigure\\\\u003E\\\\u003Cp\\\\u003EAndrew以Speech Recognition的场景为例，比较了pipeline和end-to-end两种建模方式中特征工程的差异。\\\\u003Cbr\\\\u002F\\\\u003E其中pipeline的搭建依赖于人工设计的特征，需要依赖于人类可以理解的音节，将一段音频转化为文字；而end-to-end模型基于大量的音频素材，自动找出语音和文字间的关系，不依赖于音节而自动翻译成文字。\\\\u003Cbr\\\\u002F\\\\u003E\\\\u003Cb\\\\u003E总而言之，除去语音和图像等特定场景，对于大部分生活中的机器学习项目，由于没有足够的训练数据支撑，我们还无法完全信任算法自动生成的特征，因而基于人工经验的特征工程依然是目前的主流。\\\\u003C\\\\u002Fb\\\\u003E\\\\u003Cbr\\\\u002F\\\\u003E人工经验这件事比较虚，加之许多业界的项目由于隐私性的考虑，很少会透露底层的入模特征和计算逻辑，使得目前网络上关于特征工程细节的文章少之又少。答主在这里结合自己这几年在金融领域的建模经验，介绍一些常见的数据源类型和特征计算方法，希望可以帮助刚入行或者想入行的从业者们开开脑洞。\\\\u003Cbr\\\\u002F\\\\u003E\\\\u003Cb\\\\u003E（1）\\\\u003C\\\\u002Fb\\\\u003E \\\\u003Cb\\\\u003E支付流水\\\\u003C\\\\u002Fb\\\\u003E：通常包括\\\\u003Cb\\\\u003E支付账户、时间、金额、地点、目的、状态\\\\u003C\\\\u002Fb\\\\u003E等字段，可以反映出客户的经济实力和消费习惯。其中特别的，账户间的复杂交易关系和异常金额时间地点的支付行为，都可以在反欺诈场景中应用，视为团伙作案或者反洗钱的重要指标。\\\\u003Cbr\\\\u002F\\\\u003E\\\\u003Cb\\\\u003E（2）\\\\u003C\\\\u002Fb\\\\u003E \\\\u003Cb\\\\u003E财富管理\\\\u003C\\\\u002Fb\\\\u003E：基金理财类产品的申购历史记录，体现出客户的资金储备和购买偏好。对于风险偏好较低的客户，我们可以推荐小金库这类收益稳定、波动较小的债券类产品；对于追求高收益的客户，我们可以推荐在京东金融app上代销的各类基金，以及智能投顾产品。\\\\u003Cbr\\\\u002F\\\\u003E\\\\u003Cb\\\\u003E（3）\\\\u003C\\\\u002Fb\\\\u003E \\\\u003Cb\\\\u003E贷款信息\\\\u003C\\\\u002Fb\\\\u003E：伴随着近几年国内现金贷以及场景贷市场的迅速发展，国家也在大力推动各家资方信贷数据的治理与共享。基于一个客户在各个平台上的贷款申请、提现、还款信息，可以刻画出这个客户的还款意愿和征信表现，从而为其下一次的信贷申请决策提供建议。常见的，多个平台申请和在贷以及当前有贷款发生90天以上逾期的用户，都会被其他平台列入自动拒绝的名单。\\\\u003Cbr\\\\u002F\\\\u003E\\\\u003Cb\\\\u003E（4）\\\\u003C\\\\u002Fb\\\\u003E \\\\u003Cb\\\\u003EApp登录\\\\u003C\\\\u002Fb\\\\u003E：从SDK埋点获取的各类app登录数据中，我们可以分析出用户在每个app上的停留时间，从而侧面了解这个用户的兴趣爱好，甚至预测用户的年龄和性别。例如京东、阿里等电商app登录较频繁的用户，通常以女性居多，并且消费能力较强；而抖音、快手等小视频app停留时间较长的，一般为年轻人群体。\\\\u003Cbr\\\\u002F\\\\u003E\\\\u003Cb\\\\u003E（5）\\\\u003C\\\\u002Fb\\\\u003E \\\\u003Cb\\\\u003E电商流水\\\\u003C\\\\u002Fb\\\\u003E：从电商公司丰富的订单流水数据中，可以挖掘出较为完整的客户画像。客户Alice近一年内购买频繁，但是平均单笔订单金额较低，通常集中在生活用品以及水果生鲜，可以推断出Alice应该是一位家庭妇女；而客户Ben消费总金额较高，购买过车饰类产品，收货地址集中在办公场所，则大概率Ben是有车一族的白领青年。\\\\u003Cbr\\\\u002F\\\\u003E\\\\u003Cb\\\\u003E（6）\\\\u003C\\\\u002Fb\\\\u003E \\\\u003Cb\\\\u003E收货地址\\\\u003C\\\\u002Fb\\\\u003E：在信贷风控场景中，通常近一年内地址数量较少、地址稳定性高的用户，贷款逾期风险更低；而对于地址变动频繁或者涉黑的用户，建议贷前申请直接拒绝，或者把这些收货地址运用到贷后催收之中。\\\\u003Cbr\\\\u002F\\\\u003E\\\\u003Cb\\\\u003E（7）\\\\u003C\\\\u002Fb\\\\u003E \\\\u003Cb\\\\u003E运营商信息\\\\u003C\\\\u002Fb\\\\u003E：数据市场上比较常见的第三方数据源，可以用作各个场景下的身份证、姓名、手机号的三要素核验，以及利用在网时长和在网状态判断一个用户是否有欺诈风险。\\\\u003Cbr\\\\u002F\\\\u003E除去上面整理的简单底层特征，在实际工作中数据分析师和算法工程师们还需要针对不同的业务场景，利用规则和模型构造一些复杂特征。\\\\u003Cbr\\\\u002F\\\\u003E\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Cul\\\\u003E\\\\u003Cli\\\\u003E\\\\u003Cb\\\\u003E举两个实际的例子：\\\\u003C\\\\u002Fb\\\\u003E\\\\u003C\\\\u002Fli\\\\u003E\\\\u003C\\\\u002Ful\\\\u003E\\\\u003Cp\\\\u003E第一个例子，为了计算用户的年收入，可以利用近一年内支付总金额+理财总余额-信贷总负债的大公式，通过线性回归拟合出三个指标的系数，来得到每个用户预测的收入水平；\\\\u003Cbr\\\\u002F\\\\u003E第二个例子，\\\\u003Cb\\\\u003E给自己在做的模型打个小广告\\\\u003C\\\\u002Fb\\\\u003E，京东金融金融科技业务部基于京东集团商城、金融和物流三大自有数据源以及海量外部数据源，利用XGBoost、LightGBM、CatBoost等复杂集成树类算法，计算得到玉衡分特征，用来衡量京东客户在现金贷场景的信用等级，帮助服务的银行和小贷公司搭建信贷智能决策系统。\\\\u003Cbr\\\\u002F\\\\u003E最后，由于篇幅有限，答主将更多的数据源常见字段以及建议特征工程方向整理到如下表格中，供大家参考，也欢迎交流。\\\\u003Cbr\\\\u002F\\\\u003E\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Cfigure data-size=\\\\\"normal\\\\\"\\\\u003E\\\\u003Cnoscript\\\\u003E\\\\u003Cimg src=\\\\\"https:\\\\u002F\\\\u002Fpic2.zhimg.com\\\\u002F50\\\\u002Fv2-ca5a8ab348ab66b2ee6907a2fe04935e_hd.jpg\\\\\" data-caption=\\\\\"\\\\\" data-size=\\\\\"normal\\\\\" data-rawwidth=\\\\\"806\\\\\" data-rawheight=\\\\\"1522\\\\\" class=\\\\\"origin_image zh-lightbox-thumb\\\\\" width=\\\\\"806\\\\\" data-original=\\\\\"https:\\\\u002F\\\\u002Fpic2.zhimg.com\\\\u002Fv2-ca5a8ab348ab66b2ee6907a2fe04935e_r.jpg\\\\\"\\\\u002F\\\\u003E\\\\u003C\\\\u002Fnoscript\\\\u003E\\\\u003Cimg src=\\\\\"data:image\\\\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\\\\u002F\\\\u002Fwww.w3.org\\\\u002F2000\\\\u002Fsvg&#39; width=&#39;806&#39; height=&#39;1522&#39;&gt;&lt;\\\\u002Fsvg&gt;\\\\\" data-caption=\\\\\"\\\\\" data-size=\\\\\"normal\\\\\" data-rawwidth=\\\\\"806\\\\\" data-rawheight=\\\\\"1522\\\\\" class=\\\\\"origin_image zh-lightbox-thumb lazy\\\\\" width=\\\\\"806\\\\\" data-original=\\\\\"https:\\\\u002F\\\\u002Fpic2.zhimg.com\\\\u002Fv2-ca5a8ab348ab66b2ee6907a2fe04935e_r.jpg\\\\\" data-actualsrc=\\\\\"https:\\\\u002F\\\\u002Fpic2.zhimg.com\\\\u002F50\\\\u002Fv2-ca5a8ab348ab66b2ee6907a2fe04935e_hd.jpg\\\\\"\\\\u002F\\\\u003E\\\\u003C\\\\u002Ffigure\\\\u003E\\\\u003Cp\\\\u003E\\\\u003C\\\\u002Fp\\\\u003E\",\"editableContent\":\"\",\"excerpt\":\"原文已发表于 @京东金融 。前面的很多高赞答案已经把特征工程的方法论总结的很到位了，我这里结合自己的工作经验补充一些具体的例子。 ################################# 找特征这件事，Andrew Ng在深度学习网课中提到过，原课件见第3课结构化机器学习项目中的2.9和2.10两节，笔记整理如下： [图片] [图片] Andrew以Speech Recognition的场景为例，比较了pipeline和end-to-end两种建模方式中特征工程的差异。 其中pipeline的搭建依赖于人工设…\",\"collapsedBy\":\"nobody\",\"collapseReason\":\"\",\"annotationAction\":null,\"markInfos\":[],\"relevantInfo\":{\"isRelevant\":false,\"relevantType\":\"\",\"relevantText\":\"\"},\"suggestEdit\":{\"reason\":\"\",\"status\":false,\"tip\":\"\",\"title\":\"\",\"unnormalDetails\":{\"status\":\"\",\"description\":\"\",\"reason\":\"\",\"reasonId\":0,\"note\":\"\"},\"url\":\"\"},\"isLabeled\":false,\"rewardInfo\":{\"canOpenReward\":false,\"isRewardable\":false,\"rewardMemberCount\":0,\"rewardTotalMoney\":0,\"tagline\":\"\"},\"relationship\":{\"isAuthor\":false,\"isAuthorized\":false,\"isNothelp\":false,\"isThanked\":false,\"isRecognized\":false,\"voting\":0,\"upvotedFollowees\":[]},\"adAnswer\":null},\"607394337\":{\"id\":607394337,\"type\":\"answer\",\"answerType\":\"normal\",\"question\":{\"type\":\"question\",\"id\":29316149,\"title\":\"特征工程到底是什么？\",\"questionType\":\"normal\",\"created\":1428220876,\"updatedTime\":1428220876,\"url\":\"https:\\\\u002F\\\\u002Fwww.zhihu.com\\\\u002Fapi\\\\u002Fv4\\\\u002Fquestions\\\\u002F29316149\",\"relationship\":{}},\"author\":{\"id\":\"6504722cb29a9a52e7a25005c573b267\",\"urlToken\":\"bao-bao-12-67\",\"name\":\"包大人\",\"avatarUrl\":\"https:\\\\u002F\\\\u002Fpic3.zhimg.com\\\\u002Ff15630e4e1a339e31c859146157f7143_l.jpg\",\"avatarUrlTemplate\":\"https:\\\\u002F\\\\u002Fpic4.zhimg.com\\\\u002Ff15630e4e1a339e31c859146157f7143.jpg\",\"isOrg\":false,\"type\":\"people\",\"url\":\"https:\\\\u002F\\\\u002Fwww.zhihu.com\\\\u002Fapi\\\\u002Fv4\\\\u002Fpeople\\\\u002F6504722cb29a9a52e7a25005c573b267\",\"userType\":\"people\",\"headline\":\"深度学习炼丹劝退师\",\"badge\":[{\"type\":\"identity\",\"description\":\"Microsoft SDE\",\"topics\":[]}],\"badgeV2\":{\"title\":\"Microsoft SDE\",\"mergedBadges\":[{\"type\":\"identity\",\"detailType\":\"identity_people\",\"title\":\"已认证的个人\",\"description\":\"Microsoft SDE\",\"url\":\"https:\\\\u002F\\\\u002Fwww.zhihu.com\\\\u002Faccount\\\\u002Fverification\\\\u002Fintro\",\"sources\":[]}],\"detailBadges\":[]},\"gender\":1,\"isAdvertiser\":false,\"followerCount\":5380,\"isFollowed\":false,\"isPrivacy\":false},\"url\":\"https:\\\\u002F\\\\u002Fwww.zhihu.com\\\\u002Fapi\\\\u002Fv4\\\\u002Fanswers\\\\u002F607394337\",\"isCollapsed\":false,\"createdTime\":1551018952,\"updatedTime\":1584958382,\"extras\":\"\",\"isCopyable\":true,\"isNormal\":true,\"voteupCount\":2088,\"commentCount\":39,\"isSticky\":false,\"adminClosedComment\":false,\"commentPermission\":\"all\",\"canComment\":{\"reason\":\"\",\"status\":true},\"reshipmentSettings\":\"allowed\",\"content\":\"\\\\u003Cp\\\\u003E2020-03-20 更新，\\\\u003Cb\\\\u003E很多资料下载需要翻墙，整理好在下面公众号 i数据智能 里面，回复“特征工程”\\\\u003C\\\\u002Fb\\\\u003E即可获取：\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Cp\\\\u003E\\\\u003Ca href=\\\\\"https:\\\\u002F\\\\u002Flink.zhihu.com\\\\u002F?target=http%3A\\\\u002F\\\\u002Fweixin.qq.com\\\\u002Fr\\\\u002FPkT17dnEw4S8rZ2O9xEs\\\\\" class=\\\\\" external\\\\\" target=\\\\\"_blank\\\\\" rel=\\\\\"nofollow noreferrer\\\\\"\\\\u003E\\\\u003Cspan class=\\\\\"invisible\\\\\"\\\\u003Ehttp:\\\\u002F\\\\u002F\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"visible\\\\\"\\\\u003Eweixin.qq.com\\\\u002Fr\\\\u002FPkT17dn\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"invisible\\\\\"\\\\u003EEw4S8rZ2O9xEs\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"ellipsis\\\\\"\\\\u003E\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003C\\\\u002Fa\\\\u003E (二维码自动识别)\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Cp\\\\u003E特征工程是机器学习，甚至是深度学习中最为重要的一部分，\\\\u003Cb\\\\u003E也是课本上最不愿意讲的一部分\\\\u003C\\\\u002Fb\\\\u003E，特征工程往往是打开数据密码的钥匙，\\\\u003Cb\\\\u003E是数据科学中最有创造力的一部分\\\\u003C\\\\u002Fb\\\\u003E。因为往往和具体的数据相结合，很难优雅地系统地讲好。所以课本上会讲一下理论知识比较扎实的\\\\u003Cb\\\\u003E归一化\\\\u003C\\\\u002Fb\\\\u003E，\\\\u003Cb\\\\u003E降维\\\\u003C\\\\u002Fb\\\\u003E等部分，而忽略一些很dirty hand的特征工程技巧。\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Cp\\\\u003E什么是特征工程呢？一个非常简单的例子，现在出一非常简答的二分类问题题，请你使用逻辑回归，设计一个身材分类器。输入数据X:身高和体重 ，标签为Y:身材等级（胖，不胖）。显然，不能单纯的根据体重来判断一个人胖不胖，姚明很重，他胖吗？显然不是。针对这个问题，一个非常经典的特征工程是，\\\\u003Cb\\\\u003EBMI指数，BMI=体重\\\\u002F(身高^2)。这样，通过BMI指数，\\\\u003C\\\\u002Fb\\\\u003E就能非常显然地帮助我们，刻画一个人身材如何。\\\\u003Cb\\\\u003E甚至，你可以抛弃原始的体重和身高数据。\\\\u003C\\\\u002Fb\\\\u003E\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Cp\\\\u003E所以说，特征工程就是\\\\u003Cb\\\\u003E通过X，创造新的X&#39;。\\\\u003C\\\\u002Fb\\\\u003E基本的操作包括\\\\u003Cb\\\\u003E，衍生（升维），筛选（降维）。说\\\\u003C\\\\u002Fb\\\\u003E起来简单，实际中，衍生和筛选都是困难重重，甚至需要非常专业的专家知识。\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Cfigure data-size=\\\\\"normal\\\\\"\\\\u003E\\\\u003Cnoscript\\\\u003E\\\\u003Cimg src=\\\\\"https:\\\\u002F\\\\u002Fpic3.zhimg.com\\\\u002F50\\\\u002Fv2-2fa04777f16f18adaf9a0181fac3fa53_hd.jpg\\\\\" data-rawwidth=\\\\\"924\\\\\" data-rawheight=\\\\\"329\\\\\" data-size=\\\\\"normal\\\\\" data-caption=\\\\\"\\\\\" data-default-watermark-src=\\\\\"https:\\\\u002F\\\\u002Fpic2.zhimg.com\\\\u002F50\\\\u002Fv2-75b5986059835dcc66aa13c1f823c74b_hd.jpg\\\\\" class=\\\\\"origin_image zh-lightbox-thumb\\\\\" width=\\\\\"924\\\\\" data-original=\\\\\"https:\\\\u002F\\\\u002Fpic1.zhimg.com\\\\u002Fv2-2fa04777f16f18adaf9a0181fac3fa53_r.jpg\\\\\"\\\\u002F\\\\u003E\\\\u003C\\\\u002Fnoscript\\\\u003E\\\\u003Cimg src=\\\\\"data:image\\\\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\\\\u002F\\\\u002Fwww.w3.org\\\\u002F2000\\\\u002Fsvg&#39; width=&#39;924&#39; height=&#39;329&#39;&gt;&lt;\\\\u002Fsvg&gt;\\\\\" data-rawwidth=\\\\\"924\\\\\" data-rawheight=\\\\\"329\\\\\" data-size=\\\\\"normal\\\\\" data-caption=\\\\\"\\\\\" data-default-watermark-src=\\\\\"https:\\\\u002F\\\\u002Fpic2.zhimg.com\\\\u002F50\\\\u002Fv2-75b5986059835dcc66aa13c1f823c74b_hd.jpg\\\\\" class=\\\\\"origin_image zh-lightbox-thumb lazy\\\\\" width=\\\\\"924\\\\\" data-original=\\\\\"https:\\\\u002F\\\\u002Fpic1.zhimg.com\\\\u002Fv2-2fa04777f16f18adaf9a0181fac3fa53_r.jpg\\\\\" data-actualsrc=\\\\\"https:\\\\u002F\\\\u002Fpic3.zhimg.com\\\\u002F50\\\\u002Fv2-2fa04777f16f18adaf9a0181fac3fa53_hd.jpg\\\\\"\\\\u002F\\\\u003E\\\\u003C\\\\u002Ffigure\\\\u003E\\\\u003Cp\\\\u003E当然，人类的进化也就是一直在变懒，深度学习的兴起，让很多人相信，deep learning，作为一种强大的\\\\u003Cb\\\\u003E自动化特征工程工具，\\\\u003C\\\\u002Fb\\\\u003E能够自动学习各种低级和高级的特征。至少在视觉领域，似乎没有人愿意提起SIFT算子，仿佛被翻进了历史的另一页。\\\\u003Cb\\\\u003EEmbedding为代表的表征学习，在大规模稀疏ID问题和多值ID问题上，\\\\u003C\\\\u002Fb\\\\u003E似乎也能搞定自动化特征工程，于是互大家看到很多，各种千奇百怪的文本分类模型，CTR预估模型，似乎就是Embedding后面一顿盘他，最终转化成分类问题，似乎并不需要特征工程这门祖传的手艺了。\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Cp\\\\u003E\\\\u003Cb\\\\u003E然而真的是这样吗？\\\\u003C\\\\u002Fb\\\\u003E\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Cp\\\\u003E讲一个比较复杂的例子，\\\\u003Cb\\\\u003E如果你对speech稍有了解，或者\\\\u003C\\\\u002Fb\\\\u003E做过\\\\u003Cb\\\\u003E说话人验证\\\\u002F声纹识别（SVR）任务\\\\u003C\\\\u002Fb\\\\u003E，你会知道，有一种特征工程叫做\\\\u003Cb\\\\u003EMFCC \\\\u003C\\\\u002Fb\\\\u003E特征，现在解决说话人本身特性的问题，前端还是无法离开MFCC，而我认为MFCC是一种非常有代表性的饱含了专家知识的特征工程，感兴趣的同学可以了解一下：\\\\u003Ca href=\\\\\"https:\\\\u002F\\\\u002Flink.zhihu.com\\\\u002F?target=https%3A\\\\u002F\\\\u002Fzh.wikipedia.org\\\\u002Fwiki\\\\u002F%25E6%25A2%2585%25E5%25B0%2594%25E9%25A2%2591%25E7%258E%2587%25E5%2580%2592%25E8%25B0%25B1%25E7%25B3%25BB%25E6%2595%25B0\\\\\" class=\\\\\" wrap external\\\\\" target=\\\\\"_blank\\\\\" rel=\\\\\"nofollow noreferrer\\\\\"\\\\u003EMFCC(梅尔频率倒谱系数)\\\\u003C\\\\u002Fa\\\\u003E\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Cp\\\\u003E关于特征工程的系统部分前面的回答已经总结的很好了。如果你读过很多博客，书籍，你会发现，特征工程的\\\\u003Cb\\\\u003E实践和理论是高度割裂的\\\\u003C\\\\u002Fb\\\\u003E，在这里，我要推荐一门coursera的课程，\\\\u003Ca href=\\\\\"https:\\\\u002F\\\\u002Flink.zhihu.com\\\\u002F?target=https%3A\\\\u002F\\\\u002Fwww.coursera.org\\\\u002Fspecializations\\\\u002Faml%3FsiteID%3DlVarvwc5BD0-AqkGMb7JzoCMW0Np1uLfCA%26utm_campaign%3DlVarvwc5BD0%26utm_content%3D2%26utm_medium%3Dpartners%26utm_source%3Dlinkshare\\\\\" class=\\\\\" wrap external\\\\\" target=\\\\\"_blank\\\\\" rel=\\\\\"nofollow noreferrer\\\\\"\\\\u003EAdvanced Machine Learning（高级机器学习）\\\\u003C\\\\u002Fa\\\\u003E\\\\u003Cb\\\\u003E，\\\\u003C\\\\u002Fb\\\\u003E有Kazanova大神授课的部分，（顶级Kaggle GM，巅峰TOP3），这里面有很多来源于实践中的很多技巧。\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Cp\\\\u003EKaggle上有一句非常经典的话，\\\\u003Cb\\\\u003E数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已\\\\u003C\\\\u002Fb\\\\u003E，而这恰恰是课堂上最为缺失，一门需要在实践中学习的手艺。\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Cp\\\\u003E其实是有一本讲特征工程的书的，不过并不推荐，因为实在是太简单了，可能看了就跟喝了一杯白开水一样。链接在下面\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Ca data-draft-node=\\\\\"block\\\\\" data-draft-type=\\\\\"link-card\\\\\" href=\\\\\"https:\\\\u002F\\\\u002Flink.zhihu.com\\\\u002F?target=https%3A\\\\u002F\\\\u002Fperso.limsi.fr\\\\u002Fannlor\\\\u002Fenseignement\\\\u002Fensiie\\\\u002FFeature_Engineering_for_Machine_Learning.pdf\\\\\" class=\\\\\" wrap external\\\\\" target=\\\\\"_blank\\\\\" rel=\\\\\"nofollow noreferrer\\\\\"\\\\u003EFeature_Engineering_for_Machine_Learning.pdf\\\\u003C\\\\u002Fa\\\\u003E\\\\u003Cp\\\\u003E长这个样子\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Cfigure data-size=\\\\\"normal\\\\\"\\\\u003E\\\\u003Cnoscript\\\\u003E\\\\u003Cimg src=\\\\\"https:\\\\u002F\\\\u002Fpic2.zhimg.com\\\\u002F50\\\\u002Fv2-50420df02b8ae50c53b642ad411bd2d0_hd.jpg\\\\\" data-rawwidth=\\\\\"473\\\\\" data-rawheight=\\\\\"620\\\\\" data-size=\\\\\"normal\\\\\" data-caption=\\\\\"\\\\\" data-default-watermark-src=\\\\\"https:\\\\u002F\\\\u002Fpic2.zhimg.com\\\\u002F50\\\\u002Fv2-57367ab87b47d6070e8d8fcee3d88956_hd.jpg\\\\\" class=\\\\\"origin_image zh-lightbox-thumb\\\\\" width=\\\\\"473\\\\\" data-original=\\\\\"https:\\\\u002F\\\\u002Fpic1.zhimg.com\\\\u002Fv2-50420df02b8ae50c53b642ad411bd2d0_r.jpg\\\\\"\\\\u002F\\\\u003E\\\\u003C\\\\u002Fnoscript\\\\u003E\\\\u003Cimg src=\\\\\"data:image\\\\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\\\\u002F\\\\u002Fwww.w3.org\\\\u002F2000\\\\u002Fsvg&#39; width=&#39;473&#39; height=&#39;620&#39;&gt;&lt;\\\\u002Fsvg&gt;\\\\\" data-rawwidth=\\\\\"473\\\\\" data-rawheight=\\\\\"620\\\\\" data-size=\\\\\"normal\\\\\" data-caption=\\\\\"\\\\\" data-default-watermark-src=\\\\\"https:\\\\u002F\\\\u002Fpic2.zhimg.com\\\\u002F50\\\\u002Fv2-57367ab87b47d6070e8d8fcee3d88956_hd.jpg\\\\\" class=\\\\\"origin_image zh-lightbox-thumb lazy\\\\\" width=\\\\\"473\\\\\" data-original=\\\\\"https:\\\\u002F\\\\u002Fpic1.zhimg.com\\\\u002Fv2-50420df02b8ae50c53b642ad411bd2d0_r.jpg\\\\\" data-actualsrc=\\\\\"https:\\\\u002F\\\\u002Fpic2.zhimg.com\\\\u002F50\\\\u002Fv2-50420df02b8ae50c53b642ad411bd2d0_hd.jpg\\\\\"\\\\u002F\\\\u003E\\\\u003C\\\\u002Ffigure\\\\u003E\\\\u003Cp\\\\u003E下面有一个非常神奇的slide，非常推荐，非常好，真香。\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Ca data-draft-node=\\\\\"block\\\\\" data-draft-type=\\\\\"link-card\\\\\" href=\\\\\"https:\\\\u002F\\\\u002Flink.zhihu.com\\\\u002F?target=https%3A\\\\u002F\\\\u002Fwww.slideshare.net\\\\u002FHJvanVeen\\\\u002Ffeature-engineering-72376750\\\\\" class=\\\\\" wrap external\\\\\" target=\\\\\"_blank\\\\\" rel=\\\\\"nofollow noreferrer\\\\\"\\\\u003Efeature-engineering\\\\u003C\\\\u002Fa\\\\u003E\\\\u003Cp\\\\u003E基本涵盖了最基本的特征工程方法（套路）。\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Cp\\\\u003E大致如下：\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Cp\\\\u003E\\\\u003Cb\\\\u003E1.类别特征\\\\u003C\\\\u002Fb\\\\u003E\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Cul\\\\u003E\\\\u003Cli\\\\u003Eone-hot encoding\\\\u003C\\\\u002Fli\\\\u003E\\\\u003Cli\\\\u003Ehash encoding\\\\u003C\\\\u002Fli\\\\u003E\\\\u003Cli\\\\u003Elabel encoding\\\\u003C\\\\u002Fli\\\\u003E\\\\u003Cli\\\\u003Ecount encoding\\\\u003C\\\\u002Fli\\\\u003E\\\\u003Cli\\\\u003Elabel-count encoding\\\\u003C\\\\u002Fli\\\\u003E\\\\u003Cli\\\\u003Etarget encoding\\\\u003C\\\\u002Fli\\\\u003E\\\\u003Cli\\\\u003Ecategory embedding\\\\u003C\\\\u002Fli\\\\u003E\\\\u003Cli\\\\u003ENan encoding\\\\u003C\\\\u002Fli\\\\u003E\\\\u003Cli\\\\u003Epolynomial encoding\\\\u003C\\\\u002Fli\\\\u003E\\\\u003Cli\\\\u003Eexpansion encoding\\\\u003C\\\\u002Fli\\\\u003E\\\\u003Cli\\\\u003Econsolidation encoding\\\\u003C\\\\u002Fli\\\\u003E\\\\u003C\\\\u002Ful\\\\u003E\\\\u003Cp\\\\u003E\\\\u003Cb\\\\u003E2.数值特征\\\\u003C\\\\u002Fb\\\\u003E\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Cul\\\\u003E\\\\u003Cli\\\\u003Erounding\\\\u003C\\\\u002Fli\\\\u003E\\\\u003Cli\\\\u003Ebinning\\\\u003C\\\\u002Fli\\\\u003E\\\\u003Cli\\\\u003Escaling\\\\u003C\\\\u002Fli\\\\u003E\\\\u003Cli\\\\u003Eimputation\\\\u003C\\\\u002Fli\\\\u003E\\\\u003Cli\\\\u003Einteractions\\\\u003C\\\\u002Fli\\\\u003E\\\\u003Cli\\\\u003Eno linear encoding\\\\u003C\\\\u002Fli\\\\u003E\\\\u003Cli\\\\u003Erow statistics\\\\u003C\\\\u002Fli\\\\u003E\\\\u003C\\\\u002Ful\\\\u003E\\\\u003Cp\\\\u003E\\\\u003Cb\\\\u003E4.时间特征\\\\u003C\\\\u002Fb\\\\u003E\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Cp\\\\u003E\\\\u003Cb\\\\u003E5.空间特征\\\\u003C\\\\u002Fb\\\\u003E\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Cp\\\\u003E\\\\u003Cb\\\\u003E6.自然语言处理\\\\u003C\\\\u002Fb\\\\u003E\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Cp\\\\u003E\\\\u003Cb\\\\u003E7.深度学习\\\\u002FNN\\\\u003C\\\\u002Fb\\\\u003E\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Cp\\\\u003E\\\\u003Cb\\\\u003E8.Leakage\\\\u003C\\\\u002Fb\\\\u003E\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Cp\\\\u003E当然以上并不全，还有很多非常经典的聚合统计，Graph上的特征工程等等。\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Cp\\\\u003E文本相似性是一个非常好的体味特征工程威力，对比深度学习效果的一个问题，不妨去看看。\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Ca data-draft-node=\\\\\"block\\\\\" data-draft-type=\\\\\"link-card\\\\\" href=\\\\\"https:\\\\u002F\\\\u002Flink.zhihu.com\\\\u002F?target=https%3A\\\\u002F\\\\u002Fwww.kaggle.com\\\\u002Fc\\\\u002Fquora-question-pairs\\\\\" class=\\\\\" wrap external\\\\\" target=\\\\\"_blank\\\\\" rel=\\\\\"nofollow noreferrer\\\\\"\\\\u003EQuora Question Pairs | Kaggle\\\\u003C\\\\u002Fa\\\\u003E\\\\u003Cp\\\\u003EAutoML，抛开比较火热的NAS研究方向，对自动化特征工程也有一些研究，果然人类的本质是在变懒，关于AutoML是如何解决自动化特征工程的，可以看我专栏里的AutoML概览，在很多场景下，尤其是匿名，数据丧失原始含义的情况下，能不能一把梭就看automl或者你神奇的大脑去解谜了。\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Cp\\\\u003E再补充一些资料：\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Cp\\\\u003E书籍：\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Cp\\\\u003E\\\\u003Ca href=\\\\\"https:\\\\u002F\\\\u002Flink.zhihu.com\\\\u002F?target=http%3A\\\\u002F\\\\u002Fwww.amazon.com\\\\u002Fdp\\\\u002F0792381963%3Ftag%3Dinspiredalgor-20\\\\\" class=\\\\\" wrap external\\\\\" target=\\\\\"_blank\\\\\" rel=\\\\\"nofollow noreferrer\\\\\"\\\\u003EAmazon.com: Feature Extraction, Construction and Selection: A Data Mining Perspective (The Springer International Series in Engineering and Computer Science) (9780792381969): Huan Liu, Hiroshi Motoda: Books\\\\u003C\\\\u002Fa\\\\u003E\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Cp\\\\u003E\\\\u003Ca href=\\\\\"https:\\\\u002F\\\\u002Flink.zhihu.com\\\\u002F?target=http%3A\\\\u002F\\\\u002Fwww.amazon.com\\\\u002Fdp\\\\u002F3540354875%3Ftag%3Dinspiredalgor-20\\\\\" class=\\\\\" wrap external\\\\\" target=\\\\\"_blank\\\\\" rel=\\\\\"nofollow noreferrer\\\\\"\\\\u003EFeature Extraction: Foundations and Applications (Studies in Fuzziness and Soft Computing): Isabelle Guyon, Steve Gunn, Masoud Nikravesh, Lofti A. Zadeh: 9783540354871: Amazon.com: Books\\\\u003C\\\\u002Fa\\\\u003E\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Cp\\\\u003E\\\\u003Ca href=\\\\\"https:\\\\u002F\\\\u002Flink.zhihu.com\\\\u002F?target=http%3A\\\\u002F\\\\u002Fwww.amazon.com\\\\u002Fdp\\\\u002F0123965497%3Ftag%3Dinspiredalgor-20\\\\\" class=\\\\\" wrap external\\\\\" target=\\\\\"_blank\\\\\" rel=\\\\\"nofollow noreferrer\\\\\"\\\\u003EMark Nixon: 9780123965493: Amazon.com: Books\\\\u003C\\\\u002Fa\\\\u003E\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Cp\\\\u003E\\\\u003Ca href=\\\\\"https:\\\\u002F\\\\u002Flink.zhihu.com\\\\u002F?target=http%3A\\\\u002F\\\\u002Fwww.amazon.com\\\\u002Fdp\\\\u002F079238198X%3Ftag%3Dinspiredalgor-20\\\\\" class=\\\\\" wrap external\\\\\" target=\\\\\"_blank\\\\\" rel=\\\\\"nofollow noreferrer\\\\\"\\\\u003EHuan Liu, Hiroshi Motoda: 9780792381983: Amazon.com: Books\\\\u003C\\\\u002Fa\\\\u003E\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Cp\\\\u003E\\\\u003Ca href=\\\\\"https:\\\\u002F\\\\u002Flink.zhihu.com\\\\u002F?target=http%3A\\\\u002F\\\\u002Fwww.amazon.com\\\\u002Fdp\\\\u002F1584888784%3Ftag%3Dinspiredalgor-20\\\\\" class=\\\\\" wrap external\\\\\" target=\\\\\"_blank\\\\\" rel=\\\\\"nofollow noreferrer\\\\\"\\\\u003EAmazon.com: Computational Methods of Feature Selection (Chapman &amp; Hall\\\\u002FCRC Data Mining and Knowledge Discovery Series) (9781584888789): Huan Liu, Hiroshi Motoda: Books\\\\u003C\\\\u002Fa\\\\u003E\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Cp\\\\u003ESlides:\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Cp\\\\u003E\\\\u003Ca href=\\\\\"https:\\\\u002F\\\\u002Flink.zhihu.com\\\\u002F?target=http%3A\\\\u002F\\\\u002Fkti.tugraz.at\\\\u002Fstaff\\\\u002Fdenis\\\\u002Fcourses\\\\u002Fkddm1\\\\u002Ffeatureengineering.pdf\\\\\" class=\\\\\" wrap external\\\\\" target=\\\\\"_blank\\\\\" rel=\\\\\"nofollow noreferrer\\\\\"\\\\u003EFeature Engineering (PDF), Knowledge Discover and Data Mining 1, by Roman Kern\\\\u003C\\\\u002Fa\\\\u003E\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Cp\\\\u003E\\\\u003Ca href=\\\\\"https:\\\\u002F\\\\u002Flink.zhihu.com\\\\u002F?target=http%3A\\\\u002F\\\\u002Fwww.cs.berkeley.edu\\\\u002F~jordan\\\\u002Fcourses\\\\u002F294-fall09\\\\u002Flectures\\\\u002Ffeature\\\\u002Fslides.pdf\\\\\" class=\\\\\" wrap external\\\\\" target=\\\\\"_blank\\\\\" rel=\\\\\"nofollow noreferrer\\\\\"\\\\u003EFeature Engineering and Selection\\\\u003C\\\\u002Fa\\\\u003E\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Cp\\\\u003E\\\\u003Ca href=\\\\\"https:\\\\u002F\\\\u002Flink.zhihu.com\\\\u002F?target=http%3A\\\\u002F\\\\u002Fwww.cs.princeton.edu\\\\u002Fcourses\\\\u002Farchive\\\\u002Fspring10\\\\u002Fcos424\\\\u002Fslides\\\\u002F18-feat.pdf\\\\\" class=\\\\\" wrap external\\\\\" target=\\\\\"_blank\\\\\" rel=\\\\\"nofollow noreferrer\\\\\"\\\\u003EFeature Engineering\\\\u003C\\\\u002Fa\\\\u003E\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Cp\\\\u003E\\\\u003Ca href=\\\\\"https:\\\\u002F\\\\u002Flink.zhihu.com\\\\u002F?target=http%3A\\\\u002F\\\\u002Fpslcdatashop.org\\\\u002FKDDCup\\\\u002Fworkshop\\\\u002Fpapers\\\\u002Fkdd2010ntu.pdf\\\\\" class=\\\\\" wrap external\\\\\" target=\\\\\"_blank\\\\\" rel=\\\\\"nofollow noreferrer\\\\\"\\\\u003EKDD CUP 2010年冠军的论文\\\\u003C\\\\u002Fa\\\\u003E\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Cp class=\\\\\"ztext-empty-paragraph\\\\\"\\\\u003E\\\\u003Cbr\\\\u002F\\\\u003E\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Cp\\\\u003E课程\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Cp\\\\u003E\\\\u003Ca href=\\\\\"https:\\\\u002F\\\\u002Flink.zhihu.com\\\\u002F?target=http%3A\\\\u002F\\\\u002Fwww.cs.berkeley.edu\\\\u002F~jordan\\\\u002Fcourses\\\\u002F294-fall09\\\\u002Flectures\\\\u002Ffeature\\\\u002F\\\\\" class=\\\\\" wrap external\\\\\" target=\\\\\"_blank\\\\\" rel=\\\\\"nofollow noreferrer\\\\\"\\\\u003EFeature selection. berkeley\\\\u003C\\\\u002Fa\\\\u003E\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Cp\\\\u003E\\\\u003Ca href=\\\\\"https:\\\\u002F\\\\u002Flink.zhihu.com\\\\u002F?target=http%3A\\\\u002F\\\\u002Fkti.tugraz.at\\\\u002Fstaff\\\\u002Fdenis\\\\u002Fcourses\\\\u002Fkddm1\\\\u002F\\\\\" class=\\\\\" wrap external\\\\\" target=\\\\\"_blank\\\\\" rel=\\\\\"nofollow noreferrer\\\\\"\\\\u003EKnowledge Discovery and Data Mining 1\\\\u003C\\\\u002Fa\\\\u003E\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Cp\\\\u003E\\\\u003Ca href=\\\\\"https:\\\\u002F\\\\u002Flink.zhihu.com\\\\u002F?target=https%3A\\\\u002F\\\\u002Fwww.youtube.com\\\\u002Fwatch%3Fv%3DdrUToKxEAUA\\\\\" class=\\\\\" wrap external\\\\\" target=\\\\\"_blank\\\\\" rel=\\\\\"nofollow noreferrer\\\\\"\\\\u003E油管上CMU授课的特征工程\\\\u003C\\\\u002Fa\\\\u003E\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Cp\\\\u003E\\\\u003Ca href=\\\\\"https:\\\\u002F\\\\u002Flink.zhihu.com\\\\u002F?target=http%3A\\\\u002F\\\\u002Fwww.columbia.edu\\\\u002F~rsb2162\\\\u002FFES2013\\\\u002Fmaterials.html\\\\\" class=\\\\\" wrap external\\\\\" target=\\\\\"_blank\\\\\" rel=\\\\\"nofollow noreferrer\\\\\"\\\\u003EFES.columbia\\\\u003C\\\\u002Fa\\\\u003E\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Cp\\\\u003E博客\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Cp\\\\u003E\\\\u003Ca href=\\\\\"https:\\\\u002F\\\\u002Flink.zhihu.com\\\\u002F?target=https%3A\\\\u002F\\\\u002Fmachinelearningmastery.com\\\\u002Fdiscover-feature-engineering-how-to-engineer-features-and-how-to-get-good-at-it\\\\u002F\\\\\" class=\\\\\" wrap external\\\\\" target=\\\\\"_blank\\\\\" rel=\\\\\"nofollow noreferrer\\\\\"\\\\u003Ediscover-feature-engineering-how-to-engineer-features-and-how-to-get-good-at-it\\\\u003C\\\\u002Fa\\\\u003E\\\\u003C\\\\u002Fp\\\\u003E\",\"editableContent\":\"\",\"excerpt\":\"2020-03-20 更新， 很多资料下载需要翻墙，整理好在下面公众号 i数据智能 里面，回复“特征工程”即可获取： [图片] 特征工程是机器学习，甚至是深度学习中最为重要的一部分， 也是课本上最不愿意讲的一部分，特征工程往往是打开数据密码的钥匙，是数据科学中最有创造力的一部分。因为往往和具体的数据相结合，很难优雅地系统地讲好。所以课本上会讲一下理论知识比较扎实的归一化，降维等部分，而忽略一些很dirty hand的特征工程技巧。什…\",\"collapsedBy\":\"nobody\",\"collapseReason\":\"\",\"annotationAction\":null,\"markInfos\":[],\"relevantInfo\":{\"isRelevant\":false,\"relevantType\":\"\",\"relevantText\":\"\"},\"suggestEdit\":{\"reason\":\"\",\"status\":false,\"tip\":\"\",\"title\":\"\",\"unnormalDetails\":{\"status\":\"\",\"description\":\"\",\"reason\":\"\",\"reasonId\":0,\"note\":\"\"},\"url\":\"\"},\"isLabeled\":true,\"rewardInfo\":{\"canOpenReward\":false,\"isRewardable\":true,\"rewardMemberCount\":3,\"rewardTotalMoney\":0,\"tagline\":\"真诚赞赏，手留余香\"},\"relationship\":{\"isAuthor\":false,\"isAuthorized\":false,\"isNothelp\":false,\"isThanked\":false,\"isRecognized\":false,\"voting\":0,\"upvotedFollowees\":[]},\"adAnswer\":null},\"613063254\":{\"id\":613063254,\"type\":\"answer\",\"answerType\":\"normal\",\"question\":{\"type\":\"question\",\"id\":29316149,\"title\":\"特征工程到底是什么？\",\"questionType\":\"normal\",\"created\":1428220876,\"updatedTime\":1428220876,\"url\":\"https:\\\\u002F\\\\u002Fwww.zhihu.com\\\\u002Fapi\\\\u002Fv4\\\\u002Fquestions\\\\u002F29316149\",\"relationship\":{}},\"author\":{\"id\":\"5f1c23628ef79355f7a91295d24eb80e\",\"urlToken\":\"ran-jing-14\",\"name\":\"管他叫大靖\",\"avatarUrl\":\"https:\\\\u002F\\\\u002Fpic1.zhimg.com\\\\u002Fv2-314f28f2e86ce0f3f90c4a54a06d39ec_l.jpg\",\"avatarUrlTemplate\":\"https:\\\\u002F\\\\u002Fpic1.zhimg.com\\\\u002Fv2-314f28f2e86ce0f3f90c4a54a06d39ec.jpg\",\"isOrg\":false,\"type\":\"people\",\"url\":\"https:\\\\u002F\\\\u002Fwww.zhihu.com\\\\u002Fapi\\\\u002Fv4\\\\u002Fpeople\\\\u002F5f1c23628ef79355f7a91295d24eb80e\",\"userType\":\"people\",\"headline\":\"中科院应用数学所  统计学博士在读\",\"badge\":[],\"badgeV2\":{\"title\":\"\",\"mergedBadges\":[],\"detailBadges\":[]},\"gender\":1,\"isAdvertiser\":false,\"followerCount\":4628,\"isFollowed\":false,\"isPrivacy\":false},\"url\":\"https:\\\\u002F\\\\u002Fwww.zhihu.com\\\\u002Fapi\\\\u002Fv4\\\\u002Fanswers\\\\u002F613063254\",\"isCollapsed\":false,\"createdTime\":1551599580,\"updatedTime\":1551845590,\"extras\":\"\",\"isCopyable\":true,\"isNormal\":true,\"voteupCount\":238,\"commentCount\":7,\"isSticky\":false,\"adminClosedComment\":false,\"commentPermission\":\"all\",\"canComment\":{\"reason\":\"\",\"status\":true},\"reshipmentSettings\":\"allowed\",\"content\":\"\\\\u003Cp\\\\u003E在我的理解里，特征工程可以分为两个方面：\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Col\\\\u003E\\\\u003Cli\\\\u003E\\\\u003Cb\\\\u003E构造特征\\\\u003C\\\\u002Fb\\\\u003E\\\\u003C\\\\u002Fli\\\\u003E\\\\u003Cli\\\\u003E\\\\u003Cb\\\\u003E变量选择\\\\u003C\\\\u002Fb\\\\u003E\\\\u003C\\\\u002Fli\\\\u003E\\\\u003C\\\\u002Fol\\\\u003E\\\\u003Cfigure data-size=\\\\\"normal\\\\\"\\\\u003E\\\\u003Cnoscript\\\\u003E\\\\u003Cimg src=\\\\\"https:\\\\u002F\\\\u002Fpic1.zhimg.com\\\\u002F50\\\\u002Fv2-84193c6b9cc600f22804250355ebed47_hd.jpg\\\\\" data-caption=\\\\\"\\\\\" data-size=\\\\\"normal\\\\\" data-rawwidth=\\\\\"1433\\\\\" data-rawheight=\\\\\"805\\\\\" data-default-watermark-src=\\\\\"https:\\\\u002F\\\\u002Fpic4.zhimg.com\\\\u002F50\\\\u002Fv2-3284a96d4448ad7dc7d65aa5b4ed71af_hd.jpg\\\\\" class=\\\\\"origin_image zh-lightbox-thumb\\\\\" width=\\\\\"1433\\\\\" data-original=\\\\\"https:\\\\u002F\\\\u002Fpic2.zhimg.com\\\\u002Fv2-84193c6b9cc600f22804250355ebed47_r.jpg\\\\\"\\\\u002F\\\\u003E\\\\u003C\\\\u002Fnoscript\\\\u003E\\\\u003Cimg src=\\\\\"data:image\\\\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\\\\u002F\\\\u002Fwww.w3.org\\\\u002F2000\\\\u002Fsvg&#39; width=&#39;1433&#39; height=&#39;805&#39;&gt;&lt;\\\\u002Fsvg&gt;\\\\\" data-caption=\\\\\"\\\\\" data-size=\\\\\"normal\\\\\" data-rawwidth=\\\\\"1433\\\\\" data-rawheight=\\\\\"805\\\\\" data-default-watermark-src=\\\\\"https:\\\\u002F\\\\u002Fpic4.zhimg.com\\\\u002F50\\\\u002Fv2-3284a96d4448ad7dc7d65aa5b4ed71af_hd.jpg\\\\\" class=\\\\\"origin_image zh-lightbox-thumb lazy\\\\\" width=\\\\\"1433\\\\\" data-original=\\\\\"https:\\\\u002F\\\\u002Fpic2.zhimg.com\\\\u002Fv2-84193c6b9cc600f22804250355ebed47_r.jpg\\\\\" data-actualsrc=\\\\\"https:\\\\u002F\\\\u002Fpic1.zhimg.com\\\\u002F50\\\\u002Fv2-84193c6b9cc600f22804250355ebed47_hd.jpg\\\\\"\\\\u002F\\\\u003E\\\\u003C\\\\u002Ffigure\\\\u003E\\\\u003Cp\\\\u003E下面从这两个角度来谈谈。\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Ch2\\\\u003E目录\\\\u003C\\\\u002Fh2\\\\u003E\\\\u003Col\\\\u003E\\\\u003Cli\\\\u003E\\\\u003Cb\\\\u003E构造特征\\\\u003C\\\\u002Fb\\\\u003E\\\\u003C\\\\u002Fli\\\\u003E\\\\u003Col\\\\u003E\\\\u003Cli\\\\u003E实际问题\\\\u003C\\\\u002Fli\\\\u003E\\\\u003Cli\\\\u003E构造思路\\\\u003C\\\\u002Fli\\\\u003E\\\\u003Cli\\\\u003E一些思考\\\\u003C\\\\u002Fli\\\\u003E\\\\u003C\\\\u002Fol\\\\u003E\\\\u003Cli\\\\u003E\\\\u003Cb\\\\u003E变量选择\\\\u003C\\\\u002Fb\\\\u003E\\\\u003C\\\\u002Fli\\\\u003E\\\\u003Col\\\\u003E\\\\u003Cli\\\\u003E单变量筛选\\\\u003C\\\\u002Fli\\\\u003E\\\\u003Cli\\\\u003E通过模型选择变量\\\\u003C\\\\u002Fli\\\\u003E\\\\u003C\\\\u002Fol\\\\u003E\\\\u003Cli\\\\u003E\\\\u003Cb\\\\u003E变量选择——进阶\\\\u003C\\\\u002Fb\\\\u003E\\\\u003C\\\\u002Fli\\\\u003E\\\\u003Col\\\\u003E\\\\u003Cli\\\\u003E只用模型就能选好变量么\\\\u003C\\\\u002Fli\\\\u003E\\\\u003Cli\\\\u003E认识伪相关\\\\u003C\\\\u002Fli\\\\u003E\\\\u003Cli\\\\u003E两步估计法\\\\u003C\\\\u002Fli\\\\u003E\\\\u003C\\\\u002Fol\\\\u003E\\\\u003C\\\\u002Fol\\\\u003E\\\\u003Ch2\\\\u003E一、构造特征\\\\u003C\\\\u002Fh2\\\\u003E\\\\u003Cp\\\\u003E构造特征往往是一个经验的过程，需要对业务有深入的理解。\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Ch3\\\\u003E实际问题\\\\u003C\\\\u002Fh3\\\\u003E\\\\u003Cp\\\\u003E一个问题：\\\\u003Cb\\\\u003E如何预测一家奶茶店的营业额，寻找影响营业额的外部因素？\\\\u003C\\\\u002Fb\\\\u003E\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Cp\\\\u003E假设我们可以拿到奶茶店每个月的营业额，除此之外没有更多数据。时间序列模型？可能不管用。理由如下：\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Col\\\\u003E\\\\u003Cli\\\\u003E什么是建模？\\\\u003Cb\\\\u003E根据可以得到的数据训练模型并进行靠谱的预测\\\\u003C\\\\u002Fb\\\\u003E。如果现在需要评估A、B、C、D四个位置的好坏，预测潜在市场容量，以支持门店选址。这时没有历史营业额，只能使用外部商业特征。\\\\u003C\\\\u002Fli\\\\u003E\\\\u003Cli\\\\u003E\\\\u003Cb\\\\u003E时间序列模型的解释力可能不够。\\\\u003C\\\\u002Fb\\\\u003E为什么下个月的营业额预测值较高？因为这个月的营业额高。什么？没听说隔壁那家奶茶店下个月开张，还不下调预测。\\\\u003C\\\\u002Fli\\\\u003E\\\\u003C\\\\u002Fol\\\\u003E\\\\u003Ch3\\\\u003E构造思路\\\\u003C\\\\u002Fh3\\\\u003E\\\\u003Cp\\\\u003E怎么做呢？首先，我们可以从美团点评，高德地图，搜房网等渠道爬取公开的商业数据，就是日常使用APP时能看到的那些数据：\\\\u003Cb\\\\u003E什么地方，什么店，卖什么东西，价格怎么样，口碑怎么样，人气怎么样等\\\\u003C\\\\u002Fb\\\\u003E。这些数据应该都是基于经纬度的，不能直接使用，需要整合。\\\\u003Cb\\\\u003E怎么整合这些基于经纬度的数据？这就涉及构造特征。\\\\u003C\\\\u002Fb\\\\u003E接下来，我们（主观）分析一下影响奶茶店营业额的因素。\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Col\\\\u003E\\\\u003Cli\\\\u003E\\\\u003Cb\\\\u003E周边人群密度。\\\\u003C\\\\u002Fb\\\\u003E哪些因素可以刻画人群密度呢？比如周围2km（假设）的餐馆数量，商场数量，超市数量等。人多了，吃饭的需求就多了，表现在餐馆数量多。所以，餐馆多的地方，人应该也多。我们可以把周围的餐馆数量加起来作为一个人群密度指标。假设你们家的奶茶走的是网红高端路线，那么每个餐馆的权重是不一样的。比如附近有一家海底捞肯定好于炸酱面（主观上）。这个时候就应该给餐馆加权（soft threshold），或者筛选出一部分餐馆（hard threshold）。\\\\u003C\\\\u002Fli\\\\u003E\\\\u003Cli\\\\u003E\\\\u003Cb\\\\u003E消费水平。\\\\u003C\\\\u002Fb\\\\u003E有哪些因素可以刻画消费水平呢？美团点评上有一大堆消费数据可以使用。周围一些高端品牌的数量也可以反应一个区域的消费水平。高、中、低三类商场的数量及比例。这些指标都可能刻画你们家奶茶店周边人群的消费水平。\\\\u003C\\\\u002Fli\\\\u003E\\\\u003Cli\\\\u003E\\\\u003Cb\\\\u003E人群质量。\\\\u003C\\\\u002Fb\\\\u003E附近有没有学校？有没有电影院？小区的建筑年代？举个例子，既然学生爱喝奶茶，那学校就是很好的目标客群。如果在商场里，看电影之前也可能就近买杯奶茶。此外，建筑年代可以将小区分为高龄、中龄和新小区，三类小区的比例可以大致刻画人群的年龄分布。年轻人比例越高越好啊，毕竟奶茶的主力消费群体还是年轻人。\\\\u003C\\\\u002Fli\\\\u003E\\\\u003C\\\\u002Fol\\\\u003E\\\\u003Cp\\\\u003E这样的特征还可以构造很多。可以看到，这是一个主观的逐渐深入分析的过程。构建的特征是人思考问题后的量化表达。至于提出的指标有没有用，还需要后续验证。这些指标构成的指标库是后续机器学习的基础。\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Ch3\\\\u003E一些思考\\\\u003C\\\\u002Fh3\\\\u003E\\\\u003Cp\\\\u003E\\\\u003Cb\\\\u003E有了神经网络还需要人工特征工程么？\\\\u003C\\\\u002Fb\\\\u003E神经网络在大部分时候可以节省人工构建特征的时间，但它并不是万能的，仍然依赖于一个基础的特征库。此外，虽然复杂的神经网络在理论上能逼近任何连续函数，但好的特征能够有效减少模型的复杂度。\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Cp\\\\u003E上面是一个比较极端的例子，我们需要从无到有构建特征库再建模。大多数情况下，我们手头已经有很多特征了，管它三七二十一，先跑一个随机森林试试看？\\\\u003Cb\\\\u003E如果特征中有大量噪声，会发现直接上模型的效果并不好。这时需要筛选特征，也就是变量选择。\\\\u003C\\\\u002Fb\\\\u003E下面两部分内容会重点介绍。\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Cp\\\\u003E下图展示了通过随机森林训练模型时，加入的噪声维度对模型泛化误差的影响。\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Cfigure data-size=\\\\\"normal\\\\\"\\\\u003E\\\\u003Cnoscript\\\\u003E\\\\u003Cimg src=\\\\\"https:\\\\u002F\\\\u002Fpic4.zhimg.com\\\\u002F50\\\\u002Fv2-f19189005c58088c28506fc013103235_hd.jpg\\\\\" data-caption=\\\\\"\\\\\" data-size=\\\\\"normal\\\\\" data-rawwidth=\\\\\"1800\\\\\" data-rawheight=\\\\\"1050\\\\\" data-default-watermark-src=\\\\\"https:\\\\u002F\\\\u002Fpic3.zhimg.com\\\\u002F50\\\\u002Fv2-838152ffd69806a0ed8d06d2fdd5b568_hd.jpg\\\\\" class=\\\\\"origin_image zh-lightbox-thumb\\\\\" width=\\\\\"1800\\\\\" data-original=\\\\\"https:\\\\u002F\\\\u002Fpic4.zhimg.com\\\\u002Fv2-f19189005c58088c28506fc013103235_r.jpg\\\\\"\\\\u002F\\\\u003E\\\\u003C\\\\u002Fnoscript\\\\u003E\\\\u003Cimg src=\\\\\"data:image\\\\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\\\\u002F\\\\u002Fwww.w3.org\\\\u002F2000\\\\u002Fsvg&#39; width=&#39;1800&#39; height=&#39;1050&#39;&gt;&lt;\\\\u002Fsvg&gt;\\\\\" data-caption=\\\\\"\\\\\" data-size=\\\\\"normal\\\\\" data-rawwidth=\\\\\"1800\\\\\" data-rawheight=\\\\\"1050\\\\\" data-default-watermark-src=\\\\\"https:\\\\u002F\\\\u002Fpic3.zhimg.com\\\\u002F50\\\\u002Fv2-838152ffd69806a0ed8d06d2fdd5b568_hd.jpg\\\\\" class=\\\\\"origin_image zh-lightbox-thumb lazy\\\\\" width=\\\\\"1800\\\\\" data-original=\\\\\"https:\\\\u002F\\\\u002Fpic4.zhimg.com\\\\u002Fv2-f19189005c58088c28506fc013103235_r.jpg\\\\\" data-actualsrc=\\\\\"https:\\\\u002F\\\\u002Fpic4.zhimg.com\\\\u002F50\\\\u002Fv2-f19189005c58088c28506fc013103235_hd.jpg\\\\\"\\\\u002F\\\\u003E\\\\u003C\\\\u002Ffigure\\\\u003E\\\\u003Cp\\\\u003E可以看到，噪声越多，模型越差。\\\\u003Cb\\\\u003E如果先筛选特征，也就是先降噪，再训练模型。会不会提高模型的精度？\\\\u003C\\\\u002Fb\\\\u003E答案是肯定的，下面我们从实验结果和理论两个角度展开分析。\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Ch2\\\\u003E二、变量选择\\\\u003C\\\\u002Fh2\\\\u003E\\\\u003Cp\\\\u003E这一部分转自我的博客 \\\\u003Ca href=\\\\\"https:\\\\u002F\\\\u002Flink.zhihu.com\\\\u002F?target=https%3A\\\\u002F\\\\u002Fblog.csdn.net\\\\u002Fzhanshirj\\\\u002Farticle\\\\u002Fdetails\\\\u002F88087410\\\\\" class=\\\\\" wrap external\\\\\" target=\\\\\"_blank\\\\\" rel=\\\\\"nofollow noreferrer\\\\\"\\\\u003E机器学习中的变量选择——进阶篇\\\\u003C\\\\u002Fa\\\\u003E\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Cp\\\\u003E\\\\u003Cb\\\\u003E符号说明：\\\\u003C\\\\u002Fb\\\\u003E\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Cp\\\\u003Ep:特征数量\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Cp\\\\u003En:样本数量\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Cp\\\\u003E变量选择在机器学习中扮演着重要的角色，无论是对于构建一个可解释的模型，还是提升模型的预测能力。\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Ch3\\\\u003E2.1 单变量筛选\\\\u003C\\\\u002Fh3\\\\u003E\\\\u003Cp\\\\u003E在高维情况下，有时候我们需要预先筛选部分变量，然后再训练模型。筛选过程需要做到如下两点：\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Cp\\\\u003E1.  计算复杂度不能太高\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Cp\\\\u003E2. 不能丢掉真正起作用的变量\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Cp\\\\u003E简言之，就是\\\\u003Cb\\\\u003E快而准\\\\u003C\\\\u002Fb\\\\u003E。第二点也被称之为 \\\\u003Cb\\\\u003ESure Screening Property\\\\u003C\\\\u002Fb\\\\u003E，即在一些条件下满足：\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Cp\\\\u003E\\\\u003Cimg src=\\\\\"https:\\\\u002F\\\\u002Fwww.zhihu.com\\\\u002Fequation?tex=P%28M_%5Cstar+%5Csubset+%5Chat%7BM%7D_%7Bv_n%7D%29+%5Crightarrow+1\\\\\" alt=\\\\\"P(M_\\\\\\\\star \\\\\\\\subset \\\\\\\\hat{M}_{v_n}) \\\\\\\\rightarrow 1\\\\\" eeimg=\\\\\"1\\\\\"\\\\u002F\\\\u003E\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Cp\\\\u003E其中， \\\\u003Cimg src=\\\\\"https:\\\\u002F\\\\u002Fwww.zhihu.com\\\\u002Fequation?tex=M_%5Cstar\\\\\" alt=\\\\\"M_\\\\\\\\star\\\\\" eeimg=\\\\\"1\\\\\"\\\\u002F\\\\u003E 是真正起作用的变量集（理论上的）， \\\\u003Cimg src=\\\\\"https:\\\\u002F\\\\u002Fwww.zhihu.com\\\\u002Fequation?tex=%5Chat%7BM%7D_%7Bv_n%7D\\\\\" alt=\\\\\"\\\\\\\\hat{M}_{v_n}\\\\\" eeimg=\\\\\"1\\\\\"\\\\u002F\\\\u003E 是筛选出来的变量集。最直接的筛选方式是计算每个特征和目标变量之间的相关性，并保留相关性高于某阈值的特征。最常见的相关性是\\\\u003Cb\\\\u003E皮尔森相关系数\\\\u003C\\\\u002Fb\\\\u003E，但它只能刻画两个变量之间的线性关系。如果特征变量和目标变量之间是二次相关，皮尔森相关系数就难以胜任了。\\\\u003Cb\\\\u003E秩相关系数\\\\u003C\\\\u002Fb\\\\u003E能够较好的刻画出单调的非线性关系。另外，还有基于\\\\u003Cb\\\\u003E互信息\\\\u003C\\\\u002Fb\\\\u003E的变量筛选方式，这常出现在分类模型中。除此之外，还可以用单个特征来拟合目标变量，残差越小，这个特征越重要。如果用线性模型来拟合，此时X的维度为1，\\\\u003Cb\\\\u003E它和皮尔森相关系数是等价的\\\\u003C\\\\u002Fb\\\\u003E；如果是用非参数模型，比如样条函数，局部线性模型等，通常能够较好刻画出单个特征和目标变量之间的非线性相关关系。但是非参数模型的计算量比计算相关性大很多，且结果的好坏依赖于模型的选取(model dependent)。\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Cp\\\\u003E需要注意，\\\\u003Cb\\\\u003E单变量筛选没有考虑到特征之间的交互作用，不宜将变量选得太少\\\\u003C\\\\u002Fb\\\\u003E。\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Cp\\\\u003E例如，下面的模型中， \\\\u003Cimg src=\\\\\"https:\\\\u002F\\\\u002Fwww.zhihu.com\\\\u002Fequation?tex=y\\\\\" alt=\\\\\"y\\\\\" eeimg=\\\\\"1\\\\\"\\\\u002F\\\\u003E 和 \\\\u003Cimg src=\\\\\"https:\\\\u002F\\\\u002Fwww.zhihu.com\\\\u002Fequation?tex=x_1\\\\\" alt=\\\\\"x_1\\\\\" eeimg=\\\\\"1\\\\\"\\\\u002F\\\\u003E 的相关性为0.7， \\\\u003Cimg src=\\\\\"https:\\\\u002F\\\\u002Fwww.zhihu.com\\\\u002Fequation?tex=y\\\\\" alt=\\\\\"y\\\\\" eeimg=\\\\\"1\\\\\"\\\\u002F\\\\u003E 和 \\\\u003Cimg src=\\\\\"https:\\\\u002F\\\\u002Fwww.zhihu.com\\\\u002Fequation?tex=x_2\\\\\" alt=\\\\\"x_2\\\\\" eeimg=\\\\\"1\\\\\"\\\\u002F\\\\u003E 的相关性为0.4， \\\\u003Cimg src=\\\\\"https:\\\\u002F\\\\u002Fwww.zhihu.com\\\\u002Fequation?tex=x_1\\\\\" alt=\\\\\"x_1\\\\\" eeimg=\\\\\"1\\\\\"\\\\u002F\\\\u003E 和 \\\\u003Cimg src=\\\\\"https:\\\\u002F\\\\u002Fwww.zhihu.com\\\\u002Fequation?tex=x_2\\\\\" alt=\\\\\"x_2\\\\\" eeimg=\\\\\"1\\\\\"\\\\u002F\\\\u003E 相关性很小。通过 \\\\u003Cimg src=\\\\\"https:\\\\u002F\\\\u002Fwww.zhihu.com\\\\u002Fequation?tex=+x_1\\\\\" alt=\\\\\" x_1\\\\\" eeimg=\\\\\"1\\\\\"\\\\u002F\\\\u003E 构造一个变量 \\\\u003Cimg src=\\\\\"https:\\\\u002F\\\\u002Fwww.zhihu.com\\\\u002Fequation?tex=x_3\\\\\" alt=\\\\\"x_3\\\\\" eeimg=\\\\\"1\\\\\"\\\\u002F\\\\u003E ，使得 \\\\u003Cimg src=\\\\\"https:\\\\u002F\\\\u002Fwww.zhihu.com\\\\u002Fequation?tex=x_3\\\\\" alt=\\\\\"x_3\\\\\" eeimg=\\\\\"1\\\\\"\\\\u002F\\\\u003E 和 \\\\u003Cimg src=\\\\\"https:\\\\u002F\\\\u002Fwww.zhihu.com\\\\u002Fequation?tex=y\\\\\" alt=\\\\\"y\\\\\" eeimg=\\\\\"1\\\\\"\\\\u002F\\\\u003E 的相关性达到0.6。现在令 \\\\u003Cimg src=\\\\\"https:\\\\u002F\\\\u002Fwww.zhihu.com\\\\u002Fequation?tex=X%3D%28x_1%2C+x_2%2C+x_3%29\\\\\" alt=\\\\\"X=(x_1, x_2, x_3)\\\\\" eeimg=\\\\\"1\\\\\"\\\\u002F\\\\u003E , 如果通过上述单变量方法筛选2个变量，那么 \\\\u003Cimg src=\\\\\"https:\\\\u002F\\\\u002Fwww.zhihu.com\\\\u002Fequation?tex=x_1%EF%BC%8Cx_3\\\\\" alt=\\\\\"x_1，x_3\\\\\" eeimg=\\\\\"1\\\\\"\\\\u002F\\\\u003E 被选中， \\\\u003Cimg src=\\\\\"https:\\\\u002F\\\\u002Fwww.zhihu.com\\\\u002Fequation?tex=x_2\\\\\" alt=\\\\\"x_2\\\\\" eeimg=\\\\\"1\\\\\"\\\\u002F\\\\u003E 不会被选中。然而， \\\\u003Cimg src=\\\\\"https:\\\\u002F\\\\u002Fwww.zhihu.com\\\\u002Fequation?tex=x_3\\\\\" alt=\\\\\"x_3\\\\\" eeimg=\\\\\"1\\\\\"\\\\u002F\\\\u003E 包含的关于 \\\\u003Cimg src=\\\\\"https:\\\\u002F\\\\u002Fwww.zhihu.com\\\\u002Fequation?tex=y\\\\\" alt=\\\\\"y\\\\\" eeimg=\\\\\"1\\\\\"\\\\u002F\\\\u003E 的所有信息都在来自 \\\\u003Cimg src=\\\\\"https:\\\\u002F\\\\u002Fwww.zhihu.com\\\\u002Fequation?tex=x_1\\\\\" alt=\\\\\"x_1\\\\\" eeimg=\\\\\"1\\\\\"\\\\u002F\\\\u003E 。换句话说，在考虑 \\\\u003Cimg src=\\\\\"https:\\\\u002F\\\\u002Fwww.zhihu.com\\\\u002Fequation?tex=x_1\\\\\" alt=\\\\\"x_1\\\\\" eeimg=\\\\\"1\\\\\"\\\\u002F\\\\u003E 的条件下， \\\\u003Cimg src=\\\\\"https:\\\\u002F\\\\u002Fwww.zhihu.com\\\\u002Fequation?tex=x_3\\\\\" alt=\\\\\"x_3\\\\\" eeimg=\\\\\"1\\\\\"\\\\u002F\\\\u003E 对预测 \\\\u003Cimg src=\\\\\"https:\\\\u002F\\\\u002Fwww.zhihu.com\\\\u002Fequation?tex=y\\\\\" alt=\\\\\"y\\\\\" eeimg=\\\\\"1\\\\\"\\\\u002F\\\\u003E 没有任何帮助， \\\\u003Cimg src=\\\\\"https:\\\\u002F\\\\u002Fwww.zhihu.com\\\\u002Fequation?tex=x_2\\\\\" alt=\\\\\"x_2\\\\\" eeimg=\\\\\"1\\\\\"\\\\u002F\\\\u003E 才是那个应该被选中的变量。\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Cp\\\\u003E\\\\u003Cimg src=\\\\\"https:\\\\u002F\\\\u002Fwww.zhihu.com\\\\u002Fequation?tex=y+%3D+x_1+%2B+x_2+%2B+%5Cepsilon\\\\\" alt=\\\\\"y = x_1 + x_2 + \\\\\\\\epsilon\\\\\" eeimg=\\\\\"1\\\\\"\\\\u002F\\\\u003E\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Ch3\\\\u003E2.2 通过模型选择变量\\\\u003C\\\\u002Fh3\\\\u003E\\\\u003Cp\\\\u003E接下来，我们简单回顾下通过模型选择变量。最直观的方法是筛选\\\\u003Cb\\\\u003E最佳子集\\\\u003C\\\\u002Fb\\\\u003E，但它是一个NP hard问题。如果X有p个特征，则需要考虑 \\\\u003Cimg src=\\\\\"https:\\\\u002F\\\\u002Fwww.zhihu.com\\\\u002Fequation?tex=2%5Ep\\\\\" alt=\\\\\"2^p\\\\\" eeimg=\\\\\"1\\\\\"\\\\u002F\\\\u003E 个模型。当 \\\\u003Cimg src=\\\\\"https:\\\\u002F\\\\u002Fwww.zhihu.com\\\\u002Fequation?tex=p\\\\\" alt=\\\\\"p\\\\\" eeimg=\\\\\"1\\\\\"\\\\u002F\\\\u003E 较大时，无疑太过耗时。退而求其次，\\\\u003Cb\\\\u003E逐步回归\\\\u003C\\\\u002Fb\\\\u003E在计算量上可以接受，但其贪心的性质决定了很难找出最佳模型。\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Cp\\\\u003E\\\\u003Cb\\\\u003EAIC\\\\u003C\\\\u002Fb\\\\u003E和\\\\u003Cb\\\\u003EBIC\\\\u003C\\\\u002Fb\\\\u003E可以看做是加了惩罚项的极大似然。\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Cp\\\\u003E\\\\u003Cb\\\\u003ELASSO\\\\u003C\\\\u002Fb\\\\u003E对原始问题的回归系数加了 \\\\u003Cimg src=\\\\\"https:\\\\u002F\\\\u002Fwww.zhihu.com\\\\u002Fequation?tex=L_1\\\\\" alt=\\\\\"L_1\\\\\" eeimg=\\\\\"1\\\\\"\\\\u002F\\\\u003E 惩罚。从优化角度讲，等价于原始最优化问题的可行域被限制在一个有尖点的凸集上，这样的最优解是稀疏的。且惩罚系数越大，解越稀疏。\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Cp\\\\u003E但LASSO在理论上并不完美，对于真正起作用的变量，无法满足渐进无偏性，即估计值的绝对值偏小。后来，\\\\u003Cb\\\\u003ESCAD\\\\u003C\\\\u002Fb\\\\u003E诞生了，其满足了稀疏性，无偏性和连续性，这三条性质也被称之为ORACLE性质。此后不少变量选择方法都以满足ORACL性质为最优准则。\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Cp\\\\u003E可以看到，较之于前面的单变量筛选方法，\\\\u003Cb\\\\u003E基于模型的变量选择同时考虑了所有变量，能有效刻画模型之间的交互作用。\\\\u003C\\\\u002Fb\\\\u003E\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Ch2\\\\u003E三、变量选择——进阶\\\\u003C\\\\u002Fh2\\\\u003E\\\\u003Ch3\\\\u003E3.1 只用模型就能选好变量么\\\\u003C\\\\u002Fh3\\\\u003E\\\\u003Cp\\\\u003E\\\\u003Cb\\\\u003E如果用模型就可以筛选变量，计算速度也可以接受，那还需要单变量筛选方式么？\\\\u003C\\\\u002Fb\\\\u003E\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Cp\\\\u003E为了回答这个问题，我使用sklearn中的diabetes数据集，比较下面介绍的模型的泛化能力。\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Ch3\\\\u003E数据介绍\\\\u003C\\\\u002Fh3\\\\u003E\\\\u003Cp\\\\u003Ediabetes数据集有442个样本：自变量记为 \\\\u003Cimg src=\\\\\"https:\\\\u002F\\\\u002Fwww.zhihu.com\\\\u002Fequation?tex=X_1\\\\\" alt=\\\\\"X_1\\\\\" eeimg=\\\\\"1\\\\\"\\\\u002F\\\\u003E ，连续值，10维；目标变量记为 \\\\u003Cimg src=\\\\\"https:\\\\u002F\\\\u002Fwww.zhihu.com\\\\u002Fequation?tex=Y\\\\\" alt=\\\\\"Y\\\\\" eeimg=\\\\\"1\\\\\"\\\\u002F\\\\u003E ，连续值，1维。通过正态分布采集噪声数据 \\\\u003Cimg src=\\\\\"https:\\\\u002F\\\\u002Fwww.zhihu.com\\\\u002Fequation?tex=X_2\\\\\" alt=\\\\\"X_2\\\\\" eeimg=\\\\\"1\\\\\"\\\\u002F\\\\u003E ，维度为 \\\\u003Cimg src=\\\\\"https:\\\\u002F\\\\u002Fwww.zhihu.com\\\\u002Fequation?tex=p_2\\\\\" alt=\\\\\"p_2\\\\\" eeimg=\\\\\"1\\\\\"\\\\u002F\\\\u003E 。 \\\\u003Cimg src=\\\\\"https:\\\\u002F\\\\u002Fwww.zhihu.com\\\\u002Fequation?tex=X_1\\\\\" alt=\\\\\"X_1\\\\\" eeimg=\\\\\"1\\\\\"\\\\u002F\\\\u003E 和 \\\\u003Cimg src=\\\\\"https:\\\\u002F\\\\u002Fwww.zhihu.com\\\\u002Fequation?tex=X_2\\\\\" alt=\\\\\"X_2\\\\\" eeimg=\\\\\"1\\\\\"\\\\u002F\\\\u003E 组成新的变量 \\\\u003Cimg src=\\\\\"https:\\\\u002F\\\\u002Fwww.zhihu.com\\\\u002Fequation?tex=X\\\\\" alt=\\\\\"X\\\\\" eeimg=\\\\\"1\\\\\"\\\\u002F\\\\u003E ，共 \\\\u003Cimg src=\\\\\"https:\\\\u002F\\\\u002Fwww.zhihu.com\\\\u002Fequation?tex=10%2Bp_2+\\\\\" alt=\\\\\"10+p_2 \\\\\" eeimg=\\\\\"1\\\\\"\\\\u002F\\\\u003E 个特征。考虑噪声维度：10, 50, 100, 500, 1000, 2000。\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Ch3\\\\u003E模型介绍\\\\u003C\\\\u002Fh3\\\\u003E\\\\u003Cp\\\\u003E通过LASSO选择变量时，考虑惩罚项系数：0.01, 0.05, 0.1, 0.5, 1, 5, 10, 100。根据5重交叉验证选择最佳惩罚项系数，并测试模型的泛化能力。\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Cp\\\\u003E模型一：直接用LASSO训练模型。\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Cp\\\\u003E模型二：先用单变量方法筛选一半变量，再用LASSO训练模型。\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Cp\\\\u003E模型三：先用单变量方法筛选20个变量，再用LASSO训练模型。\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Cp\\\\u003E\\\\u003Cb\\\\u003E需要特别说明：\\\\u003C\\\\u002Fb\\\\u003E模型二和模型三实验中，不能先在整个数据集上用单变量方法筛选变量，再用交叉验证选取超参alpha，计算测试误差，这属于对交叉验证的误用。\\\\u003Cb\\\\u003E单变量筛选需要和选取超参同时出现在训练集，而不能出现在测试集。\\\\u003C\\\\u002Fb\\\\u003E\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Ch3\\\\u003E实验结果\\\\u003C\\\\u002Fh3\\\\u003E\\\\u003Cfigure data-size=\\\\\"normal\\\\\"\\\\u003E\\\\u003Cnoscript\\\\u003E\\\\u003Cimg src=\\\\\"https:\\\\u002F\\\\u002Fpic3.zhimg.com\\\\u002F50\\\\u002Fv2-502dabf62ddc394169d910702ae66ace_hd.jpg\\\\\" data-caption=\\\\\"\\\\\" data-size=\\\\\"normal\\\\\" data-rawwidth=\\\\\"1800\\\\\" data-rawheight=\\\\\"1050\\\\\" data-default-watermark-src=\\\\\"https:\\\\u002F\\\\u002Fpic2.zhimg.com\\\\u002F50\\\\u002Fv2-5ad40458c49632047a416d52074a1cb8_hd.jpg\\\\\" class=\\\\\"origin_image zh-lightbox-thumb\\\\\" width=\\\\\"1800\\\\\" data-original=\\\\\"https:\\\\u002F\\\\u002Fpic1.zhimg.com\\\\u002Fv2-502dabf62ddc394169d910702ae66ace_r.jpg\\\\\"\\\\u002F\\\\u003E\\\\u003C\\\\u002Fnoscript\\\\u003E\\\\u003Cimg src=\\\\\"data:image\\\\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\\\\u002F\\\\u002Fwww.w3.org\\\\u002F2000\\\\u002Fsvg&#39; width=&#39;1800&#39; height=&#39;1050&#39;&gt;&lt;\\\\u002Fsvg&gt;\\\\\" data-caption=\\\\\"\\\\\" data-size=\\\\\"normal\\\\\" data-rawwidth=\\\\\"1800\\\\\" data-rawheight=\\\\\"1050\\\\\" data-default-watermark-src=\\\\\"https:\\\\u002F\\\\u002Fpic2.zhimg.com\\\\u002F50\\\\u002Fv2-5ad40458c49632047a416d52074a1cb8_hd.jpg\\\\\" class=\\\\\"origin_image zh-lightbox-thumb lazy\\\\\" width=\\\\\"1800\\\\\" data-original=\\\\\"https:\\\\u002F\\\\u002Fpic1.zhimg.com\\\\u002Fv2-502dabf62ddc394169d910702ae66ace_r.jpg\\\\\" data-actualsrc=\\\\\"https:\\\\u002F\\\\u002Fpic3.zhimg.com\\\\u002F50\\\\u002Fv2-502dabf62ddc394169d910702ae66ace_hd.jpg\\\\\"\\\\u002F\\\\u003E\\\\u003C\\\\u002Ffigure\\\\u003E\\\\u003Cp\\\\u003E可以看到，当噪声维度比较小时，先筛选一半变量或20个变量再训练LASSO模型，显著优于直接用LASSO训练模型。当噪声维度较大时，先筛选20个变量再训练LASSO模型，显著优于另外两种方法。\\\\u003Cb\\\\u003E这充分说明先筛选变量，即降噪再训练模型，能显著提高模型的泛化能力。\\\\u003C\\\\u002Fb\\\\u003E下面通另一个角度——\\\\u003Cb\\\\u003E伪相关\\\\u003C\\\\u002Fb\\\\u003E，再次认识预筛选变量。\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Ch3\\\\u003E3.2 认识伪相关\\\\u003C\\\\u002Fh3\\\\u003E\\\\u003Cp\\\\u003E在一般情况下 \\\\u003Cimg src=\\\\\"https:\\\\u002F\\\\u002Fwww.zhihu.com\\\\u002Fequation?tex=%28n%3Ep%29\\\\\" alt=\\\\\"(n&gt;p)\\\\\" eeimg=\\\\\"1\\\\\"\\\\u002F\\\\u003E ，通过模型选择变量就足够了。\\\\u003Cb\\\\u003E但在超高维情况下，会出现伪相关问题。导致选入额外的变量，模型过拟合并低估方差。\\\\u003C\\\\u002Fb\\\\u003E\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Cp\\\\u003E先直观认识伪相关。假定 \\\\u003Cimg src=\\\\\"https:\\\\u002F\\\\u002Fwww.zhihu.com\\\\u002Fequation?tex=y%2C+x_1%2C+...%2C+x_p\\\\\" alt=\\\\\"y, x_1, ..., x_p\\\\\" eeimg=\\\\\"1\\\\\"\\\\u002F\\\\u003E 相互独立，且服从标准正态分布。根据如下步骤产生数据模拟：\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Cp\\\\u003E 1. \\\\u003Cimg src=\\\\\"https:\\\\u002F\\\\u002Fwww.zhihu.com\\\\u002Fequation?tex=Y+%E5%92%8C+X+%E6%98%AF+y%2C++x_1%2C+...%2C+x_p\\\\\" alt=\\\\\"Y 和 X 是 y,  x_1, ..., x_p\\\\\" eeimg=\\\\\"1\\\\\"\\\\u002F\\\\u003E 的一次样本实现，共100个样本。\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Cp\\\\u003E 2. 计算 \\\\u003Cimg src=\\\\\"https:\\\\u002F\\\\u002Fwww.zhihu.com\\\\u002Fequation?tex=X\\\\\" alt=\\\\\"X\\\\\" eeimg=\\\\\"1\\\\\"\\\\u002F\\\\u003E 的每一列与 \\\\u003Cimg src=\\\\\"https:\\\\u002F\\\\u002Fwww.zhihu.com\\\\u002Fequation?tex=Y\\\\\" alt=\\\\\"Y\\\\\" eeimg=\\\\\"1\\\\\"\\\\u002F\\\\u003E 的相关系数并取绝对值，记录 \\\\u003Cimg src=\\\\\"https:\\\\u002F\\\\u002Fwww.zhihu.com\\\\u002Fequation?tex=p\\\\\" alt=\\\\\"p\\\\\" eeimg=\\\\\"1\\\\\"\\\\u002F\\\\u003E 个绝对值的最大值。\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Cp\\\\u003E重复这个过程100次，绘制最大值（ \\\\u003Cimg src=\\\\\"https:\\\\u002F\\\\u002Fwww.zhihu.com\\\\u002Fequation?tex=p\\\\\" alt=\\\\\"p\\\\\" eeimg=\\\\\"1\\\\\"\\\\u002F\\\\u003E 个相关系数绝对值的最大值）的密度函数。下图展示了密度函数曲线与 \\\\u003Cimg src=\\\\\"https:\\\\u002F\\\\u002Fwww.zhihu.com\\\\u002Fequation?tex=p\\\\\" alt=\\\\\"p\\\\\" eeimg=\\\\\"1\\\\\"\\\\u002F\\\\u003E 的关系。\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Cfigure data-size=\\\\\"normal\\\\\"\\\\u003E\\\\u003Cnoscript\\\\u003E\\\\u003Cimg src=\\\\\"https:\\\\u002F\\\\u002Fpic1.zhimg.com\\\\u002F50\\\\u002Fv2-ecac432b38bd935ff2cf0481227e4961_hd.jpg\\\\\" data-caption=\\\\\"\\\\\" data-size=\\\\\"normal\\\\\" data-rawwidth=\\\\\"1200\\\\\" data-rawheight=\\\\\"800\\\\\" data-default-watermark-src=\\\\\"https:\\\\u002F\\\\u002Fpic1.zhimg.com\\\\u002F50\\\\u002Fv2-a0284ab54cbdecfc9a384f01622f45b9_hd.jpg\\\\\" class=\\\\\"origin_image zh-lightbox-thumb\\\\\" width=\\\\\"1200\\\\\" data-original=\\\\\"https:\\\\u002F\\\\u002Fpic2.zhimg.com\\\\u002Fv2-ecac432b38bd935ff2cf0481227e4961_r.jpg\\\\\"\\\\u002F\\\\u003E\\\\u003C\\\\u002Fnoscript\\\\u003E\\\\u003Cimg src=\\\\\"data:image\\\\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\\\\u002F\\\\u002Fwww.w3.org\\\\u002F2000\\\\u002Fsvg&#39; width=&#39;1200&#39; height=&#39;800&#39;&gt;&lt;\\\\u002Fsvg&gt;\\\\\" data-caption=\\\\\"\\\\\" data-size=\\\\\"normal\\\\\" data-rawwidth=\\\\\"1200\\\\\" data-rawheight=\\\\\"800\\\\\" data-default-watermark-src=\\\\\"https:\\\\u002F\\\\u002Fpic1.zhimg.com\\\\u002F50\\\\u002Fv2-a0284ab54cbdecfc9a384f01622f45b9_hd.jpg\\\\\" class=\\\\\"origin_image zh-lightbox-thumb lazy\\\\\" width=\\\\\"1200\\\\\" data-original=\\\\\"https:\\\\u002F\\\\u002Fpic2.zhimg.com\\\\u002Fv2-ecac432b38bd935ff2cf0481227e4961_r.jpg\\\\\" data-actualsrc=\\\\\"https:\\\\u002F\\\\u002Fpic1.zhimg.com\\\\u002F50\\\\u002Fv2-ecac432b38bd935ff2cf0481227e4961_hd.jpg\\\\\"\\\\u002F\\\\u003E\\\\u003C\\\\u002Ffigure\\\\u003E\\\\u003Cp\\\\u003E可以看到：\\\\u003Cb\\\\u003E随着\\\\u003C\\\\u002Fb\\\\u003E \\\\u003Cimg src=\\\\\"https:\\\\u002F\\\\u002Fwww.zhihu.com\\\\u002Fequation?tex=p\\\\\" alt=\\\\\"p\\\\\" eeimg=\\\\\"1\\\\\"\\\\u002F\\\\u003E\\\\u003Cb\\\\u003E的增加，最大值的密度函数明显右移，变得越来越大。\\\\u003C\\\\u002Fb\\\\u003E但实际上X和Y是从相互独立的变量中采样得到。在超高维情况下 \\\\u003Cimg src=\\\\\"https:\\\\u002F\\\\u002Fwww.zhihu.com\\\\u002Fequation?tex=%EF%BC%88p%3E%3En%EF%BC%89\\\\\" alt=\\\\\"（p&gt;&gt;n）\\\\\" eeimg=\\\\\"1\\\\\"\\\\u002F\\\\u003E ，这种现象尤其明显。这会导致选入无关变量，模型过拟合，且低估模型的方差。模型的方差在构建置信区间时尤为重要，较小的方差会导致过于乐观的置信区间。Fan, Guo等提出了两步法来解决超高维情况下的方差估计。我根据自己的理解，结合变量选择和方差估计，改述如下：\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Ch3\\\\u003E3.3 两步法估计\\\\u003C\\\\u002Fh3\\\\u003E\\\\u003Cp\\\\u003E将 \\\\u003Cimg src=\\\\\"https:\\\\u002F\\\\u002Fwww.zhihu.com\\\\u002Fequation?tex=X%2C+Y\\\\\" alt=\\\\\"X, Y\\\\\" eeimg=\\\\\"1\\\\\"\\\\u002F\\\\u003E 均匀的划分为两部分， \\\\u003Cimg src=\\\\\"https:\\\\u002F\\\\u002Fwww.zhihu.com\\\\u002Fequation?tex=X%5E1%2C+Y%5E1+%E5%92%8C+X%5E2%2C+Y%5E2%E3%80%82\\\\\" alt=\\\\\"X^1, Y^1 和 X^2, Y^2。\\\\\" eeimg=\\\\\"1\\\\\"\\\\u002F\\\\u003E\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Cp\\\\u003E\\\\u003Cb\\\\u003Estep 1\\\\u003C\\\\u002Fb\\\\u003E：通过单变量筛选，在 \\\\u003Cimg src=\\\\\"https:\\\\u002F\\\\u002Fwww.zhihu.com\\\\u002Fequation?tex=X%5E1%2C+Y%5E1\\\\\" alt=\\\\\"X^1, Y^1\\\\\" eeimg=\\\\\"1\\\\\"\\\\u002F\\\\u003E 上筛选出变量子集 \\\\u003Cimg src=\\\\\"https:\\\\u002F\\\\u002Fwww.zhihu.com\\\\u002Fequation?tex=%5Chat%7BM_1%7D\\\\\" alt=\\\\\"\\\\\\\\hat{M_1}\\\\\" eeimg=\\\\\"1\\\\\"\\\\u002F\\\\u003E ，在 \\\\u003Cimg src=\\\\\"https:\\\\u002F\\\\u002Fwww.zhihu.com\\\\u002Fequation?tex=X%5E2%2C+Y%5E2\\\\\" alt=\\\\\"X^2, Y^2\\\\\" eeimg=\\\\\"1\\\\\"\\\\u002F\\\\u003E 上筛选出变量子集 \\\\u003Cimg src=\\\\\"https:\\\\u002F\\\\u002Fwww.zhihu.com\\\\u002Fequation?tex=%5Chat%7BM_2%7D\\\\\" alt=\\\\\"\\\\\\\\hat{M_2}\\\\\" eeimg=\\\\\"1\\\\\"\\\\u002F\\\\u003E 。\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Cp\\\\u003E\\\\u003Cb\\\\u003Estep 2\\\\u003C\\\\u002Fb\\\\u003E：在 \\\\u003Cimg src=\\\\\"https:\\\\u002F\\\\u002Fwww.zhihu.com\\\\u002Fequation?tex=X%5E1_%7B%5Chat%7BM_2%7D%7D%2C+Y%5E1\\\\\" alt=\\\\\"X^1_{\\\\\\\\hat{M_2}}, Y^1\\\\\" eeimg=\\\\\"1\\\\\"\\\\u002F\\\\u003E 上用LASSO等再次拟合模型，记筛选后的变量为 \\\\u003Cimg src=\\\\\"https:\\\\u002F\\\\u002Fwww.zhihu.com\\\\u002Fequation?tex=%5Chat%7BM_3%7D\\\\\" alt=\\\\\"\\\\\\\\hat{M_3}\\\\\" eeimg=\\\\\"1\\\\\"\\\\u002F\\\\u003E , 估计的方差为 \\\\u003Cimg src=\\\\\"https:\\\\u002F\\\\u002Fwww.zhihu.com\\\\u002Fequation?tex=%5Chat%7B%5Csigma%7D%5E2_1\\\\\" alt=\\\\\"\\\\\\\\hat{\\\\\\\\sigma}^2_1\\\\\" eeimg=\\\\\"1\\\\\"\\\\u002F\\\\u003E 。在 \\\\u003Cimg src=\\\\\"https:\\\\u002F\\\\u002Fwww.zhihu.com\\\\u002Fequation?tex=X%5E2_%7B%5Chat%7BM_1%7D%7D%2C+Y%5E2\\\\\" alt=\\\\\"X^2_{\\\\\\\\hat{M_1}}, Y^2\\\\\" eeimg=\\\\\"1\\\\\"\\\\u002F\\\\u003E 上用LASSO等再次拟合模型，记筛选后的变量为 \\\\u003Cimg src=\\\\\"https:\\\\u002F\\\\u002Fwww.zhihu.com\\\\u002Fequation?tex=%5Chat%7BM_4%7D\\\\\" alt=\\\\\"\\\\\\\\hat{M_4}\\\\\" eeimg=\\\\\"1\\\\\"\\\\u002F\\\\u003E , 估计的方差为 \\\\u003Cimg src=\\\\\"https:\\\\u002F\\\\u002Fwww.zhihu.com\\\\u002Fequation?tex=%5Chat%7B%5Csigma%7D%5E2_2\\\\\" alt=\\\\\"\\\\\\\\hat{\\\\\\\\sigma}^2_2\\\\\" eeimg=\\\\\"1\\\\\"\\\\u002F\\\\u003E 。\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Cp\\\\u003E最后筛选的变量为: \\\\u003Cimg src=\\\\\"https:\\\\u002F\\\\u002Fwww.zhihu.com\\\\u002Fequation?tex=%5Chat%7BM%7D+%3D+%5Chat%7BM_3%7D+%5Cbigcap+%5Chat%7BM_4%7D\\\\\" alt=\\\\\"\\\\\\\\hat{M} = \\\\\\\\hat{M_3} \\\\\\\\bigcap \\\\\\\\hat{M_4}\\\\\" eeimg=\\\\\"1\\\\\"\\\\u002F\\\\u003E\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Cp\\\\u003E方差估计为： \\\\u003Cimg src=\\\\\"https:\\\\u002F\\\\u002Fwww.zhihu.com\\\\u002Fequation?tex=%5Chat%7B%5Csigma%7D_%7BRCV%7D%5E2%3D%28%5Chat%7B%5Csigma%7D_1%5E2%2B%5Chat%7B%5Csigma%7D_2%5E2%29%2F2\\\\\" alt=\\\\\"\\\\\\\\hat{\\\\\\\\sigma}_{RCV}^2=(\\\\\\\\hat{\\\\\\\\sigma}_1^2+\\\\\\\\hat{\\\\\\\\sigma}_2^2)\\\\u002F2\\\\\" eeimg=\\\\\"1\\\\\"\\\\u002F\\\\u003E\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Cp\\\\u003E实现方法虽简单，但却很有用。第一步中可能选入伪相关的变量，第二步使用第一步筛选出来的变量在新的数据集上拟合模型，能够大概率排除伪相关的变量。从而更准确的选择出重要的变量，估计模型的方差。\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Cp class=\\\\\"ztext-empty-paragraph\\\\\"\\\\u003E\\\\u003Cbr\\\\u002F\\\\u003E\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Cp\\\\u003E\\\\u003Cb\\\\u003E附录：\\\\u003C\\\\u002Fb\\\\u003E\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Cp\\\\u003E代码及结果 \\\\u003Ca href=\\\\\"https:\\\\u002F\\\\u002Flink.zhihu.com\\\\u002F?target=https%3A\\\\u002F\\\\u002Fgithub.com\\\\u002FJ11235\\\\u002Fmachine-learning-pro\\\\u002Ftree\\\\u002Fmaster\\\\u002F%25E5%258F%2598%25E9%2587%258F%25E7%25AD%259B%25E9%2580%2589%25E5%2592%258C%25E4%25BC%25AA%25E7%259B%25B8%25E5%2585%25B3\\\\\" class=\\\\\" wrap external\\\\\" target=\\\\\"_blank\\\\\" rel=\\\\\"nofollow noreferrer\\\\\"\\\\u003Egithub:J11235\\\\u003C\\\\u002Fa\\\\u003E\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Cp\\\\u003E\\\\u003Cb\\\\u003E参考资料：\\\\u003C\\\\u002Fb\\\\u003E\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Cp\\\\u003E1. Jianqing Fan and Jinchi Lv(2008) [Sure independence screening for ultrahigh dimensional feature space](\\\\u003Ca href=\\\\\"https:\\\\u002F\\\\u002Flink.zhihu.com\\\\u002F?target=http%3A\\\\u002F\\\\u002F10.12.0.10\\\\u002Fwww-bcf.usc.edu\\\\u002F~jinchilv\\\\u002Fpublications\\\\u002FJRSSB-FL08.pdf\\\\\" class=\\\\\" external\\\\\" target=\\\\\"_blank\\\\\" rel=\\\\\"nofollow noreferrer\\\\\"\\\\u003E\\\\u003Cspan class=\\\\\"invisible\\\\\"\\\\u003Ehttp:\\\\u002F\\\\u002F\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"visible\\\\\"\\\\u003E10.12.0.10\\\\u002Fwww-bcf.usc.\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"invisible\\\\\"\\\\u003Eedu\\\\u002F~jinchilv\\\\u002Fpublications\\\\u002FJRSSB-FL08.pdf\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"ellipsis\\\\\"\\\\u003E\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003C\\\\u002Fa\\\\u003E)\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Cp\\\\u003E2. Jianqing Fan and Jinchi Lv(2010) [A Selective Overview of Variable Selection in High Dimensional Feature Space](\\\\u003Ca href=\\\\\"https:\\\\u002F\\\\u002Flink.zhihu.com\\\\u002F?target=https%3A\\\\u002F\\\\u002Fwww.ncbi.nlm.nih.gov\\\\u002Fpmc\\\\u002Farticles\\\\u002FPMC3092303\\\\u002F\\\\\" class=\\\\\" wrap external\\\\\" target=\\\\\"_blank\\\\\" rel=\\\\\"nofollow noreferrer\\\\\"\\\\u003EA Selective Overview of Variable Selection in High Dimensional Feature Space\\\\u003C\\\\u002Fa\\\\u003E)\\\\u003C\\\\u002Fp\\\\u003E\\\\u003Cp\\\\u003E3. LIU JingYuan, ZHONG Wei and LI RunZe(2015) [A selective overview of feature screening for ultrahigh-dimensional data](\\\\u003Ca href=\\\\\"https:\\\\u002F\\\\u002Flink.zhihu.com\\\\u002F?target=http%3A\\\\u002F\\\\u002F10.12.0.10\\\\u002Fwww.personal.psu.edu\\\\u002Fril4\\\\u002Fresearch\\\\u002FSciChinaMath-2015-2033.pdf\\\\\" class=\\\\\" external\\\\\" target=\\\\\"_blank\\\\\" rel=\\\\\"nofollow noreferrer\\\\\"\\\\u003E\\\\u003Cspan class=\\\\\"invisible\\\\\"\\\\u003Ehttp:\\\\u002F\\\\u002F\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"visible\\\\\"\\\\u003E10.12.0.10\\\\u002Fwww.personal\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"invisible\\\\\"\\\\u003E.psu.edu\\\\u002Fril4\\\\u002Fresearch\\\\u002FSciChinaMath-2015-2033.pdf\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003Cspan class=\\\\\"ellipsis\\\\\"\\\\u003E\\\\u003C\\\\u002Fspan\\\\u003E\\\\u003C\\\\u002Fa\\\\u003E)\\\\u003C\\\\u002Fp\\\\u003E\",\"editableContent\":\"\",\"excerpt\":\"在我的理解里，特征工程可以分为两个方面： 构造特征变量选择 [图片] 下面从这两个角度来谈谈。 目录构造特征实际问题构造思路一些思考变量选择单变量筛选通过模型选择变量变量选择——进阶只用模型就能选好变量么认识伪相关两步估计法一、构造特征构造特征往往是一个经验的过程，需要对业务有深入的理解。 实际问题一个问题： 如何预测一家奶茶店的营业额，寻找影响营业额的外部因素？假设我们可以拿到奶茶店每个月的营业额，除此之外没…\",\"collapsedBy\":\"nobody\",\"collapseReason\":\"\",\"annotationAction\":null,\"markInfos\":[],\"relevantInfo\":{\"isRelevant\":false,\"relevantType\":\"\",\"relevantText\":\"\"},\"suggestEdit\":{\"reason\":\"\",\"status\":false,\"tip\":\"\",\"title\":\"\",\"unnormalDetails\":{\"status\":\"\",\"description\":\"\",\"reason\":\"\",\"reasonId\":0,\"note\":\"\"},\"url\":\"\"},\"isLabeled\":false,\"rewardInfo\":{\"canOpenReward\":false,\"isRewardable\":false,\"rewardMemberCount\":0,\"rewardTotalMoney\":0,\"tagline\":\"\"},\"relationship\":{\"isAuthor\":false,\"isAuthorized\":false,\"isNothelp\":false,\"isThanked\":false,\"isRecognized\":false,\"voting\":0,\"upvotedFollowees\":[]},\"adAnswer\":null}},\"articles\":{},\"columns\":{},\"topics\":{},\"roundtables\":{},\"favlists\":{},\"comments\":{},\"notifications\":{},\"ebooks\":{},\"activities\":{},\"feeds\":{},\"pins\":{},\"promotions\":{},\"drafts\":{},\"chats\":{},\"posts\":{},\"clubs\":{},\"clubTags\":{}},\"currentUser\":\"\",\"account\":{\"lockLevel\":{},\"unlockTicketStatus\":false,\"unlockTicket\":null,\"challenge\":[],\"errorStatus\":false,\"message\":\"\",\"isFetching\":false,\"accountInfo\":{},\"urlToken\":{\"loading\":false}},\"settings\":{\"socialBind\":null,\"inboxMsg\":null,\"notification\":{},\"email\":{},\"privacyFlag\":null,\"blockedUsers\":{\"isFetching\":false,\"paging\":{\"pageNo\":1,\"pageSize\":6},\"data\":[]},\"blockedFollowees\":{\"isFetching\":false,\"paging\":{\"pageNo\":1,\"pageSize\":6},\"data\":[]},\"ignoredTopics\":{\"isFetching\":false,\"paging\":{\"pageNo\":1,\"pageSize\":6},\"data\":[]},\"restrictedTopics\":null,\"laboratory\":{}},\"notification\":{},\"people\":{\"profileStatus\":{},\"activitiesByUser\":{},\"answersByUser\":{},\"answersSortByVotesByUser\":{},\"answersIncludedByUser\":{},\"votedAnswersByUser\":{},\"thankedAnswersByUser\":{},\"voteAnswersByUser\":{},\"thankAnswersByUser\":{},\"topicAnswersByUser\":{},\"zvideosByUser\":{},\"articlesByUser\":{},\"articlesSortByVotesByUser\":{},\"articlesIncludedByUser\":{},\"pinsByUser\":{},\"questionsByUser\":{},\"commercialQuestionsByUser\":{},\"favlistsByUser\":{},\"followingByUser\":{},\"followersByUser\":{},\"mutualsByUser\":{},\"followingColumnsByUser\":{},\"followingQuestionsByUser\":{},\"followingFavlistsByUser\":{},\"followingTopicsByUser\":{},\"publicationsByUser\":{},\"columnsByUser\":{},\"allFavlistsByUser\":{},\"brands\":null,\"creationsByUser\":{},\"creationsSortByVotesByUser\":{},\"creationsFeed\":{}},\"env\":{\"ab\":{\"config\":{\"experiments\":[{\"expId\":\"launch-qa_cl_guest-2\",\"expPrefix\":\"qa_cl_guest\",\"isDynamicallyUpdated\":true,\"isRuntime\":false,\"includeTriggerInfo\":false},{\"expId\":\"launch-us_foltopic_user-10\",\"expPrefix\":\"us_foltopic_user\",\"isDynamicallyUpdated\":true,\"isRuntime\":false,\"includeTriggerInfo\":false},{\"expId\":\"launch-vd_bullet_second-2\",\"expPrefix\":\"vd_bullet_second\",\"isDynamicallyUpdated\":true,\"isRuntime\":false,\"includeTriggerInfo\":false},{\"expId\":\"launch-vd_profile_video-11\",\"expPrefix\":\"vd_profile_video\",\"isDynamicallyUpdated\":true,\"isRuntime\":false,\"includeTriggerInfo\":false},{\"expId\":\"launch-vd_video_replay-3\",\"expPrefix\":\"vd_video_replay\",\"isDynamicallyUpdated\":true,\"isRuntime\":false,\"includeTriggerInfo\":false},{\"expId\":\"launch-vd_zvideo_link-10\",\"expPrefix\":\"vd_zvideo_link\",\"isDynamicallyUpdated\":true,\"isRuntime\":false,\"includeTriggerInfo\":false},{\"expId\":\"se_topicfeed-1\",\"expPrefix\":\"se_topicfeed\",\"isDynamicallyUpdated\":true,\"isRuntime\":false,\"includeTriggerInfo\":false},{\"expId\":\"se_sp_button-1\",\"expPrefix\":\"se_sp_button\",\"isDynamicallyUpdated\":true,\"isRuntime\":false,\"includeTriggerInfo\":false},{\"expId\":\"se_mobilecard-1\",\"expPrefix\":\"se_mobilecard\",\"isDynamicallyUpdated\":true,\"isRuntime\":false,\"includeTriggerInfo\":false}],\"params\":[{\"id\":\"web_collection_guest\",\"type\":\"String\",\"value\":\"1\"},{\"id\":\"top_root\",\"type\":\"String\",\"value\":\"0\",\"chainId\":\"_all_\"},{\"id\":\"ls_videoad\",\"type\":\"String\",\"value\":\"2\",\"chainId\":\"_all_\"},{\"id\":\"top_ydyq\",\"type\":\"String\",\"value\":\"X\",\"chainId\":\"_all_\"},{\"id\":\"li_answer_test_2\",\"type\":\"String\",\"value\":\"0\",\"chainId\":\"_all_\"},{\"id\":\"li_viptab_name\",\"type\":\"String\",\"value\":\"0\",\"chainId\":\"_all_\"},{\"id\":\"tp_club_feed\",\"type\":\"String\",\"value\":\"0\",\"chainId\":\"_all_\"},{\"id\":\"gue_zvideo_title\",\"type\":\"String\",\"value\":\"0\"},{\"id\":\"se_col_boost\",\"type\":\"String\",\"value\":\"0\",\"chainId\":\"_all_\"},{\"id\":\"tp_sft\",\"type\":\"String\",\"value\":\"a\",\"chainId\":\"_all_\"},{\"id\":\"li_catalog_card\",\"type\":\"String\",\"value\":\"1\",\"chainId\":\"_all_\"},{\"id\":\"gue_goods_card\",\"type\":\"String\",\"value\":\"0\"},{\"id\":\"se_merger_v2\",\"type\":\"String\",\"value\":\"0\",\"chainId\":\"_all_\"},{\"id\":\"web_creator_route\",\"type\":\"String\",\"value\":\"1\"},{\"id\":\"tp_score_1\",\"type\":\"String\",\"value\":\"a\",\"chainId\":\"_all_\"},{\"id\":\"top_quality\",\"type\":\"String\",\"value\":\"0\",\"chainId\":\"_all_\"},{\"id\":\"web_mweb_launch\",\"type\":\"String\",\"value\":\"1\"},{\"id\":\"web_sem_ab\",\"type\":\"String\",\"value\":\"1\"},{\"id\":\"web_answer_list_ad\",\"type\":\"String\",\"value\":\"1\"},{\"id\":\"tp_club_qa_entrance\",\"type\":\"String\",\"value\":\"0\",\"chainId\":\"_all_\"},{\"id\":\"zr_rec_answer_cp\",\"type\":\"String\",\"value\":\"open\",\"chainId\":\"_all_\"},{\"id\":\"zr_slotpaidexp\",\"type\":\"String\",\"value\":\"1\",\"chainId\":\"_all_\"},{\"id\":\"se_hotmore\",\"type\":\"String\",\"value\":\"2\",\"chainId\":\"_all_\"},{\"id\":\"se_expired_ob\",\"type\":\"String\",\"value\":\"0\",\"chainId\":\"_all_\"},{\"id\":\"tp_discover\",\"type\":\"String\",\"value\":\"0\",\"chainId\":\"_all_\"},{\"id\":\"tsp_hotlist_ui\",\"type\":\"String\",\"value\":\"1\",\"chainId\":\"_all_\"},{\"id\":\"li_answer_test\",\"type\":\"String\",\"value\":\"3\",\"chainId\":\"_all_\"},{\"id\":\"li_vip_verti_search\",\"type\":\"String\",\"value\":\"0\",\"chainId\":\"_all_\"},{\"id\":\"zw_sameq_sorce\",\"type\":\"String\",\"value\":\"999\",\"chainId\":\"_all_\"},{\"id\":\"zr_search_topic\",\"type\":\"String\",\"value\":\"0\",\"chainId\":\"_all_\"},{\"id\":\"se_billboardsearch\",\"type\":\"String\",\"value\":\"0\",\"chainId\":\"_all_\"},{\"id\":\"soc_notification\",\"type\":\"String\",\"value\":\"1\",\"chainId\":\"_all_\"},{\"id\":\"gue_visit_n_artcard\",\"type\":\"String\",\"value\":\"0\"},{\"id\":\"qap_thanks\",\"type\":\"String\",\"value\":\"1\",\"chainId\":\"_all_\"},{\"id\":\"zr_training_boost\",\"type\":\"String\",\"value\":\"false\",\"chainId\":\"_all_\"},{\"id\":\"web_answerlist_ad\",\"type\":\"String\",\"value\":\"0\"},{\"id\":\"se_hotsearch\",\"type\":\"String\",\"value\":\"0\",\"chainId\":\"_all_\"},{\"id\":\"se_adsrank\",\"type\":\"String\",\"value\":\"4\",\"chainId\":\"_all_\"},{\"id\":\"se_video_dnn\",\"type\":\"String\",\"value\":\"1\",\"chainId\":\"_all_\"},{\"id\":\"tp_meta_card\",\"type\":\"String\",\"value\":\"0\",\"chainId\":\"_all_\"},{\"id\":\"tp_topic_tab_new\",\"type\":\"String\",\"value\":\"0-0-0\",\"chainId\":\"_all_\"},{\"id\":\"gue_self_censoring\",\"type\":\"String\",\"value\":\"1\"},{\"id\":\"zr_km_answer\",\"type\":\"String\",\"value\":\"open_cvr\",\"chainId\":\"_all_\"},{\"id\":\"se_new_bert\",\"type\":\"String\",\"value\":\"0\",\"chainId\":\"_all_\"},{\"id\":\"tp_club_feedv2\",\"type\":\"String\",\"value\":\"1\",\"chainId\":\"_all_\"},{\"id\":\"ug_newtag\",\"type\":\"String\",\"value\":\"1\",\"chainId\":\"_all_\"},{\"id\":\"li_ebook_gen_search\",\"type\":\"String\",\"value\":\"0\",\"chainId\":\"_all_\"},{\"id\":\"se_backsearch\",\"type\":\"String\",\"value\":\"0\",\"chainId\":\"_all_\"},{\"id\":\"se_cardrank_3\",\"type\":\"String\",\"value\":\"0\",\"chainId\":\"_all_\"},{\"id\":\"se_v039\",\"type\":\"String\",\"value\":\"0\",\"chainId\":\"_all_\"},{\"id\":\"zr_test_aa1\",\"type\":\"String\",\"value\":\"0\",\"chainId\":\"_all_\"},{\"id\":\"se_relation_1\",\"type\":\"String\",\"value\":\"2\",\"chainId\":\"_all_\"},{\"id\":\"se_specialbutton\",\"type\":\"String\",\"value\":\"0\",\"chainId\":\"_all_\"},{\"id\":\"se_v040\",\"type\":\"String\",\"value\":\"0\",\"chainId\":\"_all_\"},{\"id\":\"pf_foltopic_usernum\",\"type\":\"String\",\"value\":\"50\",\"chainId\":\"_all_\"},{\"id\":\"gue_video_autoplay\",\"type\":\"String\",\"value\":\"0\"},{\"id\":\"li_salt_hot\",\"type\":\"String\",\"value\":\"1\",\"chainId\":\"_all_\"},{\"id\":\"se_cardrank_2\",\"type\":\"String\",\"value\":\"1\",\"chainId\":\"_all_\"},{\"id\":\"se_clubrank\",\"type\":\"String\",\"value\":\"1\",\"chainId\":\"_all_\"},{\"id\":\"gue_zvideo_55s\",\"type\":\"String\",\"value\":\"0\"},{\"id\":\"ls_vessay_trans\",\"type\":\"String\",\"value\":\"0\",\"chainId\":\"_all_\"},{\"id\":\"se_oneboxtopic\",\"type\":\"String\",\"value\":\"1\",\"chainId\":\"_all_\"},{\"id\":\"tp_header_style\",\"type\":\"String\",\"value\":\"1\",\"chainId\":\"_all_\"},{\"id\":\"top_universalebook\",\"type\":\"String\",\"value\":\"1\",\"chainId\":\"_all_\"},{\"id\":\"tp_dingyue_video\",\"type\":\"String\",\"value\":\"0\",\"chainId\":\"_all_\"},{\"id\":\"pf_creator_card\",\"type\":\"String\",\"value\":\"1\",\"chainId\":\"_all_\"},{\"id\":\"pf_newguide_vertical\",\"type\":\"String\",\"value\":\"0\",\"chainId\":\"_all_\"},{\"id\":\"li_literature\",\"type\":\"String\",\"value\":\"0\",\"chainId\":\"_all_\"},{\"id\":\"se_v2_highlight\",\"type\":\"String\",\"value\":\"1\",\"chainId\":\"_all_\"},{\"id\":\"se_v043\",\"type\":\"String\",\"value\":\"0\",\"chainId\":\"_all_\"},{\"id\":\"se_colorfultab\",\"type\":\"String\",\"value\":\"1\",\"chainId\":\"_all_\"},{\"id\":\"se_multi_images\",\"type\":\"String\",\"value\":\"0\",\"chainId\":\"_all_\"},{\"id\":\"se_videobox\",\"type\":\"String\",\"value\":\"0\",\"chainId\":\"_all_\"},{\"id\":\"gue_video_guide\",\"type\":\"String\",\"value\":\"0\"},{\"id\":\"web_mweb_rec_length\",\"type\":\"String\",\"value\":\"1\"},{\"id\":\"zr_search_sim2\",\"type\":\"String\",\"value\":\"0\",\"chainId\":\"_all_\"},{\"id\":\"se_college\",\"type\":\"String\",\"value\":\"default\",\"chainId\":\"_all_\"},{\"id\":\"qap_question_visitor\",\"type\":\"String\",\"value\":\" 0\",\"chainId\":\"_all_\"},{\"id\":\"zr_zr_search_sims\",\"type\":\"String\",\"value\":\"0\",\"chainId\":\"_all_\"},{\"id\":\"zr_training_first\",\"type\":\"String\",\"value\":\"false\",\"chainId\":\"_all_\"},{\"id\":\"se_content0\",\"type\":\"String\",\"value\":\"1\",\"chainId\":\"_all_\"},{\"id\":\"tp_club_entrance\",\"type\":\"String\",\"value\":\"1\",\"chainId\":\"_all_\"},{\"id\":\"web_ad_banner\",\"type\":\"String\",\"value\":\"0\"},{\"id\":\"li_panswer_topic\",\"type\":\"String\",\"value\":\"0\",\"chainId\":\"_all_\"},{\"id\":\"li_training_chapter\",\"type\":\"String\",\"value\":\"0\",\"chainId\":\"_all_\"},{\"id\":\"tp_m_intro_re_topic\",\"type\":\"String\",\"value\":\"1\",\"chainId\":\"_all_\"},{\"id\":\"soc_adweeklynew\",\"type\":\"String\",\"value\":\"2\",\"chainId\":\"_all_\"},{\"id\":\"tsp_ios_cardredesign\",\"type\":\"String\",\"value\":\"0\",\"chainId\":\"_all_\"},{\"id\":\"gue_art2qa\",\"type\":\"String\",\"value\":\"0\"},{\"id\":\"qap_question_author\",\"type\":\"String\",\"value\":\"0\",\"chainId\":\"_all_\"},{\"id\":\"se_whitelist\",\"type\":\"String\",\"value\":\"0\",\"chainId\":\"_all_\"},{\"id\":\"ls_fmp4\",\"type\":\"String\",\"value\":\"0\",\"chainId\":\"_all_\"},{\"id\":\"ls_video_commercial\",\"type\":\"String\",\"value\":\"0\",\"chainId\":\"_all_\"},{\"id\":\"li_topics_search\",\"type\":\"String\",\"value\":\"0\",\"chainId\":\"_all_\"},{\"id\":\"se_dnn_mt_v2\",\"type\":\"String\",\"value\":\"0\",\"chainId\":\"_all_\"},{\"id\":\"soc_feed_intelligent\",\"type\":\"String\",\"value\":\"0\",\"chainId\":\"_all_\"},{\"id\":\"se_searchwiki\",\"type\":\"String\",\"value\":\"0\",\"chainId\":\"_all_\"},{\"id\":\"tp_club_feedv3\",\"type\":\"String\",\"value\":\"0\",\"chainId\":\"_all_\"},{\"id\":\"top_ebook\",\"type\":\"String\",\"value\":\"0\",\"chainId\":\"_all_\"},{\"id\":\"gue_card_test\",\"type\":\"String\",\"value\":\"1\"},{\"id\":\"web_audit_01\",\"type\":\"String\",\"value\":\"case1\"},{\"id\":\"se_club_boost\",\"type\":\"String\",\"value\":\"1\",\"chainId\":\"_all_\"},{\"id\":\"gue_bullet_guide\",\"type\":\"String\",\"value\":\"发个弹幕聊聊…\"},{\"id\":\"se_entity22\",\"type\":\"String\",\"value\":\"0\",\"chainId\":\"_all_\"},{\"id\":\"tp_club_flow_ai\",\"type\":\"String\",\"value\":\"0\",\"chainId\":\"_all_\"},{\"id\":\"pf_adjust\",\"type\":\"String\",\"value\":\"0\",\"chainId\":\"_all_\"},{\"id\":\"tsp_ad_cardredesign\",\"type\":\"String\",\"value\":\"0\",\"chainId\":\"_all_\"},{\"id\":\"gue_push2follow\",\"type\":\"String\",\"value\":\"0\"},{\"id\":\"zr_ans_rec\",\"type\":\"String\",\"value\":\"gbrank\",\"chainId\":\"_all_\"},{\"id\":\"web_unfriendly_comm\",\"type\":\"String\",\"value\":\"0\"},{\"id\":\"ug_follow_topic_1\",\"type\":\"String\",\"value\":\"2\",\"chainId\":\"_all_\"},{\"id\":\"zr_rel_search\",\"type\":\"String\",\"value\":\"base\",\"chainId\":\"_all_\"},{\"id\":\"se_cardrank_4\",\"type\":\"String\",\"value\":\"1\",\"chainId\":\"_all_\"},{\"id\":\"top_v_album\",\"type\":\"String\",\"value\":\"1\",\"chainId\":\"_all_\"},{\"id\":\"se_topicfeed\",\"type\":\"String\",\"value\":\"0\",\"chainId\":\"_all_\"},{\"id\":\"se_zvideo_bert\",\"type\":\"String\",\"value\":\"1\",\"chainId\":\"_all_\"},{\"id\":\"se_mobilecard\",\"type\":\"String\",\"value\":\"0\",\"chainId\":\"_all_\"},{\"id\":\"se_bsi\",\"type\":\"String\",\"value\":\"0\",\"chainId\":\"_all_\"},{\"id\":\"soc_iosweeklynew\",\"type\":\"String\",\"value\":\"2\",\"chainId\":\"_all_\"},{\"id\":\"li_paid_answer_exp\",\"type\":\"String\",\"value\":\"0\",\"chainId\":\"_all_\"},{\"id\":\"zr_slot_training\",\"type\":\"String\",\"value\":\"1\",\"chainId\":\"_all_\"},{\"id\":\"se_new_cbert\",\"type\":\"String\",\"value\":\"0\",\"chainId\":\"_all_\"},{\"id\":\"gue_video_replay\",\"type\":\"String\",\"value\":\"2\"},{\"id\":\"li_se_section\",\"type\":\"String\",\"value\":\"1\",\"chainId\":\"_all_\"},{\"id\":\"tp_topic_tab\",\"type\":\"String\",\"value\":\"0\",\"chainId\":\"_all_\"},{\"id\":\"tp_move_scorecard\",\"type\":\"String\",\"value\":\"0\",\"chainId\":\"_all_\"},{\"id\":\"pf_fuceng\",\"type\":\"String\",\"value\":\"1\",\"chainId\":\"_all_\"},{\"id\":\"gue_bullet_second\",\"type\":\"String\",\"value\":\"1\"},{\"id\":\"tp_topic_entry\",\"type\":\"String\",\"value\":\"0\",\"chainId\":\"_all_\"},{\"id\":\"tp_movie_ux\",\"type\":\"String\",\"value\":\"0\",\"chainId\":\"_all_\"},{\"id\":\"top_hotcommerce\",\"type\":\"String\",\"value\":\"1\",\"chainId\":\"_all_\"},{\"id\":\"gue_zvideo_link\",\"type\":\"String\",\"value\":\"1\"},{\"id\":\"li_svip_cardshow\",\"type\":\"String\",\"value\":\"1\",\"chainId\":\"_all_\"},{\"id\":\"gue_q_intercept\",\"type\":\"String\",\"value\":\"0\"},{\"id\":\"se_v040_2\",\"type\":\"String\",\"value\":\"0\",\"chainId\":\"_all_\"},{\"id\":\"web_column_auto_invite\",\"type\":\"String\",\"value\":\"0\"},{\"id\":\"se_v038\",\"type\":\"String\",\"value\":\"0\",\"chainId\":\"_all_\"},{\"id\":\"tp_club__entrance2\",\"type\":\"String\",\"value\":\"0\",\"chainId\":\"_all_\"},{\"id\":\"top_test_4_liguangyi\",\"type\":\"String\",\"value\":\"1\",\"chainId\":\"_all_\"},{\"id\":\"ug_goodcomment_0\",\"type\":\"String\",\"value\":\"1\",\"chainId\":\"_all_\"},{\"id\":\"web_heifetz_grow_ad\",\"type\":\"String\",\"value\":\"1\"},{\"id\":\"li_yxzl_new_style_a\",\"type\":\"String\",\"value\":\"1\",\"chainId\":\"_all_\"},{\"id\":\"qap_labeltype\",\"type\":\"String\",\"value\":\"1\",\"chainId\":\"_all_\"},{\"id\":\"se_ffzx_jushen1\",\"type\":\"String\",\"value\":\"0\",\"chainId\":\"_all_\"},{\"id\":\"se_clarify\",\"type\":\"String\",\"value\":\"0\",\"chainId\":\"_all_\"},{\"id\":\"gue_profile_video\",\"type\":\"String\",\"value\":\"1\"},{\"id\":\"ls_recommend_test\",\"type\":\"String\",\"value\":\"0\",\"chainId\":\"_all_\"},{\"id\":\"li_svip_tab_search\",\"type\":\"String\",\"value\":\"1\",\"chainId\":\"_all_\"},{\"id\":\"li_video_section\",\"type\":\"String\",\"value\":\"0\",\"chainId\":\"_all_\"},{\"id\":\"se_aa_base\",\"type\":\"String\",\"value\":\"0\",\"chainId\":\"_all_\"},{\"id\":\"se_sug_term\",\"type\":\"String\",\"value\":\"0\",\"chainId\":\"_all_\"},{\"id\":\"tp_club_top\",\"type\":\"String\",\"value\":\"0\",\"chainId\":\"_all_\"},{\"id\":\"zr_art_rec\",\"type\":\"String\",\"value\":\"base\",\"chainId\":\"_all_\"},{\"id\":\"se_relationship\",\"type\":\"String\",\"value\":\"0\",\"chainId\":\"_all_\"},{\"id\":\"se_video_dnn_2\",\"type\":\"String\",\"value\":\"0\",\"chainId\":\"_all_\"},{\"id\":\"se_hotsearch_2\",\"type\":\"String\",\"value\":\"1\",\"chainId\":\"_all_\"},{\"id\":\"se_searchvideo\",\"type\":\"String\",\"value\":\"0\",\"chainId\":\"_all_\"},{\"id\":\"se_multianswer\",\"type\":\"String\",\"value\":\"2\",\"chainId\":\"_all_\"},{\"id\":\"se_cbert_index\",\"type\":\"String\",\"value\":\"1\",\"chainId\":\"_all_\"},{\"id\":\"tp_topic_style\",\"type\":\"String\",\"value\":\"0\",\"chainId\":\"_all_\"},{\"id\":\"li_answer_card\",\"type\":\"String\",\"value\":\"0\",\"chainId\":\"_all_\"},{\"id\":\"zr_search_paid\",\"type\":\"String\",\"value\":\"1\",\"chainId\":\"_all_\"},{\"id\":\"zr_search_sims\",\"type\":\"String\",\"value\":\"0\",\"chainId\":\"_all_\"},{\"id\":\"pf_profile2_tab\",\"type\":\"String\",\"value\":\"0\",\"chainId\":\"_all_\"},{\"id\":\"tp_club_reactionv2\",\"type\":\"String\",\"value\":\"0\",\"chainId\":\"_all_\"},{\"id\":\"pf_noti_entry_num\",\"type\":\"String\",\"value\":\"0\",\"chainId\":\"_all_\"},{\"id\":\"zr_expslotpaid\",\"type\":\"String\",\"value\":\"1\",\"chainId\":\"_all_\"},{\"id\":\"zr_intervene\",\"type\":\"String\",\"value\":\"0\",\"chainId\":\"_all_\"},{\"id\":\"se_page_quality\",\"type\":\"String\",\"value\":\"1\",\"chainId\":\"_all_\"}],\"chains\":[{\"chainId\":\"_all_\"}]},\"triggers\":{}},\"userAgent\":{\"Edge\":false,\"Wechat\":false,\"Weibo\":false,\"QQ\":false,\"MQQBrowser\":false,\"Qzone\":false,\"Mobile\":false,\"Android\":false,\"iOS\":false,\"isAppleDevice\":false,\"Zhihu\":false,\"ZhihuHybrid\":false,\"isBot\":false,\"Tablet\":false,\"UC\":false,\"Sogou\":false,\"Qihoo\":false,\"Baidu\":false,\"BaiduApp\":false,\"Safari\":false,\"GoogleBot\":false,\"AndroidDaily\":false,\"iOSDaily\":false,\"isWebView\":false,\"origin\":\"helloworld\"},\"appViewConfig\":{},\"ctx\":{\"path\":\"\\\\u002Fquestion\\\\u002F29316149\",\"query\":{},\"href\":\"http:\\\\u002F\\\\u002Fwww.zhihu.com\\\\u002Fquestion\\\\u002F29316149\",\"host\":\"www.zhihu.com\"},\"trafficSource\":\"production\",\"edition\":{\"beijing\":false,\"baidu\":false,\"sogou\":false,\"baiduBeijing\":false,\"sogouBeijing\":false,\"sogouInput\":false,\"miniProgram\":false},\"theme\":\"light\",\"enableShortcut\":true,\"referer\":\"\",\"conf\":{},\"ipInfo\":{},\"logged\":false},\"me\":{\"columnContributions\":[]},\"label\":{\"recognizerLists\":{}},\"ecommerce\":{},\"comments\":{\"pagination\":{},\"collapsed\":{},\"reverse\":{},\"reviewing\":{},\"conversation\":{},\"parent\":{}},\"commentsV2\":{\"stickers\":[],\"commentWithPicPermission\":{},\"notificationsComments\":{},\"pagination\":{},\"collapsed\":{},\"reverse\":{},\"reviewing\":{},\"conversation\":{},\"conversationMore\":{},\"parent\":{}},\"pushNotifications\":{\"default\":{\"isFetching\":false,\"isDrained\":false,\"ids\":[]},\"follow\":{\"isFetching\":false,\"isDrained\":false,\"ids\":[]},\"vote_thank\":{\"isFetching\":false,\"isDrained\":false,\"ids\":[]},\"currentTab\":\"default\",\"notificationsCount\":{\"default\":0,\"follow\":0,\"vote_thank\":0}},\"messages\":{\"data\":{},\"currentTab\":\"common\",\"messageCount\":0},\"register\":{\"registerValidateSucceeded\":null,\"registerValidateErrors\":{},\"registerConfirmError\":null,\"sendDigitsError\":null,\"registerConfirmSucceeded\":null},\"login\":{\"loginUnregisteredError\":false,\"loginBindWechatError\":false,\"loginConfirmError\":null,\"sendDigitsError\":null,\"needSMSIdentify\":false,\"validateDigitsError\":false,\"loginConfirmSucceeded\":null,\"qrcodeLoginToken\":\"\",\"qrcodeLoginScanStatus\":0,\"qrcodeLoginError\":null,\"qrcodeLoginReturnNewToken\":false},\"active\":{\"sendDigitsError\":null,\"activeConfirmSucceeded\":null,\"activeConfirmError\":null},\"switches\":{},\"captcha\":{\"captchaNeeded\":false,\"captchaValidated\":false,\"captchaBase64String\":null,\"captchaValidationMessage\":null,\"loginCaptchaExpires\":false},\"sms\":{\"supportedCountries\":[]},\"chat\":{\"chats\":{},\"inbox\":{\"recents\":{\"isFetching\":false,\"isDrained\":false,\"isPrevDrained\":false,\"result\":[],\"next\":null,\"key\":null},\"strangers\":{\"isFetching\":false,\"isDrained\":false,\"isPrevDrained\":false,\"result\":[],\"next\":null,\"key\":null},\"friends\":{\"isFetching\":false,\"isDrained\":false,\"isPrevDrained\":false,\"result\":[],\"next\":null,\"key\":null},\"search\":{\"isFetching\":false,\"isDrained\":false,\"isPrevDrained\":false,\"result\":[],\"next\":null,\"key\":null},\"config\":{\"newCount\":0,\"strangerMessageSwitch\":false,\"strangerMessageUnread\":false,\"friendCount\":0}},\"global\":{\"isChatMqttExisted\":false}},\"emoticons\":{\"emoticonGroupList\":[],\"emoticonGroupDetail\":{}},\"creator\":{\"currentCreatorUrlToken\":null,\"homeData\":{\"recommendQuestions\":[]},\"tools\":{\"question\":{\"invitationCount\":{\"questionFolloweeCount\":0,\"questionTotalCount\":0},\"goodatTopics\":[]},\"customPromotion\":{\"itemLists\":{}},\"recommend\":{\"recommendTimes\":{}}},\"explore\":{\"academy\":{\"tabs\":[],\"article\":{}}},\"rights\":[],\"rightsStatus\":{},\"levelUpperLimit\":10,\"account\":{\"growthLevel\":{}},\"mcn\":{},\"applyStatus\":{}},\"question\":{\"followers\":{},\"concernedFollowers\":{},\"answers\":{\"29316149\":{\"isFetching\":false,\"isDrained\":false,\"ids\":[607394337,110159647,82949813,613063254,434884138],\"newIds\":[607394337,110159647,82949813,613063254,434884138],\"totals\":38,\"isPrevDrained\":true,\"previous\":\"https:\\\\u002F\\\\u002Fwww.zhihu.com\\\\u002Fapi\\\\u002Fv4\\\\u002Fquestions\\\\u002F29316149\\\\u002Fanswers?include=data%5B%2A%5D.is_normal%2Cadmin_closed_comment%2Creward_info%2Cis_collapsed%2Cannotation_action%2Cannotation_detail%2Ccollapse_reason%2Cis_sticky%2Ccollapsed_by%2Csuggest_edit%2Ccomment_count%2Ccan_comment%2Ccontent%2Ceditable_content%2Cvoteup_count%2Creshipment_settings%2Ccomment_permission%2Ccreated_time%2Cupdated_time%2Creview_info%2Crelevant_info%2Cquestion%2Cexcerpt%2Crelationship.is_authorized%2Cis_author%2Cvoting%2Cis_thanked%2Cis_nothelp%2Cis_labeled%2Cis_recognized%2Cpaid_info%2Cpaid_info_content%3Bdata%5B%2A%5D.mark_infos%5B%2A%5D.url%3Bdata%5B%2A%5D.author.follower_count%2Cbadge%5B%2A%5D.topics&limit=5&offset=0&platform=desktop&sort_by=default\",\"next\":\"https:\\\\u002F\\\\u002Fwww.zhihu.com\\\\u002Fapi\\\\u002Fv4\\\\u002Fquestions\\\\u002F29316149\\\\u002Fanswers?include=data%5B%2A%5D.is_normal%2Cadmin_closed_comment%2Creward_info%2Cis_collapsed%2Cannotation_action%2Cannotation_detail%2Ccollapse_reason%2Cis_sticky%2Ccollapsed_by%2Csuggest_edit%2Ccomment_count%2Ccan_comment%2Ccontent%2Ceditable_content%2Cvoteup_count%2Creshipment_settings%2Ccomment_permission%2Ccreated_time%2Cupdated_time%2Creview_info%2Crelevant_info%2Cquestion%2Cexcerpt%2Crelationship.is_authorized%2Cis_author%2Cvoting%2Cis_thanked%2Cis_nothelp%2Cis_labeled%2Cis_recognized%2Cpaid_info%2Cpaid_info_content%3Bdata%5B%2A%5D.mark_infos%5B%2A%5D.url%3Bdata%5B%2A%5D.author.follower_count%2Cbadge%5B%2A%5D.topics&limit=5&offset=5&platform=desktop&sort_by=default\"}},\"hiddenAnswers\":{},\"updatedAnswers\":{},\"collapsedAnswers\":{},\"notificationAnswers\":{},\"invitedQuestions\":{\"total\":{\"count\":null,\"isEnd\":false,\"isLoading\":false,\"questions\":[]},\"followees\":{\"count\":null,\"isEnd\":false,\"isLoading\":false,\"questions\":[]}},\"laterQuestions\":{\"count\":null,\"globalWriteAnimate\":false,\"isEnd\":false,\"isLoading\":false,\"questions\":[]},\"waitingQuestions\":{\"hot\":{\"isEnd\":false,\"isLoading\":false,\"questions\":[]},\"value\":{\"isEnd\":false,\"isLoading\":false,\"questions\":[]},\"newest\":{\"isEnd\":false,\"isLoading\":false,\"questions\":[]},\"easy\":{\"isEnd\":false,\"isLoading\":false,\"questions\":[]}},\"invitationCandidates\":{},\"inviters\":{},\"invitees\":{},\"similarQuestions\":{},\"relatedCommodities\":{},\"recommendReadings\":{},\"bio\":{},\"brand\":{},\"permission\":{},\"adverts\":{\"3\":{\"ad\":{\"adVerb\":\"\",\"brand\":{\"id\":0,\"logo\":\"https:\\\\u002F\\\\u002Fpic2.zhimg.com\\\\u002Fv2-b02efb32af2aada63f09e5d0a275df75_250x250.jpeg\",\"name\":\"Togocareer\"},\"category\":1,\"clickTracks\":[\"https:\\\\u002F\\\\u002Fsugar.zhihu.com\\\\u002Fplutus_adreaper?ut=82e95246f06443a6a017e4bcfc1f89b7&au=4280&ar=0.0000034637805014712143&nt=0&idi=2006&ed=CjEEfh4wM313QTNWCGB6AEZ9XHgBbm5yLxx_BgZhex1eJwBzXSdvcXwTZQYXMTYNWXYPbFkqPn15EGZUB2dqQAJ4CHwLdmxxaEY7WgZpegRZch8pTH5qZi1Kb1YBYGpSG3gIeghlNjVzFUUkeRqeuuuR&tu=https%3A%2F%2Fwww.togocareer.com%2Fhaiguiqiuzhi1.html%3F%3Dzhihu-0420&pf=0&ts=1591024878&pdi=1523617890848616&ui=118.165.121.143\"],\"closeTrack\":\"https:\\\\u002F\\\\u002Fsugar.zhihu.com\\\\u002Fplutus_adreaper?pf=0&au=4280&nt=0&idi=2006&ar=0.0000034637805014712143&tu=https%3A%2F%2Fwww.togocareer.com%2Fhaiguiqiuzhi1.html%3F%3Dzhihu-0420&ts=1591024878&pdi=1523617890848616&ed=CjEEfR4wM313QTNWCGB6AEZ9XHgBbm5yLxx_BgZhex1eJwBzXSdvcXwTZQYXMTYNWXYPbFkqPn15EGZUB2dqQAJ4CHwLdmxxaEY7WgZpegRZch8pTH5qZi1Kb1YBYGpSG3gIeghlNjVzFV_reES5v9O5&ut=82e95246f06443a6a017e4bcfc1f89b7&ui=118.165.121.143\",\"closeTracks\":[\"https:\\\\u002F\\\\u002Fsugar.zhihu.com\\\\u002Fplutus_adreaper?pf=0&au=4280&nt=0&idi=2006&ar=0.0000034637805014712143&tu=https%3A%2F%2Fwww.togocareer.com%2Fhaiguiqiuzhi1.html%3F%3Dzhihu-0420&ts=1591024878&pdi=1523617890848616&ed=CjEEfR4wM313QTNWCGB6AEZ9XHgBbm5yLxx_BgZhex1eJwBzXSdvcXwTZQYXMTYNWXYPbFkqPn15EGZUB2dqQAJ4CHwLdmxxaEY7WgZpegRZch8pTH5qZi1Kb1YBYGpSG3gIeghlNjVzFV_reES5v9O5&ut=82e95246f06443a6a017e4bcfc1f89b7&ui=118.165.121.143\"],\"conversionTracks\":[\"https:\\\\u002F\\\\u002Fsugar.zhihu.com\\\\u002Fplutus_adreaper?nt=0&tu=https%3A%2F%2Fwww.togocareer.com%2Fhaiguiqiuzhi1.html%3F%3Dzhihu-0420&ar=0.0000034637805014712143&au=4280&pf=0&idi=2006&pdi=1523617890848616&ut=82e95246f06443a6a017e4bcfc1f89b7&ui=118.165.121.143&ts=1591024878&ed=CjEEfx4wM313QTNWCGB6AEZ9XHgBbm5yLxx_BgZhex1eJwBzXSdvcXwTZQYXMTYNWXYPbFkqPn15EGZUB2dqQAJ4CHwLdmxxaEY7WgZpegRZch8pTH5qZi1Kb1YBYGpSG3gIeghlNjVzFZj2cFFMTivQ\"],\"creatives\":[{\"appPromotionUrl\":\"\",\"brand\":{\"id\":0,\"logo\":\"https:\\\\u002F\\\\u002Fpic2.zhimg.com\\\\u002Fv2-b02efb32af2aada63f09e5d0a275df75_250x250.jpeg\",\"name\":\"Togocareer\"},\"cta\":{\"value\":\"查看详情\"},\"description\":\"春招基本结束了。有的同学没有收到心仪的offer。有的同学没有找到心仪的企业。但是请不要沮丧。今年情况严峻特殊，秋招才是回暖的季节。所以我们更要提前准备秋招，了解岗位信息，做好自身功课。那么秋招到底什么时间开始？我们该做些什么呢...\",\"footer\":{\"value\":\"\"},\"landingUrl\":\"https:\\\\u002F\\\\u002Fwww.togocareer.com\\\\u002Fhaiguiqiuzhi1.html?=zhihu-0420\",\"title\":\"留学生参加2020秋招现在开始准备是不是太早了？\"}],\"debugTracks\":[\"https:\\\\u002F\\\\u002Fsugar.zhihu.com\\\\u002Fplutus_adreaper?tu=https%3A%2F%2Fwww.togocareer.com%2Fhaiguiqiuzhi1.html%3F%3Dzhihu-0420&ui=118.165.121.143&ts=1591024878&pf=0&pdi=1523617890848616&ed=CjEEcx4wM313QTNWCGB6AEZ9XHgBbm5yLxx_BgZhex1eJwBzXSdvcXwTZQYXMTYNWXYPbFkqPn15EGZUB2dqQAJ4CHwLdmxxaEY7WgZpegRZch8pTH5qZi1Kb1YBYGpSG3gIeghlNjVzFeRblhp7BHRo&au=4280&idi=2006&ut=82e95246f06443a6a017e4bcfc1f89b7&nt=0&ar=0.0000034637805014712143\"],\"displayAdvertisingTag\":true,\"experimentInfo\":\"{}\",\"id\":754367,\"impressionTracks\":[\"https:\\\\u002F\\\\u002Fsugar.zhihu.com\\\\u002Fplutus_adreaper?idi=2006&pdi=1523617890848616&ed=CjEEeB4wM313QTNWCGB6AEZ9XHgBbm5yLxx_BgZhex1eJwBzXSdvcXwTZQYXMTYNWXYPbFkqPn15EGZUB2dqQAJ4CHwLdmxxaEY7WgZpegRZch8pTH5qZi1Kb1YBYGpSG3gIeghlNjVzFU7gnYa7VTHi&tu=https%3A%2F%2Fwww.togocareer.com%2Fhaiguiqiuzhi1.html%3F%3Dzhihu-0420&ar=0.0000034637805014712143&au=4280&pf=0&nt=0&ts=1591024878&ut=82e95246f06443a6a017e4bcfc1f89b7&ui=118.165.121.143\"],\"isEvergreen\":false,\"isNewWebview\":true,\"isSpeeding\":false,\"isWebp\":false,\"landPrefetch\":false,\"name\":\"\",\"nativePrefetch\":true,\"partyId\":-2,\"revertCloseTrack\":\"https:\\\\u002F\\\\u002Fsugar.zhihu.com\\\\u002Fplutus_adreaper?ui=118.165.121.143&ed=CjEEch4wM313QTNWCGB6AEZ9XHgBbm5yLxx_BgZhex1eJwBzXSdvcXwTZQYXMTYNWXYPbFkqPn15EGZUB2dqQAJ4CHwLdmxxaEY7WgZpegRZch8pTH5qZi1Kb1YBYGpSG3gIeghlNjVzFTrzdEwUVGWP&tu=https%3A%2F%2Fwww.togocareer.com%2Fhaiguiqiuzhi1.html%3F%3Dzhihu-0420&idi=2006&pdi=1523617890848616&au=4280&ut=82e95246f06443a6a017e4bcfc1f89b7&ar=0.0000034637805014712143&nt=0&ts=1591024878&pf=0\",\"template\":\"web_word\",\"viewTrack\":\"https:\\\\u002F\\\\u002Fsugar.zhihu.com\\\\u002Fplutus_adreaper?ar=0.0000034637805014712143&nt=0&pf=0&au=4280&ui=118.165.121.143&idi=2006&ed=CjEEeR4wM313QTNWCGB6AEZ9XHgBbm5yLxx_BgZhex1eJwBzXSdvcXwTZQYXMTYNWXYPbFkqPn15EGZUB2dqQAJ4CHwLdmxxaEY7WgZpegRZch8pTH5qZi1Kb1YBYGpSG3gIeghlNjVzFbBz05KRYBnW&pdi=1523617890848616&ut=82e95246f06443a6a017e4bcfc1f89b7&tu=https%3A%2F%2Fwww.togocareer.com%2Fhaiguiqiuzhi1.html%3F%3Dzhihu-0420&ts=1591024878\",\"viewTracks\":[\"https:\\\\u002F\\\\u002Fsugar.zhihu.com\\\\u002Fplutus_adreaper?ar=0.0000034637805014712143&nt=0&pf=0&au=4280&ui=118.165.121.143&idi=2006&ed=CjEEeR4wM313QTNWCGB6AEZ9XHgBbm5yLxx_BgZhex1eJwBzXSdvcXwTZQYXMTYNWXYPbFkqPn15EGZUB2dqQAJ4CHwLdmxxaEY7WgZpegRZch8pTH5qZi1Kb1YBYGpSG3gIeghlNjVzFbBz05KRYBnW&pdi=1523617890848616&ut=82e95246f06443a6a017e4bcfc1f89b7&tu=https%3A%2F%2Fwww.togocareer.com%2Fhaiguiqiuzhi1.html%3F%3Dzhihu-0420&ts=1591024878\"],\"zaAdInfo\":\"CL+FLhDsASIBMV06qr1OYIvOMA==\",\"zaAdInfoJson\":\"{\\\\\"ad_id\\\\\":754367,\\\\\"ad_zone_id\\\\\":236,\\\\\"category\\\\\":\\\\\"1\\\\\",\\\\\"timestamp\\\\\":1591024900,\\\\\"creative_id\\\\\":796427}\"},\"adjson\":\"{\\\\\"ads\\\\\":[{\\\\\"id\\\\\":754367,\\\\\"ad_zone_id\\\\\":236,\\\\\"template\\\\\":\\\\\"web_word\\\\\",\\\\\"impression_tracks\\\\\":[\\\\\"https:\\\\u002F\\\\u002Fsugar.zhihu.com\\\\u002Fplutus_adreaper?au=4280\\\\\\\\u0026pdi=1523617890848616\\\\\\\\u0026tu=https%3A%2F%2Fwww.togocareer.com%2Fhaiguiqiuzhi1.html%3F%3Dzhihu-0420\\\\\\\\u0026ts=1591024878\\\\\\\\u0026ed=CjEEeB4wM313QTNWCGB6AEZ9XHgBbm5yLxx_BgZhex1eJwBzXSdvcXwTZQYXMTYNWXYPbFkqPn15EGZUB2dqQAJ4CHwLdmxxaEY7WgZpegRZch8pTH5qZi1Kb1YBYGpSG3gIeghlNjVzFU7gnYa7VTHi\\\\\\\\u0026idi=2006\\\\\\\\u0026pf=0\\\\\\\\u0026ui=118.165.121.143\\\\\\\\u0026ar=0.0000034637805014712143\\\\\\\\u0026nt=0\\\\\\\\u0026ut=82e95246f06443a6a017e4bcfc1f89b7\\\\\"],\\\\\"view_tracks\\\\\":[\\\\\"https:\\\\u002F\\\\u002Fsugar.zhihu.com\\\\u002Fplutus_adreaper?ar=0.0000034637805014712143\\\\\\\\u0026tu=https%3A%2F%2Fwww.togocareer.com%2Fhaiguiqiuzhi1.html%3F%3Dzhihu-0420\\\\\\\\u0026ut=82e95246f06443a6a017e4bcfc1f89b7\\\\\\\\u0026pdi=1523617890848616\\\\\\\\u0026pf=0\\\\\\\\u0026idi=2006\\\\\\\\u0026ed=CjEEeR4wM313QTNWCGB6AEZ9XHgBbm5yLxx_BgZhex1eJwBzXSdvcXwTZQYXMTYNWXYPbFkqPn15EGZUB2dqQAJ4CHwLdmxxaEY7WgZpegRZch8pTH5qZi1Kb1YBYGpSG3gIeghlNjVzFbBz05KRYBnW\\\\\\\\u0026ts=1591024878\\\\\\\\u0026au=4280\\\\\\\\u0026nt=0\\\\\\\\u0026ui=118.165.121.143\\\\\"],\\\\\"click_tracks\\\\\":[\\\\\"https:\\\\u002F\\\\u002Fsugar.zhihu.com\\\\u002Fplutus_adreaper?nt=0\\\\\\\\u0026pf=0\\\\\\\\u0026ui=118.165.121.143\\\\\\\\u0026tu=https%3A%2F%2Fwww.togocareer.com%2Fhaiguiqiuzhi1.html%3F%3Dzhihu-0420\\\\\\\\u0026ar=0.0000034637805014712143\\\\\\\\u0026ts=1591024878\\\\\\\\u0026au=4280\\\\\\\\u0026ut=82e95246f06443a6a017e4bcfc1f89b7\\\\\\\\u0026pdi=1523617890848616\\\\\\\\u0026idi=2006\\\\\\\\u0026ed=CjEEfh4wM313QTNWCGB6AEZ9XHgBbm5yLxx_BgZhex1eJwBzXSdvcXwTZQYXMTYNWXYPbFkqPn15EGZUB2dqQAJ4CHwLdmxxaEY7WgZpegRZch8pTH5qZi1Kb1YBYGpSG3gIeghlNjVzFUUkeRqeuuuR\\\\\"],\\\\\"close_tracks\\\\\":[\\\\\"https:\\\\u002F\\\\u002Fsugar.zhihu.com\\\\u002Fplutus_adreaper?au=4280\\\\\\\\u0026ts=1591024878\\\\\\\\u0026ar=0.0000034637805014712143\\\\\\\\u0026ui=118.165.121.143\\\\\\\\u0026ut=82e95246f06443a6a017e4bcfc1f89b7\\\\\\\\u0026pf=0\\\\\\\\u0026tu=https%3A%2F%2Fwww.togocareer.com%2Fhaiguiqiuzhi1.html%3F%3Dzhihu-0420\\\\\\\\u0026nt=0\\\\\\\\u0026pdi=1523617890848616\\\\\\\\u0026idi=2006\\\\\\\\u0026ed=CjEEfR4wM313QTNWCGB6AEZ9XHgBbm5yLxx_BgZhex1eJwBzXSdvcXwTZQYXMTYNWXYPbFkqPn15EGZUB2dqQAJ4CHwLdmxxaEY7WgZpegRZch8pTH5qZi1Kb1YBYGpSG3gIeghlNjVzFV_reES5v9O5\\\\\"],\\\\\"debug_tracks\\\\\":[\\\\\"https:\\\\u002F\\\\u002Fsugar.zhihu.com\\\\u002Fplutus_adreaper?pdi=1523617890848616\\\\\\\\u0026ut=82e95246f06443a6a017e4bcfc1f89b7\\\\\\\\u0026au=4280\\\\\\\\u0026pf=0\\\\\\\\u0026ui=118.165.121.143\\\\\\\\u0026ar=0.0000034637805014712143\\\\\\\\u0026tu=https%3A%2F%2Fwww.togocareer.com%2Fhaiguiqiuzhi1.html%3F%3Dzhihu-0420\\\\\\\\u0026idi=2006\\\\\\\\u0026nt=0\\\\\\\\u0026ed=CjEEcx4wM313QTNWCGB6AEZ9XHgBbm5yLxx_BgZhex1eJwBzXSdvcXwTZQYXMTYNWXYPbFkqPn15EGZUB2dqQAJ4CHwLdmxxaEY7WgZpegRZch8pTH5qZi1Kb1YBYGpSG3gIeghlNjVzFeRblhp7BHRo\\\\\\\\u0026ts=1591024878\\\\\"],\\\\\"conversion_tracks\\\\\":[\\\\\"https:\\\\u002F\\\\u002Fsugar.zhihu.com\\\\u002Fplutus_adreaper?pdi=1523617890848616\\\\\\\\u0026idi=2006\\\\\\\\u0026ts=1591024878\\\\\\\\u0026ui=118.165.121.143\\\\\\\\u0026pf=0\\\\\\\\u0026au=4280\\\\\\\\u0026tu=https%3A%2F%2Fwww.togocareer.com%2Fhaiguiqiuzhi1.html%3F%3Dzhihu-0420\\\\\\\\u0026nt=0\\\\\\\\u0026ut=82e95246f06443a6a017e4bcfc1f89b7\\\\\\\\u0026ar=0.0000034637805014712143\\\\\\\\u0026ed=CjEEfx4wM313QTNWCGB6AEZ9XHgBbm5yLxx_BgZhex1eJwBzXSdvcXwTZQYXMTYNWXYPbFkqPn15EGZUB2dqQAJ4CHwLdmxxaEY7WgZpegRZch8pTH5qZi1Kb1YBYGpSG3gIeghlNjVzFZj2cFFMTivQ\\\\\"],\\\\\"extra_conversion_tracks\\\\\":{\\\\\"call_back\\\\\":[\\\\\"https:\\\\u002F\\\\u002Fsugar.zhihu.com\\\\u002Fplutus_adreaper_callback?si=9da19060-8e29-42a9-a717-5b99ed51267a\\\\\\\\u0026os=3\\\\\\\\u0026zid=236\\\\\\\\u0026zaid=754367\\\\\\\\u0026zcid=796427\\\\\\\\u0026cid=796426\\\\\\\\u0026event=__EVENTTYPE__\\\\\\\\u0026value=__EVENTVALUE__\\\\\\\\u0026ts=__TIMESTAMP__\\\\\\\\u0026cts=__TS__\\\\\"]},\\\\\"za_ad_info\\\\\":\\\\\"CL+FLhDsASIBMV06qr1OYIvOMA==\\\\\",\\\\\"za_ad_info_json\\\\\":\\\\\"{\\\\\\\\\\\\\"ad_id\\\\\\\\\\\\\":754367,\\\\\\\\\\\\\"ad_zone_id\\\\\\\\\\\\\":236,\\\\\\\\\\\\\"category\\\\\\\\\\\\\":\\\\\\\\\\\\\"1\\\\\\\\\\\\\",\\\\\\\\\\\\\"timestamp\\\\\\\\\\\\\":1591024900,\\\\\\\\\\\\\"creative_id\\\\\\\\\\\\\":796427}\\\\\",\\\\\"creatives\\\\\":[{\\\\\"id\\\\\":796427,\\\\\"asset\\\\\":{\\\\\"brand_name\\\\\":\\\\\"Togocareer\\\\\",\\\\\"brand_logo\\\\\":\\\\\"https:\\\\u002F\\\\u002Fpic2.zhimg.com\\\\u002Fv2-b02efb32af2aada63f09e5d0a275df75_250x250.jpeg\\\\\",\\\\\"title\\\\\":\\\\\"留学生参加2020秋招现在开始准备是不是太早了？\\\\\",\\\\\"desc\\\\\":\\\\\"春招基本结束了。有的同学没有收到心仪的offer。有的同学没有找到心仪的企业。但是请不要沮丧。今年情况严峻特殊，秋招才是回暖的季节。所以我们更要提前准备秋招，了解岗位信息，做好自身功课。那么秋招到底什么时间开始？我们该做些什么呢...\\\\\",\\\\\"landing_url\\\\\":\\\\\"https:\\\\u002F\\\\u002Fwww.togocareer.com\\\\u002Fhaiguiqiuzhi1.html?=zhihu-0420\\\\\",\\\\\"img_size\\\\\":0,\\\\\"cta\\\\\":\\\\\"查看详情\\\\\",\\\\\"native_asset\\\\\":{}}}],\\\\\"expand\\\\\":{\\\\\"display_advertising_tag\\\\\":true,\\\\\"is_new_webview\\\\\":true,\\\\\"is_cdn_speeding\\\\\":false},\\\\\"experiment_info\\\\\":\\\\\"{}\\\\\",\\\\\"view_x_tracks\\\\\":[\\\\\"https:\\\\u002F\\\\u002Fsugar.zhihu.com\\\\u002Fplutus_adreaper?ut=82e95246f06443a6a017e4bcfc1f89b7\\\\\\\\u0026ar=0.0000034637805014712143\\\\\\\\u0026ts=1591024878\\\\\\\\u0026tu=https%3A%2F%2Fwww.togocareer.com%2Fhaiguiqiuzhi1.html%3F%3Dzhihu-0420\\\\\\\\u0026au=4280\\\\\\\\u0026ui=118.165.121.143\\\\\\\\u0026pf=0\\\\\\\\u0026ed=CjEEewhlKSlzHDYGAGl8BltoAS8Kend0fERrSlBnfQdGcFtzASY-dX8XZFBQdi1KVncKfB4iMyRzEmdTAmZ7FhssBHsOcG92fwMxDgxndQZfdw5sWzdncGhGPVoAYHwWCTUEewhzfCw7GGJXDignxJbYYg%3D%3D\\\\\\\\u0026nt=0\\\\\\\\u0026pdi=1523617890848616\\\\\\\\u0026idi=2006\\\\\"],\\\\\"mobile_experiment\\\\\":{\\\\\"am_or_ad\\\\\":\\\\\"0\\\\\"},\\\\\"web_experiment\\\\\":{\\\\\"aw_ad_intext_bg\\\\\":\\\\\"0\\\\\",\\\\\"aw_ad_intext_border\\\\\":\\\\\"0\\\\\",\\\\\"aw_intext\\\\\":\\\\\"0\\\\\"}}]}\"}},\"advancedStyle\":{},\"commonAnswerCount\":0,\"hiddenAnswerCount\":0,\"meta\":{},\"relatedSearch\":{},\"autoInvitation\":{},\"simpleConcernedFollowers\":{},\"draftStatus\":{},\"disclaimers\":{}},\"shareTexts\":{},\"answers\":{\"voters\":{},\"copyrightApplicants\":{},\"favlists\":{},\"newAnswer\":{},\"concernedUpvoters\":{},\"simpleConcernedUpvoters\":{},\"paidContent\":{},\"settings\":{}},\"banner\":{},\"topic\":{\"bios\":{},\"hot\":{},\"newest\":{},\"top\":{},\"unanswered\":{},\"questions\":{},\"followers\":{},\"contributors\":{},\"parent\":{},\"children\":{},\"bestAnswerers\":{},\"wikiMeta\":{},\"index\":{},\"intro\":{},\"meta\":{},\"schema\":{},\"creatorWall\":{},\"wikiEditInfo\":{},\"committedWiki\":{},\"landingBasicData\":{},\"landingExcellentItems\":[],\"landingExcellentEditors\":[],\"landingCatalog\":[],\"landingEntries\":{}},\"explore\":{\"recommendations\":{},\"specials\":{\"entities\":{},\"order\":[]},\"roundtables\":{\"entities\":{},\"order\":[]},\"collections\":{},\"columns\":{}},\"articles\":{\"voters\":{}},\"favlists\":{\"relations\":{}},\"pins\":{\"reviewing\":{}},\"topstory\":{\"recommend\":{\"isFetching\":false,\"isDrained\":false,\"afterId\":0,\"items\":[],\"next\":null},\"follow\":{\"isFetching\":false,\"isDrained\":false,\"afterId\":0,\"items\":[],\"next\":null},\"followWonderful\":{\"isFetching\":false,\"isDrained\":false,\"afterId\":0,\"items\":[],\"next\":null},\"sidebar\":null,\"announcement\":{},\"hotListCategories\":[],\"hotList\":[],\"guestFeeds\":{\"isFetching\":false,\"isDrained\":false,\"afterId\":0,\"items\":[],\"next\":null},\"followExtra\":{\"isNewUser\":null,\"isFetched\":false,\"followCount\":0,\"followers\":[]}},\"upload\":{},\"video\":{\"data\":{},\"shareVideoDetail\":{},\"last\":{}},\"zvideos\":{\"campaigns\":{},\"tagoreCategory\":[],\"recommendations\":{},\"recruit\":{\"stat\":{\"invitedCount\":0,\"promoteCount\":0},\"form\":{\"memberType\":\"people\",\"platform\":\"\",\"nickname\":\"\",\"followerCount\":\"\",\"screenshotUrls\":[\"\",\"\"],\"domain\":\"\",\"representativeUrl\":\"\",\"contact\":\"\"},\"submited\":false,\"ranking\":[]}},\"guide\":{\"guide\":{\"isFetching\":false,\"isShowGuide\":false}},\"reward\":{\"answer\":{},\"article\":{},\"question\":{}},\"search\":{\"recommendSearch\":[],\"topSearch\":{},\"searchValue\":{},\"suggestSearch\":{},\"attachedInfo\":{},\"nextOffset\":{},\"topicReview\":{},\"generalByQuery\":{},\"generalByQueryInADay\":{},\"generalByQueryInAWeek\":{},\"generalByQueryInThreeMonths\":{},\"peopleByQuery\":{},\"clubentityByQuery\":{},\"clubPostByQuery\":{},\"topicByQuery\":{},\"columnByQuery\":{},\"liveByQuery\":{},\"albumByQuery\":{},\"eBookByQuery\":{},\"kmGeneralByQuery\":{}},\"publicEditPermission\":{},\"readStatus\":{},\"draftHistory\":{\"history\":{},\"drafts\":{}},\"notifications\":{\"recent\":{\"isFetching\":false,\"isDrained\":false,\"isPrevDrained\":false,\"result\":[],\"next\":null,\"key\":null},\"history\":{\"isFetching\":false,\"isDrained\":false,\"isPrevDrained\":false,\"result\":[],\"next\":null,\"key\":null},\"notificationActors\":{\"isFetching\":false,\"isDrained\":false,\"isPrevDrained\":false,\"result\":[],\"next\":null,\"key\":null},\"recentNotificationEntry\":\"all\"},\"specials\":{\"entities\":{},\"all\":{\"data\":[],\"paging\":{},\"isLoading\":false}},\"collections\":{\"hot\":{\"data\":[],\"paging\":{},\"isLoading\":false},\"collectionFeeds\":{}},\"mcn\":{\"bindInfo\":{},\"memberCategoryList\":[],\"producerList\":[],\"categoryList\":[],\"lists\":{}},\"mcnActivity\":{\"household\":{\"products\":{},\"rankList\":{\"total\":{},\"yesterday\":{}}}},\"brand\":{\"contentPlugin\":{}},\"metaLink\":{\"metaLinkTemplate\":{}},\"host\":{\"roundtable\":{\"subjects\":{},\"applications\":{\"total\":0},\"online\":{\"total\":0},\"applies\":{},\"details\":{},\"includedResource\":{},\"hotQuestions\":{},\"warmupContents\":{},\"batchInclude\":{}},\"special\":{\"applications\":{\"total\":0,\"pages\":{},\"entities\":{}},\"censorHistory\":{},\"drafts\":{}}},\"campaign\":{\"single\":{},\"list\":{}},\"knowledgePlan\":{\"lists\":{},\"allCreationRankList\":{},\"featuredQuestions\":{}},\"wallE\":{\"protectHistory\":{\"total\":0,\"pages\":{},\"entities\":{}}},\"roundtables\":{\"hotQuestions\":{},\"warmupContents\":{},\"hotDiscussions\":{},\"selectedContents\":{},\"roundtables\":{}},\"helpCenter\":{\"entities\":{\"question\":{},\"category\":{}},\"categories\":[],\"commonQuestions\":[],\"relatedQuestions\":{}}},\"subAppName\":\"main\"}</script><script src=\"https://static.zhihu.com/heifetz/vendor.90b6c2c841b0448cdc35.js\"></script><script src=\"https://static.zhihu.com/heifetz/main.app.5da08b3d49e20643e893.js\"></script><script src=\"https://static.zhihu.com/heifetz/main.question-routes.f888a60c35a677d0d8e7.js\"></script></body><script src=\"https://hm.baidu.com/hm.js?98beee57fd2ef70ccdd5ca52b9740c49\" async=\"\"></script><script src=\"https://zz.bdstatic.com/linksubmit/push.js\" async=\"\"></script></html>'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Your Code Here\n",
    "\"\"\"\n",
    "import requests\n",
    "url = 'https://www.zhihu.com/question/29316149'\n",
    "headers = {'user-agent' : 'helloworld'}\n",
    "res = requests.get(url, headers = headers)\n",
    "response = res.text\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
